{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba41423",
   "metadata": {},
   "source": [
    "### good_data_ì ê²€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effc740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === ì„¤ì • ===\n",
    "base_dir = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/20250930/20250930_good_data\")\n",
    "\n",
    "# === íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ ===\n",
    "jpg_files = {p.stem for p in base_dir.glob(\"*.jpg\")}\n",
    "txt_files = {p.stem for p in base_dir.glob(\"*.txt\")}\n",
    "\n",
    "# === ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ì°¾ê¸° ===\n",
    "jpg_only = sorted(jpg_files - txt_files)\n",
    "txt_only = sorted(txt_files - jpg_files)\n",
    "\n",
    "# === ì¶œë ¥ ===\n",
    "print(f\"ğŸ“¸ JPGë§Œ ìˆê³  TXT ì—†ëŠ” íŒŒì¼ ({len(jpg_only)}ê°œ):\")\n",
    "for name in jpg_only:\n",
    "    print(f\" - {name}.jpg\")\n",
    "\n",
    "print(\"\\nğŸ“ TXTë§Œ ìˆê³  JPG ì—†ëŠ” íŒŒì¼ ({len(txt_only)}ê°œ):\")\n",
    "for name in txt_only:\n",
    "    print(f\" - {name}.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9221ef",
   "metadata": {},
   "source": [
    "### ê°ì²´í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0449dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# ====== ê²½ë¡œ ì„¤ì • ======\n",
    "LABEL_DIR = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/20250721/20250721_good_data\")\n",
    "#LABEL_DIR = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/20250725/20250725_good_data\")\n",
    "#LABEL_DIR = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/20250904/20250904_good_data\")\n",
    "#LABEL_DIR = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/20250930/20250930_good_data\")\n",
    "\n",
    "# YOLO í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘\n",
    "classes = [\n",
    "    \"Divot\",         # 0\n",
    "    \"Fixed_Divot\",   # 1\n",
    "    \"Diseased_Grass\",# 2\n",
    "    \"Confused_Object\",# 3\n",
    "    \"Pole\",          # 4\n",
    "    \"Sprinkler\",     # 5\n",
    "    \"Drain\",         # 6\n",
    "    \"Golf ball\"      # 7\n",
    "]\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for txt_file in LABEL_DIR.glob(\"*.txt\"):\n",
    "    if txt_file.name == \"classes.txt\":     # â˜… classes.txtëŠ” ê±´ë„ˆë›°ê¸°\n",
    "        continue\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if parts:\n",
    "                cls_id = int(parts[0])\n",
    "                counter[cls_id] += 1\n",
    "\n",
    "# ====== ê²°ê³¼ ì¶œë ¥ ======\n",
    "print(\"=== í´ë˜ìŠ¤ë³„ ê°ì²´ ê°œìˆ˜ ===\")\n",
    "for i, cls_name in enumerate(classes):\n",
    "    print(f\"{cls_name:<15}: {counter.get(i, 0)}\")\n",
    "\n",
    "print(\"\\nì´ ê°ì²´ ìˆ˜:\", sum(counter.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a8b86",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ë³µì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00a0b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³µì‚¬ ì¤‘: /home/dw/ws_job_msislab/Golf_Project/data/20250721/20250721_good_data  â†’  /home/dw/ws_job_msislab/Golf_Project/data/for_study/20251013_merge_data/20250721_good_data\n",
      "ë³µì‚¬ ì¤‘: /home/dw/ws_job_msislab/Golf_Project/data/20250725/20250725_good_data  â†’  /home/dw/ws_job_msislab/Golf_Project/data/for_study/20251013_merge_data/20250725_good_data\n",
      "ë³µì‚¬ ì¤‘: /home/dw/ws_job_msislab/Golf_Project/data/20250904/20250904_good_data  â†’  /home/dw/ws_job_msislab/Golf_Project/data/for_study/20251013_merge_data/20250904_good_data\n",
      "ë³µì‚¬ ì¤‘: /home/dw/ws_job_msislab/Golf_Project/data/20250930/20250930_good_data  â†’  /home/dw/ws_job_msislab/Golf_Project/data/for_study/20251013_merge_data/20250930_good_data\n",
      "\n",
      "[ì™„ë£Œ] ì´ 4ê°œ í´ë”ë¥¼ /home/dw/ws_job_msislab/Golf_Project/data/for_study/20251013_merge_data ì•„ë˜ë¡œ ë³µì‚¬í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# ===== ê²½ë¡œ ì„¤ì • =====\n",
    "DST = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251013_merge_data\")\n",
    "DST.mkdir(parents=True, exist_ok=True)  # ëª©ì ì§€ í´ë” ìƒì„±\n",
    "\n",
    "SRC_LIST = [\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/20250721/20250721_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/20250725/20250725_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/20250904/20250904_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/20250930/20250930_good_data\"),\n",
    "]\n",
    "\n",
    "# ===== ë³µì‚¬ ì‹¤í–‰ =====\n",
    "for src in SRC_LIST:\n",
    "    target = DST / src.name  # ì˜ˆ: .../20250914_merge_data/20250721_good_data\n",
    "    print(f\"ë³µì‚¬ ì¤‘: {src}  â†’  {target}\")\n",
    "    # dirs_exist_ok=True ë¡œ ì´ë¯¸ ì¡´ì¬í•´ë„ ë®ì–´ì”€\n",
    "    shutil.copytree(src, target, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"\\n[ì™„ë£Œ] ì´ {len(SRC_LIST)}ê°œ í´ë”ë¥¼ {DST} ì•„ë˜ë¡œ ë³µì‚¬í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7eeb8e",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ë¶„í•  _ ëœë¤ì‹œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_inplace_balanced_random.py\n",
    "# - ëŒ€ìƒ í´ë”(ê° good_data) ë‚´ë¶€ íŒŒì¼ì„ 'ì´ë™'í•˜ì—¬ images/{train,val,test}, labels/{train,val,test}ë¡œ ì¬ë°°ì¹˜\n",
    "# - íŒŒì¼ëª… ê·œì¹™/ì‹œí€€ìŠ¤ ë¬´ì‹œ: ì „ì—­+ë¸”ë¡ ì…”í”Œë¡œ ê°•í•˜ê²Œ ì„ì€ ë’¤ í™•ë¥ ì (ë…¸ì´ì¦ˆ) ì¸µí™”ë¡œ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\n",
    "# - data.yaml ìƒì„± ì—†ìŒ\n",
    "# - íŒŒê´´ì  ì‘ì—…(ì´ë™)ì…ë‹ˆë‹¤. í•„ìš”í•˜ë©´ ì‹¤í–‰ ì „ ë°±ì—…í•˜ì„¸ìš”.\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random, shutil\n",
    "\n",
    "# ========== ì‘ì—… ëŒ€ìƒ í´ë” ==========\n",
    "TARGETS = [\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250922_merge_data/20250721_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250922_merge_data/20250725_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250922_merge_data/20250904_good_data\"),\n",
    "]\n",
    "\n",
    "# ========== ë¶„í•  ë¹„ìœ¨ / í•˜ì´í¼íŒŒë¼ë¯¸í„° ==========\n",
    "RATIOS = {\"train\": 0.64, \"val\": 0.26, \"test\": 0.10}  # í•©=1.0\n",
    "SIZE_WEIGHT = 0.05   # ìƒ˜í”Œìˆ˜ ëª©í‘œ ì˜¤ì°¨ ê°€ì¤‘ì¹˜\n",
    "JITTER = 0.20        # ë¹„ìš©ì— ë”í•˜ëŠ” ë¬´ì‘ìœ„ ë…¸ì´ì¦ˆ(â†‘ ë” ëœë¤)\n",
    "BLOCK_SIZE = 16      # ë¸”ë¡ ì…”í”Œ í¬ê¸°(â†‘ ë” ì„ì„)\n",
    "LABEL_TOL = 0.05     # splitë³„ ë¼ë²¨ íƒ€ê¹ƒ ê³¼ì¶©ì¡± í—ˆìš©ë¥ (5% ì—¬ìœ )\n",
    "SEED = 38\n",
    "random.seed(SEED)\n",
    "\n",
    "# ========== ë°ì´í„° ìŠ¤í™ ==========\n",
    "IMG_EXTS = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"]\n",
    "LABELS = list(range(8))  # 0..7\n",
    "BG = \"bg\"                # ë¼ë²¨ ì—†ëŠ” ì´ë¯¸ì§€ìš© ê°€ìƒ í´ë˜ìŠ¤\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ìœ í‹¸\n",
    "# ------------------------------------------------------------\n",
    "def ensure_dirs(root: Path):\n",
    "    (root / \"images\").mkdir(exist_ok=True)\n",
    "    (root / \"labels\").mkdir(exist_ok=True)\n",
    "    for s, r in RATIOS.items():\n",
    "        if r <= 0: \n",
    "            continue\n",
    "        (root / \"images\" / s).mkdir(parents=True, exist_ok=True)\n",
    "        (root / \"labels\" / s).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def parse_label_file(p: Path):\n",
    "    \"\"\"ë¼ë²¨ íŒŒì¼ ì•ˆ ë“±ì¥í•œ í´ë˜ìŠ¤ ì§‘í•© ë°˜í™˜. ì˜ëª»ëœ/ë¹ˆ ë¼ì¸ì€ ë¬´ì‹œ.\"\"\"\n",
    "    labs = set()\n",
    "    try:\n",
    "        with open(p, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    try:\n",
    "                        cid = int(float(parts[0]))\n",
    "                    except:\n",
    "                        continue\n",
    "                    if cid in LABELS:\n",
    "                        labs.add(cid)\n",
    "    except:\n",
    "        pass\n",
    "    return labs\n",
    "\n",
    "def collect_items(ROOT: Path):\n",
    "    \"\"\"\n",
    "    ROOTì—ì„œ ëª©ì ì§€(images/labels í•˜ìœ„)ë¥¼ ì œì™¸í•˜ê³  ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì°¾ì•„\n",
    "    (stem, img_path, lbl_path or None, labs_set) ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.\n",
    "    ë¼ë²¨ì´ ì—†ê±°ë‚˜ ë¹ˆ ë¼ë²¨ì´ë©´ labs_set={BG}\n",
    "    \"\"\"\n",
    "    # í›„ë³´ ì´ë¯¸ì§€ ìˆ˜ì§‘\n",
    "    candidates = []\n",
    "    for ext in IMG_EXTS:\n",
    "        for img in ROOT.rglob(f\"*{ext}\"):\n",
    "            rel = img.relative_to(ROOT)\n",
    "            if rel.parts and rel.parts[0] in (\"images\", \"labels\"):\n",
    "                continue\n",
    "            candidates.append(img)\n",
    "\n",
    "    # ê°•í•œ ì„ê¸°: ì „ì—­ ì…”í”Œ + ë¸”ë¡ ì…”í”Œ\n",
    "    stems = list({p.stem for p in candidates})\n",
    "    random.shuffle(stems)\n",
    "    if BLOCK_SIZE > 1:\n",
    "        blocks = [stems[i:i+BLOCK_SIZE] for i in range(0, len(stems), BLOCK_SIZE)]\n",
    "        for b in blocks:\n",
    "            random.shuffle(b)\n",
    "        random.shuffle(blocks)\n",
    "        stems = [s for b in blocks for s in b]\n",
    "\n",
    "    items = []\n",
    "    for stem in stems:\n",
    "        # ROOT ì§í•˜ ìš°ì„ , ì—†ìœ¼ë©´ ì¬ê·€\n",
    "        img = None\n",
    "        for ext in IMG_EXTS:\n",
    "            p = ROOT / f\"{stem}{ext}\"\n",
    "            if p.exists():\n",
    "                img = p; break\n",
    "        if img is None:\n",
    "            for ext in IMG_EXTS:\n",
    "                hits = [h for h in ROOT.rglob(f\"{stem}{ext}\")\n",
    "                        if (h.relative_to(ROOT).parts and h.relative_to(ROOT).parts[0] not in (\"images\",\"labels\"))]\n",
    "                if hits:\n",
    "                    img = hits[0]; break\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        lbl = img.with_suffix(\".txt\")\n",
    "        if not lbl.exists() or lbl.stat().st_size == 0:\n",
    "            items.append((stem, img, None, {BG}))\n",
    "        else:\n",
    "            labs = parse_label_file(lbl)\n",
    "            if labs:\n",
    "                items.append((stem, img, lbl, labs))\n",
    "            else:\n",
    "                items.append((stem, img, None, {BG}))\n",
    "    return items\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ + ëœë¤ ë…¸ì´ì¦ˆ ê¸°ë°˜ ì¸µí™” ë°°ì¹˜\n",
    "# ------------------------------------------------------------\n",
    "def balanced_interleave_stratify(items, labels=list(range(8)), bg_token=\"bg\", seed=42):\n",
    "    \"\"\"\n",
    "    items: [(stem, img_path, lbl_path or None, labs_set), ...]\n",
    "    ë°˜í™˜: {split: [item_index, ...]}\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    splits = list(RATIOS.keys())\n",
    "    all_labels = labels + [bg_token]\n",
    "    N = len(items)\n",
    "\n",
    "    # 1) ì‚¬ì´ì¦ˆ íƒ€ê¹ƒ(ìƒ˜í”Œ ìˆ˜)\n",
    "    target_sizes = {s: int(round(N * RATIOS[s])) for s in splits}\n",
    "    while sum(target_sizes.values()) < N:\n",
    "        k = min(target_sizes, key=lambda x: target_sizes[x]); target_sizes[k] += 1\n",
    "    while sum(target_sizes.values()) > N:\n",
    "        k = max(target_sizes, key=lambda x: target_sizes[x]); target_sizes[k] -= 1\n",
    "\n",
    "    # 2) ì „ì—­ ë¼ë²¨ ë¶„í¬ & ë¼ë²¨ íƒ€ê¹ƒ\n",
    "    global_counts = Counter()\n",
    "    for _, _, _, labs in items:\n",
    "        for l in labs:\n",
    "            global_counts[l] += 1\n",
    "    label_targets = {s: {l: global_counts[l] * RATIOS[s] for l in all_labels} for s in splits}\n",
    "\n",
    "    # ìƒíƒœ\n",
    "    split_sizes = {s: 0 for s in splits}\n",
    "    split_label_counts = {s: Counter() for s in splits}\n",
    "    assigned = {s: [] for s in splits}\n",
    "\n",
    "    # splitë³„ ê³¼ì¶©ì¡± ìƒí•œ(ë¼ë²¨): ëª©í‘œ * (1 + LABEL_TOL)\n",
    "    label_caps = {s: {l: label_targets[s][l] * (1.0 + LABEL_TOL) for l in all_labels} for s in splits}\n",
    "\n",
    "    # ì•„ì´í…œ ìˆœíšŒ ìˆœì„œ(ê°•í•œ ì„ê¸°)\n",
    "    order = list(range(N))\n",
    "    random.shuffle(order)\n",
    "    if BLOCK_SIZE > 1:\n",
    "        blocks = [order[i:i+BLOCK_SIZE] for i in range(0, N, BLOCK_SIZE)]\n",
    "        for b in blocks:\n",
    "            random.shuffle(b)\n",
    "        random.shuffle(blocks)\n",
    "        order = [i for b in blocks for i in b]\n",
    "\n",
    "    def split_cost(s, labs):\n",
    "        # ë¼ë²¨ ì˜¤ì°¨ ë³€í™”\n",
    "        label_term = 0.0\n",
    "        for l in labs:\n",
    "            cur = split_label_counts[s][l]\n",
    "            tgt = label_targets[s][l]\n",
    "            label_term += abs((cur + 1) - tgt) - abs(cur - tgt)\n",
    "        # ì‚¬ì´ì¦ˆ ì˜¤ì°¨ ë³€í™”\n",
    "        size_term = abs((split_sizes[s] + 1) - target_sizes[s]) - abs(split_sizes[s] - target_sizes[s])\n",
    "        # ë¬´ì‘ìœ„ ë…¸ì´ì¦ˆ(ê·¼ì ‘ì¼ ë•Œ ë¬´ì‘ìœ„ë¡œ ê°ˆë¼ì§€ê²Œ)\n",
    "        noise = random.uniform(0.0, JITTER)\n",
    "        return label_term + SIZE_WEIGHT * size_term + noise\n",
    "\n",
    "    for idx in order:\n",
    "        stem, img, lbl, labs = items[idx]\n",
    "\n",
    "        # 1) ì•„ì§ ì‚¬ì´ì¦ˆ ì—¬ìœ  ìˆëŠ” splitë§Œ 1ì°¨ í›„ë³´\n",
    "        cand = [s for s in splits if split_sizes[s] < target_sizes[s]]\n",
    "        if not cand:\n",
    "            cand = splits[:]  # ì „ë¶€ ì°¼ìœ¼ë©´ ì „ë¶€ í—ˆìš©\n",
    "\n",
    "        # 2) ë¼ë²¨ ê³¼ì¶©ì¡± ì œì•½(í•´ë‹¹ ì•„ì´í…œì˜ ë¼ë²¨ ì¤‘ í•˜ë‚˜ë¼ë„ capì„ ë„˜ê¸°ë©´ ì œì™¸)\n",
    "        feasible = []\n",
    "        for s in cand:\n",
    "            ok = True\n",
    "            for l in labs:\n",
    "                cap = label_caps[s].get(l, float(\"inf\"))\n",
    "                if split_label_counts[s][l] + 1 > cap and cap > 0:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                feasible.append(s)\n",
    "        if not feasible:\n",
    "            feasible = cand  # ëª¨ë‘ ë§‰íˆë©´ ì™„í™”\n",
    "\n",
    "        # 3) ë¹„ìš© ìµœì†Œ split ì„ íƒ\n",
    "        best_s, best_c = None, 1e18\n",
    "        for s in feasible:\n",
    "            c = split_cost(s, labs)\n",
    "            if c < best_c:\n",
    "                best_c = c\n",
    "                best_s = s\n",
    "\n",
    "        assigned[best_s].append(idx)\n",
    "        split_sizes[best_s] += 1\n",
    "        for l in labs:\n",
    "            split_label_counts[best_s][l] += 1\n",
    "\n",
    "    return assigned\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ì´ë™ & ë¦¬í¬íŠ¸\n",
    "# ------------------------------------------------------------\n",
    "def move_into_structure(ROOT: Path, assigned, items):\n",
    "    ensure_dirs(ROOT)\n",
    "\n",
    "    def mv(src: Path, dst: Path):\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if src.resolve() == dst.resolve():\n",
    "            return\n",
    "        shutil.move(str(src), str(dst))\n",
    "\n",
    "    for split, idxs in assigned.items():\n",
    "        for i in idxs:\n",
    "            stem, img, lbl, _ = items[i]\n",
    "            dst_img = ROOT / \"images\" / split / img.name\n",
    "            dst_lbl = ROOT / \"labels\" / split / f\"{stem}.txt\"\n",
    "            mv(img, dst_img)\n",
    "            if lbl and lbl.exists():\n",
    "                mv(lbl, dst_lbl)\n",
    "\n",
    "    # ê°„ë‹¨ ë¦¬í¬íŠ¸\n",
    "    counts = {s: len(idxs) for s, idxs in assigned.items()}\n",
    "    print(f\"  -> moved: train {counts.get('train',0)}, val {counts.get('val',0)}, test {counts.get('test',0)}\")\n",
    "\n",
    "def summarize_split(ROOT: Path):\n",
    "    \"\"\"ê° splitë³„ ì´ë¯¸ì§€ ìˆ˜Â·ë¼ë²¨ ë¶„í¬ ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "    from collections import defaultdict\n",
    "    def list_images(p: Path):\n",
    "        return [x for x in p.rglob(\"*\") if x.suffix in IMG_EXTS]\n",
    "\n",
    "    for s in RATIOS.keys():\n",
    "        img_dir = ROOT / \"images\" / s\n",
    "        lbl_dir = ROOT / \"labels\" / s\n",
    "        if not img_dir.exists():\n",
    "            continue\n",
    "        imgs = list_images(img_dir)\n",
    "        stems_img = {p.stem for p in imgs}\n",
    "        stems_lbl = {p.stem for p in lbl_dir.rglob(\"*.txt\")} if lbl_dir.exists() else set()\n",
    "        bg = len(stems_img - stems_lbl)\n",
    "\n",
    "        cls_cnt = Counter()\n",
    "        if lbl_dir.exists():\n",
    "            for txt in lbl_dir.rglob(\"*.txt\"):\n",
    "                for line in txt.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "                    if not line.strip(): continue\n",
    "                    parts = line.split()\n",
    "                    try: cid = int(float(parts[0]))\n",
    "                    except: continue\n",
    "                    cls_cnt[cid] += 1\n",
    "\n",
    "        print(f\"  [{s}] images={len(imgs)}  BG(no-label)={bg}  objs={sum(cls_cnt.values())}  per-class={dict(sorted(cls_cnt.items()))}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ì‹¤í–‰\n",
    "# ------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    for root in TARGETS:\n",
    "        print(f\"\\n=== Re-structuring in place: {root} ===\")\n",
    "        items = collect_items(root)\n",
    "        if not items:\n",
    "            print(\"  -> No images found. Skip.\")\n",
    "            continue\n",
    "        assigned = balanced_interleave_stratify(items, labels=LABELS, bg_token=BG, seed=SEED)\n",
    "        move_into_structure(root, assigned, items)\n",
    "        summarize_split(root)\n",
    "        print(\"âœ… Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1aa68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_by_sequence_change.py\n",
    "# - ìˆœì°¨ ìŠ¤ìº” ê¸°ë°˜ ì»· ì „í™˜(í´ëŸ¬ìŠ¤í„° ë¶„í• ): ì—°ì† í”„ë ˆì„ ê°„ ë³€í™”ê°€ ì»¤ì§ˆ ë•Œ ìƒˆ í´ëŸ¬ìŠ¤í„° ì‹œì‘\n",
    "# - ë³€í™”ëŸ‰ = w1*(1-SSIM) + w2*HSV íˆìŠ¤í† ê·¸ë¨ ê±°ë¦¬ + w3*(1-ORB ë§¤ì¹­ë¹„)\n",
    "# - íˆìŠ¤í…Œë¦¬ì‹œìŠ¤(THR_START > THR_CONT) + ìµœì†Œ í´ëŸ¬ìŠ¤í„° ê¸¸ì´ë¡œ ì•ˆì •í™”\n",
    "# - â˜… ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ê¸¸ì´ ìº¡: MAX_CLUSTER_LEN ì´ˆê³¼ ì‹œ ê°•ì œ ë¶„í• \n",
    "# - ì´ë™/ë¶„í•  ì—†ìŒ: ê° í´ë”ë³„ cluster_map.tsv ì €ì¥ + í†µê³„ ì¶œë ¥\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ===== ëŒ€ìƒ í´ë” =====\n",
    "TARGETS = [\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250925_merge_data/20250721_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250925_merge_data/20250725_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250925_merge_data/20250904_good_data\"),\n",
    "]\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"}\n",
    "SAVE_MAP = True\n",
    "\n",
    "# ===== ë°ì´í„°ì…‹ë³„ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ê¸¸ì´(ì„ íƒ) & ê¸°ë³¸ê°’ =====\n",
    "# í‚¤ëŠ” í´ë”ëª…(root.name). ì§€ì •ì´ ì—†ìœ¼ë©´ DEFAULT_MAX_LEN ì‚¬ìš©.\n",
    "PER_DATASET_MAX_LEN = {\n",
    "    \"20250721_good_data\": 5,\n",
    "    \"20250725_good_data\": 5,\n",
    "    \"20250904_good_data\": 12,\n",
    "}\n",
    "DEFAULT_MAX_LEN = 10\n",
    "\n",
    "# ===== íŠ¹ì§•/ê±°ë¦¬ ê³„ì‚° íŒŒë¼ë¯¸í„° =====\n",
    "RESIZE_WH = (512, 288)   # ë¹„êµìš© ë¦¬ì‚¬ì´ì¦ˆ(ê°€ì† + ë…¸ì´ì¦ˆì™„í™”)\n",
    "HIST_BINS = (32, 32, 32) # HSV íˆìŠ¤í† ê·¸ë¨ bin\n",
    "HIST_RANGE = [0,180, 0,256, 0,256]\n",
    "ORB_N = 800              # ORB keypoints ìƒí•œ\n",
    "\n",
    "# ===== ì»· ì „í™˜(í´ëŸ¬ìŠ¤í„° ë¶„í• ) íŒŒë¼ë¯¸í„° =====\n",
    "W_SSIM = 0.55\n",
    "W_HIST = 0.35\n",
    "W_ORB  = 0.10            # ëŠë¦¬ë©´ 0.0ìœ¼ë¡œ êº¼ë„ ë¨\n",
    "\n",
    "THR_START = 0.48         # ë³€í™”ëŸ‰ì´ ì´ ê°’ ì´ìƒì´ë©´ ìƒˆ í´ëŸ¬ìŠ¤í„° ì‹œì‘\n",
    "THR_CONT  = 0.38         # í´ëŸ¬ìŠ¤í„° ìœ ì§€(ì „í™˜ ì·¨ì†Œ) í•˜í•œ(íˆìŠ¤í…Œë¦¬ì‹œìŠ¤)\n",
    "MIN_CLUSTER_LEN = 1      # ë„ˆë¬´ ìì£¼ ëŠê¸° ë°©ì§€\n",
    "\n",
    "# íŒŒì¼ëª… ìˆ«ì ì í”„ì— ë”°ë¥¸ ë³´ì •(ì„ íƒ)\n",
    "FNAME_NUM_PATTERN = re.compile(r\"(\\d+)\")\n",
    "NUM_JUMP_BOOST = 0.10\n",
    "NUM_JUMP_K = 100\n",
    "\n",
    "# -------------------------------\n",
    "# ìœ í‹¸\n",
    "# -------------------------------\n",
    "def natural_key(p: Path):\n",
    "    s = str(p).lower()\n",
    "    return [int(t) if t.isdigit() else t for t in re.findall(r'\\d+|\\D+', s)]\n",
    "\n",
    "def list_images_sorted(ROOT: Path):\n",
    "    cands = []\n",
    "    for ext in IMG_EXTS:\n",
    "        for img in ROOT.rglob(f\"*{ext}\"):\n",
    "            rel = img.relative_to(ROOT)\n",
    "            if rel.parts and rel.parts[0] in (\"images\", \"labels\"):\n",
    "                continue\n",
    "            cands.append(img)\n",
    "    cands.sort(key=natural_key)\n",
    "    return cands\n",
    "\n",
    "def load_and_preprocess(p: Path):\n",
    "    arr = cv2.imdecode(np.fromfile(str(p), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "    if arr is None:\n",
    "        return None, None, None\n",
    "    arr = cv2.resize(arr, RESIZE_WH, interpolation=cv2.INTER_AREA)\n",
    "    gray = cv2.cvtColor(arr, cv2.COLOR_BGR2GRAY)\n",
    "    hsv  = cv2.cvtColor(arr, cv2.COLOR_BGR2HSV)\n",
    "    return arr, gray, hsv\n",
    "\n",
    "def ssim_gray(a, b):\n",
    "    C1 = (0.01*255)**2\n",
    "    C2 = (0.03*255)**2\n",
    "    a = a.astype(np.float32); b = b.astype(np.float32)\n",
    "    mu1 = cv2.GaussianBlur(a, (11,11), 1.5)\n",
    "    mu2 = cv2.GaussianBlur(b, (11,11), 1.5)\n",
    "    mu1_sq = mu1*mu1; mu2_sq = mu2*mu2; mu12 = mu1*mu2\n",
    "    sigma1_sq = cv2.GaussianBlur(a*a, (11,11), 1.5) - mu1_sq\n",
    "    sigma2_sq = cv2.GaussianBlur(b*b, (11,11), 1.5) - mu2_sq\n",
    "    sigma12 = cv2.GaussianBlur(a*b, (11,11), 1.5) - mu12\n",
    "    ssim_map = ((2*mu12 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2) + 1e-8)\n",
    "    return float(np.clip(ssim_map.mean(), -1.0, 1.0))\n",
    "\n",
    "def hist_dist_hsv(hsv1, hsv2):\n",
    "    h1 = cv2.calcHist([hsv1],[0,1,2],None,HIST_BINS,HIST_RANGE)\n",
    "    h2 = cv2.calcHist([hsv2],[0,1,2],None,HIST_BINS,HIST_RANGE)\n",
    "    cv2.normalize(h1, h1, 1, 0, cv2.NORM_L1)\n",
    "    cv2.normalize(h2, h2, 1, 0, cv2.NORM_L1)\n",
    "    sim = cv2.compareHist(h1, h2, cv2.HISTCMP_BHATTACHARYYA)\n",
    "    return float(np.clip(sim, 0.0, 1.0))\n",
    "\n",
    "def orb_mismatch_ratio(img1, img2):\n",
    "    orb = cv2.ORB_create(nfeatures=ORB_N)\n",
    "    k1, d1 = orb.detectAndCompute(img1, None)\n",
    "    k2, d2 = orb.detectAndCompute(img2, None)\n",
    "    if d1 is None or d2 is None or len(k1) < 10 or len(k2) < 10:\n",
    "        return 1.0\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(d1, d2)\n",
    "    if not matches:\n",
    "        return 1.0\n",
    "    matches = sorted(matches, key=lambda m: m.distance)\n",
    "    keep = matches[:max(20, len(matches)//2)]\n",
    "    good = sum(1 for m in keep if m.distance <= 70)\n",
    "    ratio = good / max(1, len(keep))\n",
    "    return float(1.0 - ratio)\n",
    "\n",
    "def fname_num(p: Path):\n",
    "    m = FNAME_NUM_PATTERN.findall(p.stem)\n",
    "    if not m: return None\n",
    "    try: return int(m[-1])\n",
    "    except: return None\n",
    "\n",
    "def change_score(prev, curr):\n",
    "    b1, g1, h1 = prev\n",
    "    b2, g2, h2 = curr\n",
    "    ssim = ssim_gray(g1, g2)\n",
    "    d_ssim = 1.0 - ((ssim + 1.0) / 2.0)\n",
    "    d_hist = hist_dist_hsv(h1, h2)\n",
    "    d_orb = orb_mismatch_ratio(b1, b2) if W_ORB > 0.0 else 0.0\n",
    "    return float(W_SSIM*d_ssim + W_HIST*d_hist + W_ORB*d_orb)\n",
    "\n",
    "# -------------------------------\n",
    "# ìˆœì°¨ ì»· ì „í™˜ ë¶„í•  (+ ìµœëŒ€ ê¸¸ì´ ìº¡)\n",
    "# -------------------------------\n",
    "def segment_sequence(images, max_cluster_len):\n",
    "    preps = []\n",
    "    nums  = []\n",
    "    for p in images:\n",
    "        bgr, gray, hsv = load_and_preprocess(p)\n",
    "        preps.append((bgr, gray, hsv))\n",
    "        nums.append(fname_num(p))\n",
    "\n",
    "    clusters = []\n",
    "    cur = []\n",
    "    state_high = False\n",
    "\n",
    "    def flush():\n",
    "        if cur:\n",
    "            clusters.append(cur.copy())\n",
    "            cur.clear()\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        if preps[i][0] is None:\n",
    "            flush()\n",
    "            clusters.append([i])\n",
    "            state_high = False\n",
    "            continue\n",
    "\n",
    "        if not cur:\n",
    "            cur.append(i)\n",
    "            continue\n",
    "\n",
    "        # ìµœëŒ€ ê¸¸ì´ ìº¡ ìš°ì„  ì ìš©: ì´ˆê³¼ ì§ì „ì— ì»·\n",
    "        if len(cur) >= max_cluster_len:\n",
    "            flush()\n",
    "            cur.append(i)\n",
    "            state_high = False\n",
    "            continue\n",
    "\n",
    "        # ë³€í™”ëŸ‰ ê³„ì‚°(i-1 -> i)\n",
    "        score = change_score(preps[i-1], preps[i])\n",
    "\n",
    "        # íŒŒì¼ëª… ìˆ«ì ì í”„ ë³´ì •\n",
    "        if nums[i-1] is not None and nums[i] is not None:\n",
    "            if nums[i] - nums[i-1] > NUM_JUMP_K:\n",
    "                score += NUM_JUMP_BOOST\n",
    "\n",
    "        if not state_high:\n",
    "            if score >= THR_START and len(cur) >= MIN_CLUSTER_LEN:\n",
    "                flush()\n",
    "                cur.append(i)\n",
    "                state_high = True\n",
    "            else:\n",
    "                cur.append(i)\n",
    "        else:\n",
    "            # ì „í™˜ ìƒíƒœ í•´ì œ\n",
    "            if score <= THR_CONT:\n",
    "                state_high = False\n",
    "            # ìƒˆ í´ëŸ¬ìŠ¤í„° ì‹œì‘ ìƒíƒœì´ë¯€ë¡œ í˜„ì¬ iëŠ” ì´ë¯¸ curì— í¬í•¨ë¨\n",
    "\n",
    "    flush()\n",
    "    return clusters\n",
    "\n",
    "def print_stats(ROOT: Path, images, clusters, max_len_used):\n",
    "    n_items = len(images)\n",
    "    n_cl = len(clusters)\n",
    "    avg_sz = n_items / max(1, n_cl)\n",
    "    size_counts = Counter(len(c) for c in clusters)\n",
    "    buckets = [(i, size_counts.get(i, 0)) for i in range(1, 11)]\n",
    "    ge11 = sum(v for k, v in size_counts.items() if k >= 11)\n",
    "\n",
    "    print(f\"\\n=== Sequential change-based clusters: {ROOT} ===\")\n",
    "    print(f\"  -> MAX_CLUSTER_LEN = {max_len_used}\")\n",
    "    print(f\"  -> items: {n_items}, clusters: {n_cl} (avg size ~ {avg_sz:.2f})\")\n",
    "    line = \"  -> size histogram: \" + \"  \".join([f\"{k}:{v}\" for k, v in buckets])\n",
    "    if ge11: line += f\"  11+:{ge11}\"\n",
    "    print(line)\n",
    "\n",
    "    top = sorted(range(n_cl), key=lambda c: -len(clusters[c]))[:5]\n",
    "    for rank, cid in enumerate(top, 1):\n",
    "        sz = len(clusters[cid])\n",
    "        samples = [str(images[i].relative_to(ROOT)) for i in clusters[cid][:3]]\n",
    "        print(f\"  -> top#{rank}: cid={cid+1:03d}, size={sz}, samples={samples}\")\n",
    "\n",
    "    if SAVE_MAP:\n",
    "        out = ROOT / \"cluster_map.tsv\"\n",
    "        with out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"cluster_id\\timage_relpath\\n\")\n",
    "            for cid, cl in enumerate(clusters, start=1):\n",
    "                for idx in cl:\n",
    "                    rel = images[idx].relative_to(ROOT)\n",
    "                    f.write(f\"{cid}\\t{rel}\\n\")\n",
    "        print(f\"  -> saved: {out}\")\n",
    "\n",
    "# -------------------------------\n",
    "# ì‹¤í–‰\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    for root in TARGETS:\n",
    "        imgs = list_images_sorted(root)\n",
    "        if not imgs:\n",
    "            print(f\"\\n=== Sequential change-based clusters: {root} ===\")\n",
    "            print(\"  -> No images found. Skip.\")\n",
    "            continue\n",
    "\n",
    "        # ë°ì´í„°ì…‹ë³„ ìµœëŒ€ ê¸¸ì´ ì„ íƒ\n",
    "        max_len = PER_DATASET_MAX_LEN.get(root.name, DEFAULT_MAX_LEN)\n",
    "        clusters = segment_sequence(imgs, max_len)\n",
    "        print_stats(root, imgs, clusters, max_len)\n",
    "\n",
    "    print(\"\\nâœ… Analysis only (max cluster length cap enabled)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9d43a",
   "metadata": {},
   "source": [
    "### í´ëŸ¬ìŠ¤í„° csv ë§Œë“¤ê¸° _ by LBP + ZNCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ìˆœì°¨ ìŠ¤ìº”: ì¡°ëª… ì˜í–¥ ì œê±°(ë°ê¸°/ëŒ€ë¹„ ë¶ˆë³€) ê¸°ì¤€ + ë“œë¦¬í”„íŠ¸ ì–µì œ(ì•µì»¤ ë¹„êµ) + í´ë”ë³„ ìµœëŒ€ ê¸¸ì´ ì œí•œ\n",
    "- íŒŒì¼ëª… ìì—°ì •ë ¬\n",
    "- LBP íˆìŠ¤í† ê·¸ë¨ + ZNCC ì‚¬ìš© (ë°ê¸°/ëŒ€ë¹„ ë³€í™”ì— ë‘”ê°)\n",
    "- change = GAMMA * d_lbp + (1-GAMMA) * (1-zncc)\n",
    "- (prev ë˜ëŠ” anchor ê¸°ì¤€ change) >= CHANGE_THRESH ì´ë©´ ìƒˆ ê·¸ë£¹\n",
    "- í´ë”ë³„ íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ(FOLDER_CFG) + í´ë”ë³„ MAX_CLUSTER_LEN\n",
    "- MAX_CLUSTER_LEN ë„ë‹¬ ì‹œ ìœ ì‚¬ë„ì™€ ë¬´ê´€í•˜ê²Œ ìƒˆ í´ëŸ¬ìŠ¤í„° ì‹œì‘(ì•µì»¤ ê°±ì‹ )\n",
    "\n",
    "CSV: clusters_invariant_<basename>.csv\n",
    "  filename, cluster_id, index_in_folder,\n",
    "  d_lbp_prev, zncc_prev, change_prev,\n",
    "  d_lbp_anchor, zncc_anchor, change_anchor,\n",
    "  is_cluster_start, cluster_len_so_far, reason\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import csv, re\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "# ====== ëŒ€ìƒ í´ë” ======\n",
    "FOLDERS = [\n",
    "    #Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250721_good_data\"),\n",
    "    #Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250725_good_data\"),\n",
    "    #Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250904_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250930_good_data\"),\n",
    "]\n",
    "\n",
    "# ====== í´ë”ë³„ íŒŒë¼ë¯¸í„°(ì—†ìœ¼ë©´ DEFAULT_CFG ì‚¬ìš©) ======\n",
    "# - GAMMA: LBP ë¹„ì¤‘(0~1). ë‚®ì¶œìˆ˜ë¡ ZNCC ë¹„ì¤‘â†‘ â†’ êµ¬ì¡° ì°¨ì´ì— ë” ë¯¼ê°\n",
    "# - CHANGE_THRESH: ì»· ì „í™˜ ì„ê³„ (ë‚®ì„ìˆ˜ë¡ ë” ì˜ ëŠê¹€)\n",
    "# - GAUSS_BLUR: LBP/ZNCC ì „ì²˜ë¦¬ ë¸”ëŸ¬ ì»¤ë„ (ì‘ì„ìˆ˜ë¡ ë¯¼ê°)\n",
    "# - ZNCC_SIZE: ZNCC ê³„ì‚° ë¦¬ì‚¬ì´ì¦ˆ í•œ ë³€\n",
    "# - MAX_CLUSTER_LEN: í˜„ì¬ í´ëŸ¬ìŠ¤í„° ìµœëŒ€ ê¸¸ì´(ì´ˆê³¼ ì‹œ ê°•ì œ ë¶„ë¦¬). ì˜ˆ: 3ì´ë©´ 3ì¥ê¹Œì§€ ê°™ì€ ê·¸ë£¹.\n",
    "FOLDER_CFG: Dict[str, Dict] = {\n",
    "    \"20250721_good_data\": {\n",
    "        \"GAMMA\": 0.60,\n",
    "        \"CHANGE_THRESH\": 0.32,\n",
    "        \"GAUSS_BLUR\": (3,3),\n",
    "        \"ZNCC_SIZE\": 256,\n",
    "        \"MAX_CLUSTER_LEN\": 5,\n",
    "    },\n",
    "    \"20250725_good_data\": {\n",
    "        \"GAMMA\": 0.60,\n",
    "        \"CHANGE_THRESH\": 0.35,\n",
    "        \"GAUSS_BLUR\": (3,3),\n",
    "        \"ZNCC_SIZE\": 256,\n",
    "        \"MAX_CLUSTER_LEN\": 5,\n",
    "    },\n",
    "    \"20250904_good_data\": {\n",
    "        \"GAMMA\": 0.55,\n",
    "        \"CHANGE_THRESH\": 0.25,\n",
    "        \"GAUSS_BLUR\": (3,3),\n",
    "        \"ZNCC_SIZE\": 256,\n",
    "        \"MAX_CLUSTER_LEN\": 20,  # ì˜ˆì‹œ: 12ì¥ê¹Œì§€ë§Œ ê°™ì€ ê·¸ë£¹\n",
    "    },\n",
    "    \"20250930_good_data\": {\n",
    "        \"GAMMA\": 0.60,\n",
    "        \"CHANGE_THRESH\": 0.28,\n",
    "        \"GAUSS_BLUR\": (3,3),\n",
    "        \"ZNCC_SIZE\": 256,\n",
    "        \"MAX_CLUSTER_LEN\": 15,  # ì˜ˆì‹œ: 12ì¥ê¹Œì§€ë§Œ ê°™ì€ ê·¸ë£¹\n",
    "    },\n",
    "}\n",
    "\n",
    "# ê¸°ë³¸ê°’ (ìœ„ì— ì—†ìœ¼ë©´ ì´ ê°’ì„ ì‚¬ìš©)\n",
    "DEFAULT_CFG = {\n",
    "    \"GAMMA\": 0.60,\n",
    "    \"CHANGE_THRESH\": 0.20,\n",
    "    \"GAUSS_BLUR\": (3,3),\n",
    "    \"ZNCC_SIZE\": 256,\n",
    "    \"MAX_CLUSTER_LEN\": 5,\n",
    "}\n",
    "\n",
    "# ====== ì´ë¯¸ì§€ í™•ì¥ì ======\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\",\n",
    "            \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"}\n",
    "\n",
    "# ====== LBP ì„¤ì • ======\n",
    "LBP_RADIUS = 1\n",
    "LBP_POINTS = 8\n",
    "LBP_HIST_BINS = 256  # 8-bit LBP\n",
    "\n",
    "# ====== ê¸°íƒ€ ======\n",
    "NATURAL_SORT = True\n",
    "MAX_IMAGES = None\n",
    "DOWNSCALE_IF_NEEDED = False\n",
    "PREPROC_MAX_SIDE = 0   # 0=ì¶•ì†Œ ì•ˆí•¨(ì •í™•ë„ ìš°ì„ )\n",
    "\n",
    "\n",
    "# ---------------- ìœ í‹¸ ----------------\n",
    "def get_cfg(folder: Path) -> Dict:\n",
    "    return {**DEFAULT_CFG, **FOLDER_CFG.get(folder.name, {})}\n",
    "\n",
    "def natural_key(s: str):\n",
    "    if not NATURAL_SORT: return s\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r\"(\\d+)\", s)]\n",
    "\n",
    "def list_images(folder: Path) -> List[Path]:\n",
    "    files = [p for p in folder.iterdir() if p.is_file() and p.suffix in IMG_EXTS]\n",
    "    files.sort(key=lambda p: natural_key(p.name))\n",
    "    return files[:MAX_IMAGES] if MAX_IMAGES else files\n",
    "\n",
    "def resize_longest(img: np.ndarray, max_side: int) -> np.ndarray:\n",
    "    if max_side is None or max_side <= 0: return img\n",
    "    h, w = img.shape[:2]\n",
    "    longest = max(h, w)\n",
    "    if longest <= max_side: return img\n",
    "    scale = max_side / float(longest)\n",
    "    return cv2.resize(img, (int(round(w*scale)), int(round(h*scale))), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def load_bgr(path: Path) -> Optional[np.ndarray]:\n",
    "    img = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "    if img is None: return None\n",
    "    if DOWNSCALE_IF_NEEDED and PREPROC_MAX_SIDE > 0:\n",
    "        img = resize_longest(img, PREPROC_MAX_SIDE)\n",
    "    return img\n",
    "\n",
    "\n",
    "# ---------------- LBP & ZNCC ----------------\n",
    "def lbp_image(gray: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"ê°„ë‹¨ 3x3 LBP (8-bit).\"\"\"\n",
    "    g = gray\n",
    "    c = g[1:-1, 1:-1]\n",
    "    code = np.zeros_like(c, dtype=np.uint8)\n",
    "    nbrs = [\n",
    "        (0, 0),  (0, 1),  (0, 2),\n",
    "        (1, 2),  (2, 2),  (2, 1),\n",
    "        (2, 0),  (1, 0),\n",
    "    ]\n",
    "    for bit, (dy, dx) in enumerate(nbrs):\n",
    "        n = g[dy:dy + c.shape[0], dx:dx + c.shape[1]]\n",
    "        code |= ((n >= c) << (7 - bit)).astype(np.uint8)\n",
    "    return code\n",
    "\n",
    "def lbp_hist(gray: np.ndarray, blur_kernel: Tuple[int,int]) -> np.ndarray:\n",
    "    \"\"\"ì •ê·œí™”ëœ LBP íˆìŠ¤í† ê·¸ë¨(ë°ê¸°/ëŒ€ë¹„ ë³€í™”ì— ë‘”ê°).\"\"\"\n",
    "    if blur_kernel and blur_kernel[0] > 0 and blur_kernel[1] > 0:\n",
    "        gray = cv2.GaussianBlur(gray, blur_kernel, 0)\n",
    "    lbp = lbp_image(gray)\n",
    "    hist = cv2.calcHist([lbp], [0], None, [LBP_HIST_BINS], [0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist\n",
    "\n",
    "def d_bhat(h1: np.ndarray, h2: np.ndarray) -> float:\n",
    "    return float(cv2.compareHist(h1, h2, cv2.HISTCMP_BHATTACHARYYA))\n",
    "\n",
    "def zncc(gray1: np.ndarray, gray2: np.ndarray, size: int, blur_kernel: Tuple[int,int]) -> float:\n",
    "    \"\"\"Zero-mean NCC (ë°ê¸°/ëŒ€ë¹„ ì„ í˜• ë³€í™” ë¶ˆë³€). -1~1, 1=ì™„ì „ ë™ì¼\"\"\"\n",
    "    g1 = cv2.resize(gray1, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    g2 = cv2.resize(gray2, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    if blur_kernel and blur_kernel[0] > 0 and blur_kernel[1] > 0:\n",
    "        g1 = cv2.GaussianBlur(g1, blur_kernel, 0)\n",
    "        g2 = cv2.GaussianBlur(g2, blur_kernel, 0)\n",
    "    g1 = g1.astype(np.float32)\n",
    "    g2 = g2.astype(np.float32)\n",
    "    g1 -= g1.mean(); g2 -= g2.mean()\n",
    "    std1 = g1.std() + 1e-6\n",
    "    std2 = g2.std() + 1e-6\n",
    "    g1 /= std1; g2 /= std2\n",
    "    num = float(np.sum(g1 * g2))\n",
    "    den = float(np.sqrt(np.sum(g1*g1)) * np.sqrt(np.sum(g2*g2)) + 1e-6)\n",
    "    return float(np.clip(num / den, -1.0, 1.0))\n",
    "\n",
    "def gray_of(img_bgr: np.ndarray) -> np.ndarray:\n",
    "    return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# ---------------- ë©”ì¸ ----------------\n",
    "def cluster_folder(folder: Path):\n",
    "    cfg = get_cfg(folder)\n",
    "    GAMMA = float(cfg[\"GAMMA\"])\n",
    "    CHANGE_THRESH = float(cfg[\"CHANGE_THRESH\"])\n",
    "    GAUSS_BLUR = tuple(cfg[\"GAUSS_BLUR\"]) if isinstance(cfg[\"GAUSS_BLUR\"], (list, tuple)) else (3,3)\n",
    "    ZNCC_SIZE = int(cfg[\"ZNCC_SIZE\"])\n",
    "    MAX_CLUSTER_LEN = int(cfg.get(\"MAX_CLUSTER_LEN\", 0))  # 0 ë˜ëŠ” ìŒìˆ˜ë©´ ì œí•œ ì—†ìŒ\n",
    "\n",
    "    imgs = list_images(folder)\n",
    "    if not imgs:\n",
    "        print(f\"[WARN] No images in: {folder}\")\n",
    "        return\n",
    "\n",
    "    img_prev = load_bgr(imgs[0])\n",
    "    if img_prev is None:\n",
    "        print(f\"[WARN] Can't read: {imgs[0]}\")\n",
    "        return\n",
    "\n",
    "    g_prev = gray_of(img_prev)\n",
    "    lbp_prev = lbp_hist(g_prev, GAUSS_BLUR)\n",
    "\n",
    "    # ì•µì»¤(ê·¸ë£¹ ì²« í”„ë ˆì„)\n",
    "    anchor_img = img_prev\n",
    "    g_anchor = g_prev\n",
    "    lbp_anchor = lbp_prev\n",
    "\n",
    "    cluster_id = 0\n",
    "    curr_len = 1  # í˜„ì¬ í´ëŸ¬ìŠ¤í„° ê¸¸ì´(ì²« í”„ë ˆì„ í¬í•¨)\n",
    "\n",
    "    rows = [{\n",
    "        \"filename\": imgs[0].name,\n",
    "        \"cluster_id\": cluster_id,\n",
    "        \"index_in_folder\": 0,\n",
    "        \"d_lbp_prev\": 0.0,\n",
    "        \"zncc_prev\": 1.0,\n",
    "        \"change_prev\": 0.0,\n",
    "        \"d_lbp_anchor\": 0.0,\n",
    "        \"zncc_anchor\": 1.0,\n",
    "        \"change_anchor\": 0.0,\n",
    "        \"is_cluster_start\": 1,\n",
    "        \"cluster_len_so_far\": curr_len,\n",
    "        \"reason\": \"start\",\n",
    "    }]\n",
    "\n",
    "    for idx in range(1, len(imgs)):\n",
    "        path = imgs[idx]\n",
    "        img = load_bgr(path)\n",
    "        if img is None:\n",
    "            print(f\"[WARN] Can't read: {path}\")\n",
    "            continue\n",
    "\n",
    "        g_cur = gray_of(img)\n",
    "        lbp_cur = lbp_hist(g_cur, GAUSS_BLUR)\n",
    "\n",
    "        # prev ê¸°ì¤€\n",
    "        d_prev = d_bhat(lbp_prev, lbp_cur)\n",
    "        z_prev = zncc(g_prev, g_cur, ZNCC_SIZE, GAUSS_BLUR)\n",
    "        change_prev = GAMMA * d_prev + (1.0 - GAMMA) * (1.0 - z_prev)\n",
    "\n",
    "        # anchor ê¸°ì¤€\n",
    "        d_anchor = d_bhat(lbp_anchor, lbp_cur)\n",
    "        z_anchor = zncc(g_anchor, g_cur, ZNCC_SIZE, GAUSS_BLUR)\n",
    "        change_anchor = GAMMA * d_anchor + (1.0 - GAMMA) * (1.0 - z_anchor)\n",
    "\n",
    "        # ê¸¸ì´ ì œí•œ ì²´í¬ (í˜„ì¬ í´ëŸ¬ìŠ¤í„°ê°€ ì´ë¯¸ MAXì— ë„ë‹¬í–ˆëŠ”ì§€)\n",
    "        limit_hit = (MAX_CLUSTER_LEN > 0) and (curr_len >= MAX_CLUSTER_LEN)\n",
    "\n",
    "        # ì»· íŒì •: ê¸¸ì´ ì œí•œ ìš°ì„ , ì•„ë‹ˆë©´ ë³€í™” ê¸°ì¤€\n",
    "        if limit_hit or (change_prev >= CHANGE_THRESH) or (change_anchor >= CHANGE_THRESH):\n",
    "            cluster_id += 1\n",
    "            is_start = 1\n",
    "            reason = \"limit\" if limit_hit else \"new\"\n",
    "            curr_len = 1  # ìƒˆ í´ëŸ¬ìŠ¤í„° ê¸¸ì´ ì´ˆê¸°í™”\n",
    "            # ì•µì»¤ ê°±ì‹ \n",
    "            anchor_img = img\n",
    "            g_anchor = g_cur\n",
    "            lbp_anchor = lbp_cur\n",
    "        else:\n",
    "            is_start = 0\n",
    "            reason = \"similar\"\n",
    "            curr_len += 1\n",
    "\n",
    "        rows.append({\n",
    "            \"filename\": path.name,\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"index_in_folder\": idx,\n",
    "            \"d_lbp_prev\": round(d_prev, 6),\n",
    "            \"zncc_prev\": round(z_prev, 6),\n",
    "            \"change_prev\": round(change_prev, 6),\n",
    "            \"d_lbp_anchor\": round(d_anchor, 6),\n",
    "            \"zncc_anchor\": round(z_anchor, 6),\n",
    "            \"change_anchor\": round(change_anchor, 6),\n",
    "            \"is_cluster_start\": is_start,\n",
    "            \"cluster_len_so_far\": curr_len,\n",
    "            \"reason\": reason,\n",
    "        })\n",
    "\n",
    "        # ë‹¤ìŒ ë¹„êµ ê¸°ì¤€ ì—…ë°ì´íŠ¸\n",
    "        img_prev = img\n",
    "        g_prev = g_cur\n",
    "        lbp_prev = lbp_cur\n",
    "\n",
    "    out_csv = folder.parent / f\"clusters_invariant_{folder.name}.csv\"\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\n",
    "            \"filename\",\"cluster_id\",\"index_in_folder\",\n",
    "            \"d_lbp_prev\",\"zncc_prev\",\"change_prev\",\n",
    "            \"d_lbp_anchor\",\"zncc_anchor\",\"change_anchor\",\n",
    "            \"is_cluster_start\",\"cluster_len_so_far\",\"reason\"\n",
    "        ])\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "\n",
    "    n_clusters = (rows[-1][\"cluster_id\"] if rows else -1) + 1\n",
    "    print(f\"[OK] {folder.name}: {len(rows)} images â†’ {n_clusters} clusters \"\n",
    "          f\"(GAMMA={GAMMA}, TH={CHANGE_THRESH}, BLUR={GAUSS_BLUR}, ZNCC={ZNCC_SIZE}, MAXLEN={MAX_CLUSTER_LEN}) \"\n",
    "          f\"â†’ {out_csv}\")\n",
    "\n",
    "def main():\n",
    "    for folder in FOLDERS:\n",
    "        if not folder.exists():\n",
    "            print(f\"[WARN] Not found: {folder}\")\n",
    "            continue\n",
    "        cluster_folder(folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30832892",
   "metadata": {},
   "source": [
    "### csvë¡œ í•´ë‹¹í´ë”ì•ˆì— ìˆëŠ” íŒŒì¼ë“¤ ì´ë¦„ ë°”ê¾¸ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "CSV_DIR = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data\")\n",
    "CSV_GLOB = \"clusters_invariant_*.csv\"\n",
    "IMG_ROOT = CSV_DIR\n",
    "\n",
    "APPLY_CHANGES = True # ì‹¤ì œ ì ìš©í•˜ë ¤ë©´ True\n",
    "\n",
    "# suffix: \"_cluster_<id>\"\n",
    "def make_suffix(cluster_id: int) -> str:\n",
    "    return f\"_cluster_{cluster_id}\"\n",
    "\n",
    "# ì´ë¯¸ ë¶™ì–´ìˆëŠ”ì§€ ì²´í¬ (basename ëì´ _cluster_ìˆ«ì)\n",
    "SUFFIX_RE = re.compile(r\"_cluster_(\\d+)$\", re.IGNORECASE)\n",
    "\n",
    "def with_seq(p: Path, k: int) -> Path:\n",
    "    return p.with_name(f\"{p.stem}__{k}{p.suffix}\")\n",
    "\n",
    "def parse_folder_from_csv(csv_path: Path) -> Path:\n",
    "    m = re.match(r\"clusters_invariant_(.+)\\.csv$\", csv_path.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"CSV ì´ë¦„ í˜•ì‹ì´ ì•„ë‹˜: {csv_path.name}\")\n",
    "    folder_name = m.group(1)\n",
    "    return IMG_ROOT / folder_name\n",
    "\n",
    "def ensure_nonconflicting(target: Path) -> Path:\n",
    "    if not target.exists():\n",
    "        return target\n",
    "    k = 2\n",
    "    cand = with_seq(target, k)\n",
    "    while cand.exists():\n",
    "        k += 1\n",
    "        cand = with_seq(target, k)\n",
    "    return cand\n",
    "\n",
    "def plan_for_csv(csv_path: Path):\n",
    "    folder = parse_folder_from_csv(csv_path)\n",
    "    if not folder.exists():\n",
    "        print(f\"[WARN] í´ë” ì—†ìŒ: {folder}  (CSV={csv_path.name})\")\n",
    "        return []\n",
    "\n",
    "    plans = []  # list of dicts: {old_jpg, new_jpg, old_txt, new_txt}\n",
    "    with csv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if \"filename\" not in reader.fieldnames or \"cluster_id\" not in reader.fieldnames:\n",
    "            raise ValueError(f\"[ERR] CSVì— filename/cluster_id ì—†ìŒ: {csv_path}\")\n",
    "\n",
    "        for row in reader:\n",
    "            fn = row[\"filename\"]\n",
    "            cid = int(row[\"cluster_id\"])\n",
    "\n",
    "            # jpgë§Œ ì²˜ë¦¬\n",
    "            if not fn.lower().endswith(\".jpg\"):\n",
    "                continue\n",
    "\n",
    "            jpg_path = folder / fn\n",
    "            if not jpg_path.exists():\n",
    "                continue\n",
    "\n",
    "            stem = jpg_path.stem\n",
    "            # ì´ë¯¸ _cluster_xxx ë¶™ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ\n",
    "            if SUFFIX_RE.search(stem):\n",
    "                continue\n",
    "\n",
    "            txt_path = folder / f\"{stem}.txt\"\n",
    "            suffix = make_suffix(cid)\n",
    "            new_stem = f\"{stem}{suffix}\"\n",
    "\n",
    "            new_jpg = jpg_path.with_name(f\"{new_stem}{jpg_path.suffix}\")\n",
    "            new_txt = (txt_path.with_name(f\"{new_stem}{txt_path.suffix}\")\n",
    "                       if txt_path.exists() else None)\n",
    "\n",
    "            plans.append({\n",
    "                \"old_jpg\": jpg_path,\n",
    "                \"new_jpg\": new_jpg,\n",
    "                \"old_txt\": txt_path if txt_path.exists() else None,\n",
    "                \"new_txt\": new_txt\n",
    "            })\n",
    "    return plans\n",
    "\n",
    "def run():\n",
    "    csvs = sorted(CSV_DIR.glob(CSV_GLOB))\n",
    "    if not csvs:\n",
    "        print(f\"[WARN] CSV ì—†ìŒ: {CSV_DIR}/{CSV_GLOB}\")\n",
    "        return\n",
    "\n",
    "    total_sets = 0\n",
    "    for csv_path in csvs:\n",
    "        plans = plan_for_csv(csv_path)\n",
    "        if not plans:\n",
    "            print(f\"[INFO] ë³€ê²½ ê³„íš ì—†ìŒ: {csv_path.name}\")\n",
    "            continue\n",
    "\n",
    "        folder = parse_folder_from_csv(csv_path)\n",
    "        print(f\"\\n[PLAN] {csv_path.name} â†’ í´ë”: {folder}\")\n",
    "        for p in plans:\n",
    "            old_jpg, new_jpg = p[\"old_jpg\"], p[\"new_jpg\"]\n",
    "            old_txt, new_txt = p[\"old_txt\"], p[\"new_txt\"]\n",
    "\n",
    "            safe_new_jpg = ensure_nonconflicting(new_jpg)\n",
    "            safe_new_txt = ensure_nonconflicting(new_txt) if new_txt else None\n",
    "\n",
    "            # ì¶œë ¥(ê³„íš)\n",
    "            if old_txt and safe_new_txt:\n",
    "                print(f\"  - JPG: {old_jpg.name} â†’ {safe_new_jpg.name}\")\n",
    "                print(f\"    TXT: {old_txt.name} â†’ {safe_new_txt.name}\")\n",
    "            else:\n",
    "                print(f\"  - JPG: {old_jpg.name} â†’ {safe_new_jpg.name} (txt ì—†ìŒ, jpgë§Œ ë³€ê²½)\")\n",
    "\n",
    "            if APPLY_CHANGES:\n",
    "                # txtê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ê°™ì´ ë³€ê²½\n",
    "                if old_txt and safe_new_txt:\n",
    "                    old_txt.rename(safe_new_txt)\n",
    "                    # txt ë³€ê²½ í›„ jpg íƒ€ê²Ÿ ì¶©ëŒ ì¬í™•ì¸\n",
    "                    safe_new_jpg = ensure_nonconflicting(safe_new_jpg)\n",
    "                    old_jpg.rename(safe_new_jpg)\n",
    "                else:\n",
    "                    # txtê°€ ì—†ìœ¼ë©´ jpgë§Œ ë³€ê²½\n",
    "                    old_jpg.rename(safe_new_jpg)\n",
    "                total_sets += 1\n",
    "\n",
    "    if APPLY_CHANGES:\n",
    "        print(f\"\\n[OK] ì‹¤ì œ ë³€ê²½ ì™„ë£Œ. ì´ {total_sets}ê±´ ì²˜ë¦¬.\")\n",
    "    else:\n",
    "        print(\"\\n[DRY-RUN] ì‹¤ì œ ë³€ê²½ ì—†ìŒ. APPLY_CHANGES=True ë¡œ ì ìš©í•˜ì„¸ìš”.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54db97",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ë¶„í• í•˜ê¸°_1 ì¼ë‹¨ 0 1 2 ìœ„ì£¼ë¡œ clusterë³„ë¡œ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "FOLDERS = [\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250721_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250725_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250904_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250930_good_data\"),\n",
    "]\n",
    "\n",
    "# ë¦¬í¬íŠ¸ ì €ì¥ ë””ë ‰í„°ë¦¬(ê° í´ë”ì˜ ìƒìœ„ ê²½ë¡œ)\n",
    "REPORT_DIR = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data\")\n",
    "\n",
    "# split ë¹„ìœ¨ (0=train, 1=val, 2=test)\n",
    "RATIOS = {0: 0.70, 1: 0.18, 2: 0.12}\n",
    "SPLIT_NAME = {0: \"train\", 1: \"val\", 2: \"test\"}\n",
    "SEED = 13\n",
    "\n",
    "# ë¶„í¬ ìœ ì§€(ê· í˜•)ì— ì“¸ í´ë˜ìŠ¤ / ë¦¬í¬íŒ… í´ë˜ìŠ¤\n",
    "BALANCE_CLASS_IDS = [0, 1, 2]         # ê· í˜• ìœ ì§€ìš©\n",
    "ALL_CLASS_IDS = list(range(8))        # ë¦¬í¬íŠ¸ ì¶œë ¥ìš©\n",
    "\n",
    "# ì…ë ¥ íŒŒì¼ íŒ¨í„´ (_cluster_<id> ì „ì œ)\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\",\n",
    "            \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"}\n",
    "CLUSTER_RE = re.compile(r\"^(?P<stem>.+)_cluster_(?P<cid>\\d+)(?:__\\d+)?$\", re.IGNORECASE)\n",
    "\n",
    "# ì‹¤ì œ ë¶„í•  ìˆ˜í–‰\n",
    "DO_MATERIALIZE = True\n",
    "# \"move\" | \"copy\" | \"symlink\"\n",
    "MATERIALIZE_MODE = \"move\"\n",
    "# ê¸°ì¡´ images/labels ë‚´ train/val/test ë””ë ‰í„°ë¦¬ ìˆìœ¼ë©´ ë¹„ìš°ê³  ë‹¤ì‹œ ë§Œë“¤ê¸°\n",
    "CLEAN_PREVIOUS = True\n",
    "# ë¼ë²¨ì´ ì—†ì„ ë•Œ ë¹ˆ txtë¥¼ ìƒì„±í• ì§€ ì—¬ë¶€ (False => ìƒì„±í•˜ì§€ ì•ŠìŒ)\n",
    "CREATE_EMPTY_LABEL_IF_MISSING = False\n",
    "\n",
    "# ìŠ¤ì½”ì–´ ê°€ì¤‘ì¹˜: ì´ë¯¸ì§€/í´ë˜ìŠ¤/bg(ë¬´ë¼ë²¨ ì´ë¯¸ì§€ ê°œìˆ˜)\n",
    "IMG_W = 1.0\n",
    "CLS_W = 1.0\n",
    "BG_W  = 1.0\n",
    "\n",
    "# ---------- ìœ í‹¸ ----------\n",
    "def find_images(folder: Path):\n",
    "    # ë£¨íŠ¸ì— ì„ì—¬ìˆëŠ” ì´ë¯¸ì§€ë§Œ ìŠ¤ìº” (ì´ë¯¸ ë¶„í• ëœ í•˜ìœ„ ë””ë ‰í† ë¦¬ëŠ” ë¬´ì‹œ)\n",
    "    return [p for p in folder.iterdir() if p.is_file() and p.suffix in IMG_EXTS]\n",
    "\n",
    "def parse_cluster_id(stem: str):\n",
    "    m = CLUSTER_RE.match(stem)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    return m.group(\"stem\"), int(m.group(\"cid\"))\n",
    "\n",
    "def read_label_lines(txt_path: Path):\n",
    "    if not txt_path.exists():\n",
    "        return []\n",
    "    try:\n",
    "        with txt_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return [ln.strip() for ln in f if ln.strip()]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def count_classes_from_txt_lines(lines, class_ids):\n",
    "    counts = {k: 0 for k in class_ids}\n",
    "    for s in lines:\n",
    "        parts = s.split()\n",
    "        try:\n",
    "            cid = int(parts[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if cid in counts:\n",
    "            counts[cid] += 1\n",
    "    return counts\n",
    "\n",
    "def build_cluster_stats(folder: Path):\n",
    "    \"\"\"\n",
    "    clusters[cid] = {\n",
    "        \"images\": [Path(img1), ...],\n",
    "        \"class_counts_bal\": {0:x,1:y,2:z},  # ë¶„í•  ê· í˜•ìš©\n",
    "        \"class_counts_all\": {0..7: n},      # ë¦¬í¬íŒ…ìš©\n",
    "        \"bg_count\": int,                    # â˜… ë¬´ë¼ë²¨ ì´ë¯¸ì§€ ìˆ˜ (íŒŒì¼ ì—†ìŒ ë˜ëŠ” ë¼ì¸ 0)\n",
    "    }\n",
    "    \"\"\"\n",
    "    clusters = {}\n",
    "    for img in find_images(folder):\n",
    "        _, cid = parse_cluster_id(img.stem)\n",
    "        if cid is None:\n",
    "            # _cluster_<id> ì—†ëŠ” íŒŒì¼ì€ ë¶„í•  ëŒ€ìƒì—ì„œ ì œì™¸\n",
    "            continue\n",
    "\n",
    "        txt = img.with_suffix(\".txt\")\n",
    "        lines = read_label_lines(txt)\n",
    "        counts_all = count_classes_from_txt_lines(lines, ALL_CLASS_IDS)\n",
    "        counts_bal = {k: counts_all.get(k, 0) for k in BALANCE_CLASS_IDS}\n",
    "        is_bg = (len(lines) == 0)  # â˜… íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¼ì¸ì´ 0ì´ë©´ bgë¡œ ê°„ì£¼\n",
    "\n",
    "        if cid not in clusters:\n",
    "            clusters[cid] = {\n",
    "                \"images\": [],\n",
    "                \"class_counts_bal\": {k: 0 for k in BALANCE_CLASS_IDS},\n",
    "                \"class_counts_all\": {k: 0 for k in ALL_CLASS_IDS},\n",
    "                \"bg_count\": 0,\n",
    "            }\n",
    "        clusters[cid][\"images\"].append(img)\n",
    "        for k in BALANCE_CLASS_IDS:\n",
    "            clusters[cid][\"class_counts_bal\"][k] += counts_bal[k]\n",
    "        for k in ALL_CLASS_IDS:\n",
    "            clusters[cid][\"class_counts_all\"][k] += counts_all[k]\n",
    "        clusters[cid][\"bg_count\"] += 1 if is_bg else 0\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def stratified_split_for_folder(clusters: dict, ratios: dict, balance_ids: list, seed=42):\n",
    "    \"\"\"\n",
    "    í´ë” í•˜ë‚˜ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„° ë‹¨ìœ„ë¡œ train/val/test ë°°ì •.\n",
    "    ëª©í‘œ: ì „ì²´ ì´ë¯¸ì§€ ìˆ˜ + class 0/1/2 ë¶„í¬ + bg(ë¬´ë¼ë²¨ ì´ë¯¸ì§€ ìˆ˜)ê°€ ratiosì— ê·¼ì ‘.\n",
    "    ë°©ë²•: í° í´ëŸ¬ìŠ¤í„°ë¶€í„° í•˜ë‚˜ì”© ë°°ì¹˜í•˜ë©° ê°€ì¤‘ ì”ì°¨^2 ì¦ê°€ ìµœì†Œ split ì„ íƒ.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    cluster_items = []\n",
    "    total_imgs = 0\n",
    "    total_cls = {k: 0 for k in balance_ids}\n",
    "    total_bg = 0\n",
    "\n",
    "    for cid, info in clusters.items():\n",
    "        sz = len(info[\"images\"])\n",
    "        total_imgs += sz\n",
    "        vec = {\"imgs\": sz, \"bg\": info.get(\"bg_count\", 0)}\n",
    "        total_bg += vec[\"bg\"]\n",
    "        for k in balance_ids:\n",
    "            vec[k] = info[\"class_counts_bal\"][k]\n",
    "            total_cls[k] += vec[k]\n",
    "        cluster_items.append((cid, vec))\n",
    "\n",
    "    if total_imgs == 0:\n",
    "        return {}\n",
    "\n",
    "    targets_imgs = {s: ratios[s] * total_imgs for s in ratios}\n",
    "    targets_bg   = {s: ratios[s] * total_bg   for s in ratios}\n",
    "    targets_class = {k: {s: ratios[s] * total_cls[k] for s in ratios} for k in balance_ids}\n",
    "\n",
    "    # í° í´ëŸ¬ìŠ¤í„°ë¶€í„°\n",
    "    cluster_items.sort(key=lambda x: (-x[1][\"imgs\"], x[0]))\n",
    "\n",
    "    assigned_imgs = {s: 0.0 for s in ratios}\n",
    "    assigned_bg   = {s: 0.0 for s in ratios}\n",
    "    assigned_class = {k: {s: 0.0 for s in ratios} for k in balance_ids}\n",
    "    mapping = {}\n",
    "\n",
    "    def score_components(ai_imgs, ai_bg, ai_cls):\n",
    "        img_resid = sum((ai_imgs[s] - targets_imgs[s])**2 for s in ratios)\n",
    "        bg_resid  = sum((ai_bg[s]   - targets_bg[s])**2   for s in ratios)\n",
    "        cls_resid = 0.0\n",
    "        for k in balance_ids:\n",
    "            cls_resid += sum((ai_cls[k][s] - targets_class[k][s])**2 for s in ratios)\n",
    "        # ê°€ì¤‘í•©\n",
    "        return IMG_W * img_resid + BG_W * bg_resid + CLS_W * cls_resid\n",
    "\n",
    "    def score_after_add(split: int, vec: dict):\n",
    "        ai_imgs = {s: assigned_imgs[s] + (vec[\"imgs\"] if s == split else 0.0) for s in ratios}\n",
    "        ai_bg   = {s: assigned_bg[s]   + (vec[\"bg\"]   if s == split else 0.0) for s in ratios}\n",
    "        ai_cls  = {k: {s: assigned_class[k][s] + (vec[k] if s == split else 0.0) for s in ratios}\n",
    "                   for k in balance_ids}\n",
    "        return score_components(ai_imgs, ai_bg, ai_cls)\n",
    "\n",
    "    for cid, vec in cluster_items:\n",
    "        best_s, best_sc = None, None\n",
    "        for s in ratios:\n",
    "            sc = score_after_add(s, vec)\n",
    "            if best_sc is None or sc < best_sc:\n",
    "                best_sc, best_s = sc, s\n",
    "        mapping[cid] = best_s\n",
    "        assigned_imgs[best_s] += vec[\"imgs\"]\n",
    "        assigned_bg[best_s]   += vec[\"bg\"]\n",
    "        for k in balance_ids:\n",
    "            assigned_class[k][best_s] += vec[k]\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# --- ì—¬ê¸°ë¶€í„° ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡°: images/labels ì•ˆì— train|val|test ---\n",
    "def ensure_clean_split_dirs(base: Path):\n",
    "    \"\"\"\n",
    "    <base>/images/train|val|test\n",
    "    <base>/labels/train|val|test\n",
    "    \"\"\"\n",
    "    images_root = base / \"images\"\n",
    "    labels_root = base / \"labels\"\n",
    "    if CLEAN_PREVIOUS:\n",
    "        if images_root.exists(): shutil.rmtree(images_root, ignore_errors=True)\n",
    "        if labels_root.exists(): shutil.rmtree(labels_root, ignore_errors=True)\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        (images_root / split).mkdir(parents=True, exist_ok=True)\n",
    "        (labels_root / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def place_file(src: Path, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if MATERIALIZE_MODE == \"move\":\n",
    "        shutil.move(str(src), str(dst))\n",
    "    elif MATERIALIZE_MODE == \"copy\":\n",
    "        shutil.copy2(str(src), str(dst))\n",
    "    elif MATERIALIZE_MODE == \"symlink\":\n",
    "        try:\n",
    "            dst.symlink_to(src.resolve())\n",
    "        except FileExistsError:\n",
    "            dst.unlink()\n",
    "            dst.symlink_to(src.resolve())\n",
    "    else:\n",
    "        raise ValueError(\"MATERIALIZE_MODE must be 'move'|'copy'|'symlink'.\")\n",
    "\n",
    "def materialize_and_save_report(folder: Path, clusters: dict, mapping: dict, report_dir: Path):\n",
    "    \"\"\"\n",
    "    YOLO êµ¬ì¡°ë¡œ ì‹¤ì œ ë°°ì¹˜ (images/labels ì•„ë˜ train|val|test) +\n",
    "    splitë³„ í´ë˜ìŠ¤ ë¶„í¬(0~7) ì½˜ì†” ì¶œë ¥ & í…ìŠ¤íŠ¸ ì €ì¥\n",
    "    \"\"\"\n",
    "    ensure_clean_split_dirs(folder)\n",
    "\n",
    "    split_img_counts = {0:0, 1:0, 2:0}\n",
    "    split_bg_counts  = {0:0, 1:0, 2:0}  # â˜… bg ì´ë¯¸ì§€ ìˆ˜ ë¦¬í¬íŒ…\n",
    "    split_class_counts = {s: {k:0 for k in ALL_CLASS_IDS} for s in (0,1,2)}\n",
    "\n",
    "    for cid, info in clusters.items():\n",
    "        sp = mapping[cid]\n",
    "        img_dir = folder / \"images\" / SPLIT_NAME[sp]\n",
    "        lbl_dir = folder / \"labels\" / SPLIT_NAME[sp]\n",
    "\n",
    "        # ë¦¬í¬íŠ¸ ì§‘ê³„\n",
    "        for k in ALL_CLASS_IDS:\n",
    "            split_class_counts[sp][k] += info[\"class_counts_all\"][k]\n",
    "        split_bg_counts[sp] += info.get(\"bg_count\", 0)\n",
    "\n",
    "        for img in info[\"images\"]:\n",
    "            split_img_counts[sp] += 1\n",
    "\n",
    "            # ì´ë¯¸ì§€ ë°°ì¹˜\n",
    "            img_dst = img_dir / img.name\n",
    "            place_file(img, img_dst)\n",
    "\n",
    "            # ë¼ë²¨ ë°°ì¹˜ (ì—†ìœ¼ë©´ ì •ì±…ì— ë”°ë¼ ìƒì„±/ë¯¸ìƒì„±)\n",
    "            txt_src = img.with_suffix(\".txt\")\n",
    "            txt_dst = lbl_dir / txt_src.name\n",
    "            if txt_src.exists():\n",
    "                place_file(txt_src, txt_dst)\n",
    "            elif CREATE_EMPTY_LABEL_IF_MISSING:\n",
    "                txt_dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                txt_dst.touch(exist_ok=True)\n",
    "            # else: ìƒì„±í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "    # ----- ë¦¬í¬íŠ¸ êµ¬ì„± -----\n",
    "    lines = []\n",
    "    total = sum(split_img_counts.values())\n",
    "    header = f\"[OK] {folder.name} ë¶„í•  ì™„ë£Œ (ì´ {total} images)  mode={MATERIALIZE_MODE}\"\n",
    "    lines.append(header)\n",
    "    for s in (0,1,2):\n",
    "        cnt = split_img_counts[s]\n",
    "        pct = (cnt/total)*100 if total else 0\n",
    "        lines.append(f\"  {SPLIT_NAME[s]}: {cnt} imgs ({pct:.1f}%)\")\n",
    "\n",
    "    # bg ë¶„í¬ë„ ì¶”ê°€ ì¶œë ¥\n",
    "    total_bg = sum(split_bg_counts.values())\n",
    "    lines.append(\"\\n[BG (no-label) image distribution]\")\n",
    "    for s in (0,1,2):\n",
    "        pct = (split_bg_counts[s]/total_bg*100) if total_bg else 0.0\n",
    "        lines.append(f\"  {SPLIT_NAME[s]}: {split_bg_counts[s]} bg ({pct:.1f}%)\")\n",
    "\n",
    "    lines.append(\"\\n[Class distribution (instances per split)]\")\n",
    "    hdr = \" \".join([f\"{SPLIT_NAME[s]:>10}\" for s in (0,1,2)])\n",
    "    lines.append(f\"{'class':>10} {hdr}\")\n",
    "    for k in ALL_CLASS_IDS:\n",
    "        row = \" \".join([f\"{split_class_counts[s][k]:>10d}\" for s in (0,1,2)])\n",
    "        lines.append(f\"{k:>10} {row}\")\n",
    "\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_txt = report_dir / f\"split_report_{folder.name}.txt\"\n",
    "    with out_txt.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"\\n\".join(lines))\n",
    "    print(f\"\\n[Saved] {out_txt}\")\n",
    "\n",
    "def run():\n",
    "    random.seed(SEED)\n",
    "    REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for folder in FOLDERS:\n",
    "        if not folder.exists():\n",
    "            print(f\"[WARN] í´ë” ì—†ìŒ: {folder}\")\n",
    "            continue\n",
    "\n",
    "        # 1) í´ë” ë£¨íŠ¸ì—ì„œ _cluster_<id> íŒŒì¼ë“¤ë§Œ ëª¨ì•„ í†µê³„ (+ bg_count í¬í•¨)\n",
    "        clusters = build_cluster_stats(folder)\n",
    "        if not clusters:\n",
    "            print(f\"[INFO] í´ëŸ¬ìŠ¤í„° ì—†ìŒ(_cluster_<id> íŒŒì¼ ë¯¸ì¡´ì¬): {folder.name}\")\n",
    "            continue\n",
    "\n",
    "        # 2) í´ë” ë‹¨ìœ„ ë¶„í•  (í´ëŸ¬ìŠ¤í„° ìœ ì§€ + class 0/1/2 + bg ë¶„í¬ ìœ ì§€)\n",
    "        mapping = stratified_split_for_folder(clusters, RATIOS, BALANCE_CLASS_IDS, seed=SEED)\n",
    "\n",
    "        # 3) ì‹¤ì œ YOLO êµ¬ì¡°ë¡œ ë°°ì¹˜(images/labels ë°‘ì— split) + ë¦¬í¬íŠ¸ ì €ì¥\n",
    "        if DO_MATERIALIZE:\n",
    "            materialize_and_save_report(folder, clusters, mapping, REPORT_DIR)\n",
    "        else:\n",
    "            print(f\"[DRY] ë¶„í•  ë§¤í•‘ë§Œ ì™„ë£Œ(ë°°ì¹˜ ë¯¸ìˆ˜í–‰): {folder.name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2aa987",
   "metadata": {},
   "source": [
    "### ì„ì˜€ëŠ”ì§€ ì ê²€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12610bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== ì ê²€í•  ë°ì´í„° í´ë”ë“¤ =====\n",
    "FOLDERS = [\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250721_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250725_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250904_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250930_good_data\"),\n",
    "]\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\",\n",
    "            \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"}\n",
    "CLUSTER_RE = re.compile(r\"_cluster_(\\d+)\", re.IGNORECASE)\n",
    "\n",
    "def check_folder(folder: Path):\n",
    "    print(f\"\\n=== Checking {folder.name} ===\")\n",
    "    # cluster_id -> set of splits where it appears\n",
    "    cluster_splits = defaultdict(set)\n",
    "\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        img_dir = folder / \"images\" / split\n",
    "        if not img_dir.exists():\n",
    "            continue\n",
    "        for p in img_dir.rglob(\"*\"):\n",
    "            if p.is_file() and p.suffix in IMG_EXTS:\n",
    "                m = CLUSTER_RE.search(p.stem)\n",
    "                if not m:\n",
    "                    continue\n",
    "                cluster_splits[m.group(1)].add(split)\n",
    "\n",
    "    # ì¤‘ë³µ í™•ì¸\n",
    "    bad = {cid: sorted(splits) for cid, splits in cluster_splits.items() if len(splits) > 1}\n",
    "    if not bad:\n",
    "        print(\"âœ… ëª¨ë“  í´ëŸ¬ìŠ¤í„°ê°€ í•œ splitì—ë§Œ ì¡´ì¬ (ë¬¸ì œ ì—†ìŒ)\")\n",
    "    else:\n",
    "        print(f\"âŒ {len(bad)}ê°œì˜ í´ëŸ¬ìŠ¤í„°ê°€ ì—¬ëŸ¬ splitì— ì„ì—¬ ìˆìŒ\")\n",
    "        for cid, splits in bad.items():\n",
    "            print(f\"  cluster {cid}: {', '.join(splits)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for f in FOLDERS:\n",
    "        check_folder(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee95386",
   "metadata": {},
   "source": [
    "### class 4567ë„ ë¹„ìœ¨ì— ë§ì¶° ì˜®ê²¨ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4541a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import random, shutil\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "FOLDERS = [\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250721_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250725_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250904_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250930_good_data\"),\n",
    "]\n",
    "REPORT_DIR = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data\")\n",
    "\n",
    "# ë¹„ìœ¨\n",
    "RATIOS = {0: 0.70, 1: 0.18, 2: 0.12}\n",
    "SPLIT_NAME = {0: \"train\", 1: \"val\", 2: \"test\"}\n",
    "SEED = 13\n",
    "\n",
    "# ëŒ€ìƒ: í´ë˜ìŠ¤ 4/5/6/7 + BG(ë¼ë²¨ ì—†ìŒ/ë¹ˆ ë¼ë²¨)\n",
    "TARGET_CLASS_IDS = [4,5,6,7]\n",
    "EXCLUDE_IF_HAS_012 = True   # âœ… 0/1/2ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ëŒ€ìƒì—ì„œ ì œì™¸; 012ë¥¼ ë¬´ì‹œ(í¬í•¨)í•˜ë ¤ë©´ False\n",
    "HANDLE_BG = True            # âœ… BGë„ ì¬ë¶„í•  ëŒ€ìƒ í¬í•¨\n",
    "\n",
    "# ì‹¤ì œ ë°˜ì˜\n",
    "MATERIALIZE_MODE = \"move\"   # \"move\" | \"copy\" | \"symlink\"\n",
    "DRY_RUN = False             # âœ… ì‹¤ì œë¡œ ì˜®ê¸°ë ¤ë©´ False\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\",\n",
    "            \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"}\n",
    "\n",
    "def split_dirs(base: Path):\n",
    "    return {\n",
    "        0: (base / \"images\" / \"train\", base / \"labels\" / \"train\"),\n",
    "        1: (base / \"images\" / \"val\",   base / \"labels\" / \"val\"),\n",
    "        2: (base / \"images\" / \"test\",  base / \"labels\" / \"test\"),\n",
    "    }\n",
    "\n",
    "def iter_split_images(base: Path):\n",
    "    for sid, (dimg, _) in split_dirs(base).items():\n",
    "        if not dimg.exists(): continue\n",
    "        for p in dimg.iterdir():\n",
    "            if p.is_file() and p.suffix in IMG_EXTS:\n",
    "                yield sid, p\n",
    "\n",
    "def read_label_lines(txt: Path):\n",
    "    if not txt.exists(): return []\n",
    "    try:\n",
    "        return [ln.strip() for ln in txt.read_text(encoding=\"utf-8\").splitlines() if ln.strip()]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_class_counts_and_flags(txt: Path):\n",
    "    lines = read_label_lines(txt)\n",
    "    is_bg = (len(lines) == 0)\n",
    "    counts = {k: 0 for k in TARGET_CLASS_IDS}\n",
    "    has_012 = False\n",
    "    for s in lines:\n",
    "        parts = s.split()\n",
    "        try:\n",
    "            cid = int(parts[0])\n",
    "        except:\n",
    "            continue\n",
    "        if cid in counts: counts[cid] += 1\n",
    "        if cid in (0,1,2): has_012 = True\n",
    "    return counts, is_bg, has_012\n",
    "\n",
    "def place_file(src: Path, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if MATERIALIZE_MODE == \"move\":\n",
    "        if src.resolve() == dst.resolve():  # ê°™ì€ ê²½ë¡œë©´ ìŠ¤í‚µ\n",
    "            return\n",
    "        shutil.move(str(src), str(dst))\n",
    "    elif MATERIALIZE_MODE == \"copy\":\n",
    "        shutil.copy2(str(src), str(dst))\n",
    "    elif MATERIALIZE_MODE == \"symlink\":\n",
    "        try:\n",
    "            dst.symlink_to(src.resolve())\n",
    "        except FileExistsError:\n",
    "            dst.unlink(missing_ok=True)\n",
    "            dst.symlink_to(src.resolve())\n",
    "    else:\n",
    "        raise ValueError(\"MATERIALIZE_MODE must be move/copy/symlink\")\n",
    "\n",
    "def plan_and_apply(folder: Path):\n",
    "    random.seed(SEED)\n",
    "\n",
    "    # 1) ëŒ€ìƒ ì´ë¯¸ì§€ ìˆ˜ì§‘\n",
    "    items=[]   # (img, lbl, from_split, vec={\"imgs\":1,\"bg\":0/1, 4:cnt,5:cnt,6:cnt,7:cnt})\n",
    "    totals={\"imgs\":0,\"bg\":0,**{k:0 for k in TARGET_CLASS_IDS}}\n",
    "\n",
    "    for sid,img in iter_split_images(folder):\n",
    "        lbl = folder / \"labels\" / SPLIT_NAME[sid] / (img.stem + \".txt\")\n",
    "        cnts, is_bg, has_012 = get_class_counts_and_flags(lbl)\n",
    "\n",
    "        if EXCLUDE_IF_HAS_012 and has_012:\n",
    "            continue\n",
    "        has_target = sum(cnts.values()) > 0\n",
    "        if not has_target and not (HANDLE_BG and is_bg):\n",
    "            continue\n",
    "\n",
    "        vec = {\"imgs\":1, \"bg\": 1 if (HANDLE_BG and is_bg) else 0}\n",
    "        vec.update(cnts)\n",
    "        items.append((img, lbl, sid, vec))\n",
    "\n",
    "        totals[\"imgs\"] += 1\n",
    "        totals[\"bg\"]   += vec[\"bg\"]\n",
    "        for k in TARGET_CLASS_IDS: totals[k] += cnts[k]\n",
    "\n",
    "    if not items:\n",
    "        print(f\"[INFO] {folder.name}: ëŒ€ìƒ(4/5/6/7 + BG) ì—†ìŒ\")\n",
    "        return\n",
    "\n",
    "    # 2) ëª©í‘œ (ì´ ì„œë¸Œì…‹ ê¸°ì¤€)\n",
    "    targets = {s: RATIOS[s] * totals[\"imgs\"] for s in RATIOS}\n",
    "\n",
    "    # 3) ë‹¨ìˆœ ë¹„ìœ¨ ê¸°ë°˜ ë°°ì •(ì´ë¯¸ì§€ ê°œìˆ˜ ì¤‘ì‹¬) â€“ í•„ìš”ì‹œ ê°€ì¤‘ ëª©ì í•¨ìˆ˜ë¡œ êµì²´ ê°€ëŠ¥\n",
    "    random.shuffle(items)\n",
    "    counts = {s:0 for s in RATIOS}\n",
    "    assignment=[]  # (img,lbl,from_s,to_s,vec)\n",
    "    for it in items:\n",
    "        # í˜„ì¬ ëª©í‘œ ëŒ€ë¹„ ê°€ì¥ ë¶€ì¡±í•œ split ì„ íƒ\n",
    "        best = min(RATIOS.keys(), key=lambda s: counts[s] - targets[s])\n",
    "        assignment.append((it[0], it[1], it[2], best, it[3]))\n",
    "        counts[best] += 1\n",
    "\n",
    "    # 4) ì½˜ì†” ê³„íš ìš”ì•½\n",
    "    print(f\"\\n=== {'DRY-RUN' if DRY_RUN else 'APPLY'}: 4/5/6/7 + BG ì¬ë¶„í•  â†’ {folder.name} ===\")\n",
    "    move_preview = 0\n",
    "    for img,lbl,from_s,to_s,vec in assignment:\n",
    "        if from_s != to_s:\n",
    "            print(f\"{img.name}: {SPLIT_NAME[from_s]} -> {SPLIT_NAME[to_s]}\")\n",
    "            move_preview += 1\n",
    "    print(f\"ëŒ€ìƒ ì´ë¯¸ì§€: {len(items)} / ì´ë™ ì˜ˆì •: {move_preview}\")\n",
    "\n",
    "    # 5) ì‹¤ì œ ë°˜ì˜\n",
    "    if not DRY_RUN:\n",
    "        for img,lbl,from_s,to_s,vec in assignment:\n",
    "            if from_s == to_s:\n",
    "                continue\n",
    "            img_dst = folder / \"images\" / SPLIT_NAME[to_s] / img.name\n",
    "            lbl_dst = folder / \"labels\" / SPLIT_NAME[to_s] / (img.stem + \".txt\")\n",
    "            place_file(img, img_dst)\n",
    "            if lbl.exists():     # BGëŠ” ë¼ë²¨ ì—†ìŒ ê°€ëŠ¥\n",
    "                place_file(lbl, lbl_dst)\n",
    "\n",
    "    # 6) ìµœì¢… ë¶„í¬ ì§‘ê³„(ì´ ì„œë¸Œì…‹ì— í•œì •í•´ ë¦¬í¬íŠ¸)\n",
    "    final_imgs = {s:0 for s in RATIOS}\n",
    "    final_bg   = {s:0 for s in RATIOS}\n",
    "    final_cls  = {s:{k:0 for k in TARGET_CLASS_IDS} for s in RATIOS}\n",
    "    for _,_,_,to_s,vec in assignment:\n",
    "        final_imgs[to_s] += 1\n",
    "        final_bg[to_s]   += vec[\"bg\"]\n",
    "        for k in TARGET_CLASS_IDS:\n",
    "            final_cls[to_s][k] += vec[k]\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"[{'DRY' if DRY_RUN else 'OK'}] ({'plan only' if DRY_RUN else 'applied'}) \"\n",
    "                 f\"{folder.name} â€“ 4/5/6/7 + BG ì¬ë¶„í•   mode={MATERIALIZE_MODE}\")\n",
    "    lines.append(f\"  ëŒ€ìƒ ì´ë¯¸ì§€: {len(items)} / ì´ë™: {move_preview}\")\n",
    "    for s in (0,1,2):\n",
    "        pct = (final_imgs[s]/len(items)*100) if items else 0\n",
    "        lines.append(f\"  {SPLIT_NAME[s]}: {final_imgs[s]} imgs ({pct:.1f}%), BG={final_bg[s]}, \"\n",
    "                     + \", \".join([f\"{k}:{final_cls[s][k]}\" for k in TARGET_CLASS_IDS]))\n",
    "\n",
    "    print(\"\\n\".join(lines))\n",
    "\n",
    "    REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out_txt = REPORT_DIR / f\"{'dry_' if DRY_RUN else ''}applied_c4567_bg_{folder.name}.txt\"\n",
    "    with out_txt.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "    print(f\"[Saved] {out_txt}\")\n",
    "\n",
    "def run():\n",
    "    for folder in FOLDERS:\n",
    "        if not folder.exists():\n",
    "            print(f\"[WARN] í´ë” ì—†ìŒ: {folder}\")\n",
    "            continue\n",
    "        sd = split_dirs(folder)\n",
    "        if not all(sd[s][0].exists() and sd[s][1].exists() for s in (0,1,2)):\n",
    "            print(f\"[WARN] YOLO êµ¬ì¡°(images/labels/<split>) ì—†ìŒ: {folder}\")\n",
    "            continue\n",
    "        plan_and_apply(folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154c031",
   "metadata": {},
   "source": [
    "### ëª‡ê°œë‚˜ ì„ì—¬ìˆëŠ”ì§€ í™•ì¸ ê·¸ë¦¬ê³  ì„ì¸ ê³³ì— ë¬´ìŠ¨ classê°€ìˆëŠ”ì§€ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466adbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ===== ì ê²€í•  ë°ì´í„° í´ë”ë“¤ =====\n",
    "FOLDERS = [\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250721_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250725_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250904_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250930_good_data\"),\n",
    "]\n",
    "\n",
    "# ë¦¬í¬íŠ¸ ì €ì¥ ìœ„ì¹˜(ê° good_data ìƒìœ„ì¸ merge_data)\n",
    "REPORT_DIR = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data\")\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\",\n",
    "            \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"}\n",
    "CLUSTER_RE = re.compile(r\"_cluster_(\\d+)\", re.IGNORECASE)\n",
    "\n",
    "def read_classes(txt_path: Path) -> Counter:\n",
    "    c = Counter()\n",
    "    if not txt_path.exists():\n",
    "        return c\n",
    "    try:\n",
    "        for ln in txt_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "            if not ln.strip():\n",
    "                continue\n",
    "            try:\n",
    "                cid = int(ln.split()[0])\n",
    "                c[cid] += 1\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "    return c\n",
    "\n",
    "def check_folder(folder: Path):\n",
    "    lines = []\n",
    "    lines.append(f\"=== Checking {folder.name} ===\")\n",
    "    cluster_info = defaultdict(lambda: {\"splits\": set(),\n",
    "                                        \"imgs\": 0,\n",
    "                                        \"classes\": Counter()})\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        img_dir = folder / \"images\" / split\n",
    "        if not img_dir.exists():\n",
    "            continue\n",
    "        for p in img_dir.rglob(\"*\"):\n",
    "            if p.is_file() and p.suffix in IMG_EXTS:\n",
    "                m = CLUSTER_RE.search(p.stem)\n",
    "                if not m:\n",
    "                    continue\n",
    "                cid = m.group(1)\n",
    "                cluster_info[cid][\"splits\"].add(split)\n",
    "                cluster_info[cid][\"imgs\"] += 1\n",
    "                lbl = folder / \"labels\" / split / (p.stem + \".txt\")\n",
    "                cluster_info[cid][\"classes\"].update(read_classes(lbl))\n",
    "\n",
    "    bad = {cid: v for cid, v in cluster_info.items() if len(v[\"splits\"]) > 1}\n",
    "    if not bad:\n",
    "        msg = \"âœ… ëª¨ë“  í´ëŸ¬ìŠ¤í„°ê°€ í•œ splitì—ë§Œ ì¡´ì¬ (ë¬¸ì œ ì—†ìŒ)\"\n",
    "        print(\"\\n\" + msg)\n",
    "        lines.append(msg)\n",
    "    else:\n",
    "        total_imgs = sum(v[\"imgs\"] for v in bad.values())\n",
    "        msg = (f\"âŒ {len(bad)}ê°œ í´ëŸ¬ìŠ¤í„°ê°€ ì—¬ëŸ¬ splitì— ì„ì—¬ ìˆìŒ \"\n",
    "               f\"(ì„ì¸ ì´ë¯¸ì§€ ì´ {total_imgs} ì¥)\")\n",
    "        print(\"\\n\" + msg)\n",
    "        lines.append(msg + \"\\n\")\n",
    "        for cid, v in sorted(bad.items(), key=lambda x: int(x[0])):\n",
    "            splits = \", \".join(sorted(v[\"splits\"]))\n",
    "            cls_sum = \", \".join(f\"{k}:{v['classes'][k]}\" for k in sorted(v[\"classes\"]))\n",
    "            line = (f\"  cluster {cid:>4} | splits: {splits:<15} | \"\n",
    "                    f\"imgs:{v['imgs']:<3} | classes: {cls_sum or 'â€”'}\")\n",
    "            print(line)\n",
    "            lines.append(line)\n",
    "\n",
    "    # txt ì €ì¥\n",
    "    REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = REPORT_DIR / f\"cluster_mixed_report_{folder.name}.txt\"\n",
    "    out_path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\"[Saved] {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for f in FOLDERS:\n",
    "        check_folder(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1e2f0",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ë¶„í•  í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee53fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# ====== ì„¤ì • ======\n",
    "ROOT = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data\")\n",
    "TARGETS = [\n",
    "    ROOT / \"20250721_good_data\",\n",
    "    ROOT / \"20250725_good_data\",\n",
    "    ROOT / \"20250904_good_data\",\n",
    "    ROOT / \"20250930_good_data\",\n",
    "]\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"}\n",
    "\n",
    "# ë³´ê³ ì„œ íŒŒì¼\n",
    "out_file = ROOT / \"split_class_report.txt\"\n",
    "\n",
    "# ====== ìœ í‹¸ ======\n",
    "def read_classes_txt(folder: Path):\n",
    "    \"\"\"\n",
    "    classes.txtê°€ ìˆìœ¼ë©´ ê·¸ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©.\n",
    "    ì—†ìœ¼ë©´ 0~7ê¹Œì§€ class_ië¡œ ê°€ì •.\n",
    "    \"\"\"\n",
    "    f = folder / \"classes.txt\"\n",
    "    if f.exists():\n",
    "        names = [ln.strip() for ln in f.read_text(encoding=\"utf-8\").splitlines() if ln.strip()]\n",
    "        return {i: names[i] for i in range(len(names))}\n",
    "    return {i: f\"class_{i}\" for i in range(8)}\n",
    "\n",
    "def list_images(dirpath: Path):\n",
    "    return [p for p in dirpath.rglob(\"*\") if p.suffix in IMG_EXTS]\n",
    "\n",
    "def count_split(folder: Path, split: str, id2name: dict):\n",
    "    \"\"\"\n",
    "    í•œ ë°ì´í„°ì…‹(folder)ì˜ íŠ¹ì • split(train/val/test)ì— ëŒ€í•´:\n",
    "      - per_class: {í´ë˜ìŠ¤ëª…: ê°ì²´ìˆ˜}\n",
    "      - bg: BG ì´ë¯¸ì§€ ìˆ˜(ë¼ë²¨ íŒŒì¼ ì—†ëŠ” ì´ë¯¸ì§€)\n",
    "      - total_images: ì´ë¯¸ì§€ ì´ ê°œìˆ˜\n",
    "      - labeled_images: ë¼ë²¨ íŒŒì¼ì´ ìˆëŠ” ì´ë¯¸ì§€ ê°œìˆ˜\n",
    "      - total_labels: ê°ì²´ ì´ ê°œìˆ˜(ë°•ìŠ¤ ìˆ˜)\n",
    "    \"\"\"\n",
    "    img_dir, lbl_dir = folder / \"images\" / split, folder / \"labels\" / split\n",
    "    cls_counter = Counter()\n",
    "    total_labels = 0\n",
    "\n",
    "    if lbl_dir.exists():\n",
    "        for txt in lbl_dir.rglob(\"*.txt\"):\n",
    "            for line in txt.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                try:\n",
    "                    cid = int(float(parts[0]))\n",
    "                except:\n",
    "                    continue\n",
    "                cls_counter[cid] += 1\n",
    "                total_labels += 1\n",
    "\n",
    "    bg = 0\n",
    "    stems_lbl = set()\n",
    "    if img_dir.exists():\n",
    "        imgs = list_images(img_dir)\n",
    "        stems_img = {p.stem for p in imgs}\n",
    "        stems_lbl = {p.stem for p in lbl_dir.rglob(\"*.txt\")} if lbl_dir.exists() else set()\n",
    "        bg = len(stems_img - stems_lbl)\n",
    "\n",
    "    total_images = len(list_images(img_dir)) if img_dir.exists() else 0\n",
    "    labeled_images = len(stems_lbl) if lbl_dir.exists() else 0\n",
    "    per_class = {id2name.get(k, f\"class_{k}\"): v for k, v in sorted(cls_counter.items())}\n",
    "    return per_class, bg, total_images, labeled_images, total_labels\n",
    "\n",
    "# ====== ì´í•© ì§‘ê³„ìš© ======\n",
    "global_total_images = 0\n",
    "global_total_labeled = 0\n",
    "global_total_bg = 0\n",
    "global_total_objects = 0\n",
    "global_class_totals = Counter()\n",
    "all_class_names_seen = set()\n",
    "\n",
    "# ì¶œë ¥ ë¼ì¸(ë°ì´í„°ì…‹ë³„ ìƒì„¸)\n",
    "report_lines = []\n",
    "\n",
    "# ì„ í˜¸ ì¶œë ¥ ìˆœì„œ(ìˆìœ¼ë©´ ì´ ìˆœì„œëŒ€ë¡œ ë¨¼ì €, ë‚˜ë¨¸ì§€ëŠ” ì´ë¦„ìˆœ)\n",
    "PREFERRED_ORDER = [\n",
    "    \"Divot\", \"Fixed_Divot\", \"Diseased_Grass\", \"Confused_Object\",\n",
    "    \"Pole\", \"Sprinkler\", \"Drain\", \"Golf ball\"\n",
    "]\n",
    "\n",
    "# ====== ë£¨í”„: ë°ì´í„°ì…‹/ìŠ¤í”Œë¦¿ ======\n",
    "for tgt in TARGETS:\n",
    "    id2name = read_classes_txt(tgt)\n",
    "    all_class_names_seen.update(id2name.values())\n",
    "\n",
    "    report_lines.append(f\"\\n=== {tgt.name} ===\")\n",
    "    for split in SPLITS:\n",
    "        per_class, bg, total_images, labeled_images, total_labels = count_split(tgt, split, id2name)\n",
    "\n",
    "        # ìƒì„¸ ë¼ì¸\n",
    "        report_lines.append(\n",
    "            f\"[{split}] images={total_images}, labeled={labeled_images}, BG(no-label)={bg}, total_objects={total_labels}\"\n",
    "        )\n",
    "        # classes.txt ìˆœì„œëŒ€ë¡œ ì¶œë ¥(ê°€ë…ì„±)\n",
    "        for i in sorted(id2name.keys()):\n",
    "            cname = id2name[i]\n",
    "            report_lines.append(f\"  {cname:16s}: {per_class.get(cname, 0)}\")\n",
    "        report_lines.append(f\"  {'BG':16s}: {bg}\")\n",
    "\n",
    "        # ===== ì´í•© ì§‘ê³„ =====\n",
    "        global_total_images += total_images\n",
    "        global_total_labeled += labeled_images\n",
    "        global_total_bg += bg\n",
    "        global_total_objects += total_labels\n",
    "        for cname, cnt in per_class.items():\n",
    "            global_class_totals[cname] += cnt\n",
    "\n",
    "# ====== ë§¨ ìœ„ì— TOTAL ì„¹ì…˜ ë§Œë“¤ê¸° ======\n",
    "# í´ë˜ìŠ¤ ì¶œë ¥ ìˆœì„œ êµ¬ì„±\n",
    "class_order = [c for c in PREFERRED_ORDER if c in all_class_names_seen] + \\\n",
    "              [c for c in sorted(all_class_names_seen) if c not in PREFERRED_ORDER]\n",
    "\n",
    "total_lines = []\n",
    "total_lines.append(\"=== TOTAL (ALL DATASETS) ===\")\n",
    "total_lines.append(\n",
    "    f\"images={global_total_images}, labeled={global_total_labeled}, \"\n",
    "    f\"BG(no-label)={global_total_bg}, total_objects={global_total_objects}\"\n",
    ")\n",
    "for cname in class_order:\n",
    "    total_lines.append(f\"  {cname:16s}: {global_class_totals.get(cname, 0)}\")\n",
    "total_lines.append(f\"  {'BG':16s}: {global_total_bg}\")\n",
    "\n",
    "# ====== íŒŒì¼ ì €ì¥ (TOTALì„ ë§¨ ìœ„ë¡œ) ======\n",
    "final_text = \"\\n\".join(total_lines + [\"\"] + report_lines)\n",
    "out_file.write_text(final_text, encoding=\"utf-8\")\n",
    "\n",
    "# ====== ì½˜ì†” ìš”ì•½ ì¶œë ¥ ======\n",
    "print(f\"âœ… ì „ì²´ ê²°ê³¼ë¥¼ {out_file} ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"\\n[TOP SUMMARY]\")\n",
    "print(total_lines[0])\n",
    "print(total_lines[1])\n",
    "for line in total_lines[2:]:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\n[Per-dataset quick image counts]\")\n",
    "for tgt in TARGETS:\n",
    "    for split in SPLITS:\n",
    "        img_count = len(list_images(tgt / \"images\" / split))\n",
    "        print(f\"{tgt.name} [{split}] images: {img_count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ac710",
   "metadata": {},
   "source": [
    "### ì˜›ë‚  ì¦ê°•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # augment_balance_train_only_multi.py\n",
    "# # - ê° good_data í´ë”ì˜ trainë§Œ ì¦ê°• (images/train, labels/train)\n",
    "# # - ì¶œë ¥ë„ trainì— ì €ì¥(ì›ë³¸ ë³´ì¡´)\n",
    "# # - ì¤‘ë³µ ì–µì œ: í•˜ë“œìº¡/ì¿¨ë‹¤ìš´/ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜\n",
    "# # - TARGETì€ \"train ë‚´ ìµœì¢… ê°ì²´ ìˆ˜\" ëª©í‘œ(ê°ì†Œ ì‘ì—…ì€ ì•ˆ í•¨)\n",
    "\n",
    "# from pathlib import Path\n",
    "# from collections import Counter, defaultdict, deque\n",
    "# import random, csv, os, re\n",
    "# import cv2\n",
    "# import albumentations as A\n",
    "\n",
    "# # ========== ëŒ€ìƒ í´ë”ë“¤ ==========\n",
    "# BASES = [\n",
    "#     #Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250918_merge_data/20250721_good_data\"),\n",
    "#     #Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250918_merge_data/20250725_good_data\"),\n",
    "#     Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250918_merge_data/20250904_good_data\"),\n",
    "# ]\n",
    "\n",
    "# # ========== ê³µí†µ ì„¤ì • ==========\n",
    "# IMG_EXTS = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"]\n",
    "# SEED = 33\n",
    "# random.seed(SEED)\n",
    "\n",
    "# # í´ë˜ìŠ¤ ë§¤í•‘(8í´ë˜ìŠ¤, í•„ìš”ì‹œ ë³€ê²½)\n",
    "# # 0 Divot, 1 Fixed_Divot, 2 Diseased_Grass, 3 Confused_Object, 4 Pole, 5 Sprinkler, 6 Drain, 7 Golf ball\n",
    "# TARGET = {\n",
    "#     0: 3500,  # Divot\n",
    "#     1: 2915,  # Fixed_Divot\n",
    "#     2: 800,  # Diseased_Grass\n",
    "#     3: 0,     # Confused_Object\n",
    "#     4: 150,   # Pole\n",
    "#     5: 50,    # Sprinkler\n",
    "#     6: 330,   # Drain\n",
    "#     7: 200,   # Golf ball\n",
    "# }\n",
    "# ALL_CLASS_IDS = list(range(8))\n",
    "# ALLOW_IDS = set(ALL_CLASS_IDS)\n",
    "\n",
    "# # --- ì¤‘ë³µ ì–µì œ íŒŒë¼ë¯¸í„° ---\n",
    "# MAX_USES_BASE = 1                     # ê¸°ë³¸ ì¬ì‚¬ìš© ìƒí•œ(ë‚®ê²Œ)\n",
    "# MAX_USES_BOOST_PER_CLASS = {          # ë¶€ì¡± í´ë˜ìŠ¤ì¼ ë•Œ ê°€ì‚°ì¹˜\n",
    "#     0: 2, 1: 2, 2: 3, 3: 0, 4: 1, 5: 1, 6: 2, 7: 2\n",
    "# }\n",
    "# MAX_PER_IMAGE_HARD = 3                # í•œ ì›ë³¸ì—ì„œ ì ˆëŒ€ ìµœëŒ€ ìƒì„± ìˆ˜\n",
    "# RECENT_COOLDOWN = 5                   # ìµœê·¼ ì‚¬ìš© ì›ë³¸ ì¿¨ë‹¤ìš´ ê¸¸ì´\n",
    "\n",
    "# # ë°°ê²½(ë¼ë²¨ ì—†ëŠ” train ì´ë¯¸ì§€) ì¦ê°• ë°°ìˆ˜\n",
    "# BG_AUG_MULTIPLIER = 2                # ì›ë³¸ 1 + ì¦ê°• 2 = ì´ 3ë°°\n",
    "\n",
    "# # ========== ì¦ê°• íŒŒì´í”„ë¼ì¸ ==========\n",
    "# aug = A.Compose([\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomBrightnessContrast(brightness_limit=(-0.12, 0.09),\n",
    "#                                contrast_limit=(-0.08, 0.08), p=0.9),\n",
    "#     A.HueSaturationValue(hue_shift_limit=6, sat_shift_limit=8, val_shift_limit=8, p=0.9),\n",
    "#     A.RGBShift(r_shift_limit=8, g_shift_limit=8, b_shift_limit=8, p=0.6),\n",
    "#     A.RandomGamma(gamma_limit=(95, 105), p=0.3),\n",
    "#     A.OneOf([\n",
    "#         A.CLAHE(clip_limit=3.0, tile_grid_size=(8, 8), p=0.6),\n",
    "#         A.ISONoise(color_shift=(0.01, 0.08), intensity=(0.08, 0.35), p=0.4),\n",
    "#     ], p=0.45),\n",
    "#     A.OneOf([\n",
    "#         A.MotionBlur(blur_limit=3, p=0.2),\n",
    "#         A.GaussianBlur(blur_limit=3, p=0.3),\n",
    "#         A.Sharpen(alpha=(0.05, 0.2), lightness=(0.9, 1.1), p=0.2),\n",
    "#     ], p=0.20),\n",
    "# ], bbox_params=A.BboxParams(\n",
    "#     format=\"yolo\",\n",
    "#     label_fields=[\"class_labels\"],\n",
    "#     min_visibility=0.25\n",
    "# ))\n",
    "\n",
    "# # ========== ìœ í‹¸ ==========\n",
    "# def read_yolo_label(lbl: Path):\n",
    "#     boxes, cls = [], []\n",
    "#     if not lbl.exists(): return boxes, cls\n",
    "#     with open(lbl, \"r\") as f:\n",
    "#         for line in f:\n",
    "#             p = line.strip().split()\n",
    "#             if len(p) < 5: continue\n",
    "#             try: c = int(float(p[0]))\n",
    "#             except: continue\n",
    "#             try: x, y, w, h = map(float, p[1:5])\n",
    "#             except: continue\n",
    "#             x = min(max(x, 0.0), 1.0); y = min(max(y, 0.0), 1.0)\n",
    "#             w = min(max(w, 0.0), 1.0); h = min(max(h, 0.0), 1.0)\n",
    "#             if w <= 0.0 or h <= 0.0: continue\n",
    "#             boxes.append([x, y, w, h]); cls.append(c)\n",
    "#     return boxes, cls\n",
    "\n",
    "# def write_yolo_label(lbl_path: Path, boxes, cls, *, allow_ids=ALLOW_IDS):\n",
    "#     lbl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#     with open(lbl_path, \"w\") as f:\n",
    "#         for c, (x, y, w, h) in zip(cls, boxes):\n",
    "#             try: c_int = int(float(c))\n",
    "#             except: continue\n",
    "#             if allow_ids is not None and c_int not in allow_ids: continue\n",
    "#             x = float(max(0.0, min(1.0, x)))\n",
    "#             y = float(max(0.0, min(1.0, y)))\n",
    "#             w = float(max(0.0, min(1.0, w)))\n",
    "#             h = float(max(0.0, min(1.0, h)))\n",
    "#             if w <= 0.0 or h <= 0.0: continue\n",
    "#             f.write(f\"{c_int} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "\n",
    "# def yolo_sanitize(boxes):\n",
    "#     eps = 1e-6; out = []\n",
    "#     for x, y, w, h in boxes:\n",
    "#         l = max(0.0, min(1.0, x - w/2.0))\n",
    "#         r = max(0.0, min(1.0, x + w/2.0))\n",
    "#         t = max(0.0, min(1.0, y - h/2.0))\n",
    "#         b = max(0.0, min(1.0, y + h/2.0))\n",
    "#         if r - l <= eps or b - t <= eps: continue\n",
    "#         nx = (l + r)/2.0; ny = (t + b)/2.0; nw = (r - l); nh = (b - t)\n",
    "#         nx = min(max(nx, eps), 1.0 - eps); ny = min(max(ny, eps), 1.0 - eps)\n",
    "#         nw = min(max(nw, eps), 1.0 - eps); nh = min(max(nh, eps), 1.0 - eps)\n",
    "#         out.append([nx, ny, nw, nh])\n",
    "#     return out\n",
    "\n",
    "# def collect_background_images(img_dir: Path, lbl_dir: Path):\n",
    "#     stems_lbl = {p.stem for p in lbl_dir.rglob(\"*.txt\")} if lbl_dir.exists() else set()\n",
    "#     bg_images = []\n",
    "#     for ext in IMG_EXTS:\n",
    "#         for img in img_dir.rglob(f\"*{ext}\"):\n",
    "#             if img.stem not in stems_lbl:\n",
    "#                 bg_images.append(img)\n",
    "#     return sorted(bg_images)\n",
    "\n",
    "# def load_image(path: Path):\n",
    "#     img = cv2.imread(str(path))\n",
    "#     if img is None:\n",
    "#         raise RuntimeError(f\"Failed to read image: {path}\")\n",
    "#     return img\n",
    "\n",
    "# def next_unique_name(stem: str, used: set, tag: str):\n",
    "#     i = 1\n",
    "#     while True:\n",
    "#         s = f\"{stem}_{tag}{i:04d}\"\n",
    "#         if s not in used:\n",
    "#             used.add(s)\n",
    "#             return s\n",
    "#         i += 1\n",
    "\n",
    "# def effective_cap_for_file(file_cls_counter: Counter, deficit: Counter):\n",
    "#     cap = MAX_USES_BASE\n",
    "#     best_boost = 0\n",
    "#     for c, n in file_cls_counter.items():\n",
    "#         if n <= 0: continue\n",
    "#         if deficit.get(c, 0) > 0:\n",
    "#             best_boost = max(best_boost, MAX_USES_BOOST_PER_CLASS.get(c, 0))\n",
    "#     return cap + best_boost\n",
    "\n",
    "# def current_counts(label_dir: Path):\n",
    "#     cnt = Counter()\n",
    "#     for lbl in label_dir.rglob(\"*.txt\"):\n",
    "#         _, cls = read_yolo_label(lbl)\n",
    "#         cnt.update(cls)\n",
    "#     return cnt\n",
    "\n",
    "# def list_train_stems(img_dir: Path):\n",
    "#     return {p.stem for p in img_dir.rglob(\"*\") if p.suffix.lower() in IMG_EXTS}\n",
    "\n",
    "# # ========== ë©”ì¸ ë£¨í‹´ ==========\n",
    "# def run_one_base(BASE: Path):\n",
    "#     IMG_DIR = BASE / \"images\" / \"train\"\n",
    "#     LBL_DIR = BASE / \"labels\" / \"train\"\n",
    "#     IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "#     LBL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # ìœ ë‹ˆí¬ëª… ì¶©ëŒ ë°©ì§€: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” stem ëª¨ë‘ í™•ë³´\n",
    "#     used_names = set(list_train_stems(IMG_DIR))\n",
    "\n",
    "#     # 1) ë°°ê²½ ì¦ê°•\n",
    "#     bg_list = collect_background_images(IMG_DIR, LBL_DIR)\n",
    "#     print(f\"\\n[{BASE.name}] BG images in train: {len(bg_list)}\")\n",
    "\n",
    "#     log_path = BASE / \"aug_train_log.csv\"\n",
    "#     with open(log_path, \"w\", newline=\"\") as f:\n",
    "#         csv.writer(f).writerow([\"src_img\",\"src_lbl\",\"out_img\",\"out_lbl\",\"add_per_class\",\"totals_after\"])\n",
    "\n",
    "#     for bg_img in bg_list:\n",
    "#         try:\n",
    "#             img = load_image(bg_img)\n",
    "#         except Exception:\n",
    "#             continue\n",
    "#         for _ in range(BG_AUG_MULTIPLIER):\n",
    "#             out_stem = next_unique_name(bg_img.stem, used_names, tag=\"bg\")\n",
    "#             out_img = IMG_DIR / f\"{out_stem}{bg_img.suffix.lower()}\"\n",
    "#             out_lbl = LBL_DIR / f\"{out_stem}.txt\"\n",
    "#             transformed = aug(image=img, bboxes=[], class_labels=[])\n",
    "#             cv2.imwrite(str(out_img), transformed[\"image\"])\n",
    "#             with open(out_lbl, \"w\") as f:\n",
    "#                 pass\n",
    "#     print(\"  BG augmentation done.\")\n",
    "\n",
    "#     # 2) ë¼ë²¨ ìŒ í›„ë³´(ì˜¤ì§ train)\n",
    "#     label_files = sorted(LBL_DIR.rglob(\"*.txt\"))\n",
    "#     candidates = []  # (img_path, lbl_path, file_counts, stem)\n",
    "#     skipped = 0\n",
    "#     for lbl in label_files:\n",
    "#         boxes, cls_list = read_yolo_label(lbl)\n",
    "#         if not cls_list:\n",
    "#             continue\n",
    "#         img = None\n",
    "#         for ext in IMG_EXTS:\n",
    "#             cand = IMG_DIR / f\"{lbl.stem}{ext}\"\n",
    "#             if cand.exists():\n",
    "#                 img = cand; break\n",
    "#         if img is None:\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "#         fcnt = Counter(cls_list)\n",
    "#         candidates.append((img, lbl, fcnt, lbl.stem))\n",
    "#     if not candidates:\n",
    "#         print(\"  No labeled train pairs. Skip.\")\n",
    "#         return\n",
    "#     print(f\"  Labeled train pairs: {len(candidates)}, skipped(no image match): {skipped}\")\n",
    "\n",
    "#     # 3) í˜„ì¬(train) ì¹´ìš´íŠ¸ & ê²°ì†\n",
    "#     cur = current_counts(LBL_DIR)\n",
    "#     deficit = Counter({c: max(0, TARGET.get(c, 0) - cur.get(c, 0)) for c in ALL_CLASS_IDS})\n",
    "\n",
    "#     # 4) ë°˜ë³µ ì¦ê°•(ì¤‘ë³µ ì–µì œ í¬í•¨)\n",
    "#     use_count = defaultdict(int)\n",
    "#     recent_imgs = deque(maxlen=RECENT_COOLDOWN)\n",
    "#     MAX_ITERS = 200000\n",
    "#     iters = 0\n",
    "\n",
    "#     def base_score(deficit: Counter, fcnt: Counter):\n",
    "#         return sum(deficit[c] * fcnt.get(c, 0) for c in ALL_CLASS_IDS if deficit[c] > 0)\n",
    "\n",
    "#     while any(deficit[c] > 0 for c in ALL_CLASS_IDS) and iters < MAX_ITERS:\n",
    "#         iters += 1\n",
    "#         best = None; best_score = 0.0\n",
    "\n",
    "#         for img, lbl, fcnt, stem in candidates:\n",
    "#             if img in recent_imgs:\n",
    "#                 continue\n",
    "#             if use_count[img] >= MAX_PER_IMAGE_HARD:\n",
    "#                 continue\n",
    "#             cap = effective_cap_for_file(fcnt, deficit)\n",
    "#             if use_count[img] >= cap:\n",
    "#                 continue\n",
    "\n",
    "#             sc0 = base_score(deficit, fcnt)\n",
    "#             if sc0 <= 0:\n",
    "#                 continue\n",
    "\n",
    "#             # ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜: ë§ì´ ì“´ ì´ë¯¸ì§€ëŠ” ë¶ˆë¦¬\n",
    "#             diversity_w = 1.0 / (1.0 + use_count[img])\n",
    "#             sc = sc0 * diversity_w\n",
    "\n",
    "#             if sc > best_score:\n",
    "#                 best_score = sc\n",
    "#                 best = (img, lbl, fcnt, stem)\n",
    "\n",
    "#         if not best:\n",
    "#             print(\"  No more useful candidates (after caps/cooldown). Stop.\")\n",
    "#             break\n",
    "\n",
    "#         img_path, lbl_path, fcnt, stem = best\n",
    "#         out_stem = next_unique_name(stem, used_names, tag=\"dup\")\n",
    "#         out_img = IMG_DIR / f\"{out_stem}{img_path.suffix.lower()}\"\n",
    "#         out_lbl = LBL_DIR / f\"{out_stem}.txt\"\n",
    "\n",
    "#         # ì¦ê°•\n",
    "#         img = load_image(img_path)\n",
    "#         boxes0, cls0 = read_yolo_label(lbl_path)\n",
    "#         boxes0 = yolo_sanitize(boxes0)\n",
    "\n",
    "#         transformed = aug(image=img, bboxes=boxes0, class_labels=cls0)\n",
    "#         aug_img = transformed[\"image\"]\n",
    "#         aug_boxes = transformed[\"bboxes\"]\n",
    "#         aug_cls = transformed[\"class_labels\"]\n",
    "\n",
    "#         keep_boxes, keep_cls = [], []\n",
    "#         for (x, y, w, h), c in zip(aug_boxes, aug_cls):\n",
    "#             if w > 0 and h > 0:\n",
    "#                 keep_boxes.append([float(x), float(y), float(w), float(h)])\n",
    "#                 keep_cls.append(int(float(c)))\n",
    "\n",
    "#         # ê²°ì† ë³´ê°• ì•ˆ ë˜ë©´ ì €ì¥ ìŠ¤í‚µ(ì‚¬ìš©ëŸ‰ì€ ì†Œëª¨)\n",
    "#         if not keep_cls or sum(Counter(keep_cls)[c] for c in ALL_CLASS_IDS if deficit[c] > 0) <= 0:\n",
    "#             use_count[img_path] += 1\n",
    "#             continue\n",
    "\n",
    "#         # ì €ì¥\n",
    "#         cv2.imwrite(str(out_img), aug_img)\n",
    "#         write_yolo_label(out_lbl, keep_boxes, keep_cls, allow_ids=ALLOW_IDS)\n",
    "\n",
    "#         # ì¹´ìš´íŠ¸/ê²°ì† ê°±ì‹ \n",
    "#         add_cnt = Counter(keep_cls)\n",
    "#         cur.update(add_cnt)\n",
    "#         for c in ALL_CLASS_IDS:\n",
    "#             deficit[c] = max(0, TARGET.get(c, 0) - cur.get(c, 0))\n",
    "\n",
    "#         use_count[img_path] += 1\n",
    "#         recent_imgs.append(img_path)\n",
    "\n",
    "#         # ë¡œê·¸\n",
    "#         add_str = \"{\" + \", \".join(f\"{k}:{add_cnt.get(k,0)}\" for k in ALL_CLASS_IDS) + \"}\"\n",
    "#         tot_str = \"{\" + \", \".join(f\"{k}:{cur.get(k,0)}\" for k in ALL_CLASS_IDS) + \"}\"\n",
    "#         with open(log_path, \"a\", newline=\"\") as f:\n",
    "#             csv.writer(f).writerow([str(img_path), str(lbl_path), str(out_img), str(out_lbl), add_str, tot_str])\n",
    "\n",
    "#         if iters % 50 == 0 or all(deficit[c]==0 for c in ALL_CLASS_IDS):\n",
    "#             print(f\"  [{iters}] cur={dict(cur)} deficit={dict(deficit)} last={out_stem}\")\n",
    "\n",
    "#     print(\"  Final train counts:\", dict(cur))\n",
    "#     print(\"  Remaining deficit:\", dict(deficit))\n",
    "#     print(f\"  Log: {log_path}\")\n",
    "\n",
    "# # ===== ì‹¤í–‰ =====\n",
    "# if __name__ == \"__main__\":\n",
    "#     for base in BASES:\n",
    "#         run_one_base(base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95386628",
   "metadata": {},
   "source": [
    "### ìµœì‹ ì¦ê°•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_balance_train_only_multi.py\n",
    "# - ê° good_data í´ë”ì˜ trainë§Œ ì¦ê°• (images/train, labels/train)\n",
    "# - ì¶œë ¥ë„ trainì— ì €ì¥(ì›ë³¸ ë³´ì¡´)\n",
    "# - ì¤‘ë³µ ì–µì œ: í•˜ë“œìº¡/ì¿¨ë‹¤ìš´/ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜\n",
    "# - TARGETì€ \"train ë‚´ ìµœì¢… ê°ì²´ ìˆ˜\" ëª©í‘œ(ê°ì†Œ ì‘ì—…ì€ ì•ˆ í•¨)\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict, deque\n",
    "import random, csv\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "# ========== ëŒ€ìƒ í´ë”ë“¤ ==========\n",
    "BASES = [\n",
    "    #Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250721_good_data\"),\n",
    "    #Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250725_good_data\"),\n",
    "    #Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250904_good_data\"),\n",
    "    Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/20250930_good_data\"),\n",
    "]\n",
    "\n",
    "# ========== ê³µí†µ ì„¤ì • ==========\n",
    "IMG_EXTS = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"]\n",
    "SEED = 13\n",
    "random.seed(SEED)\n",
    "\n",
    "# í´ë˜ìŠ¤ ë§¤í•‘(8í´ë˜ìŠ¤, í•„ìš”ì‹œ ë³€ê²½)\n",
    "# 0 Divot, 1 Fixed_Divot, 2 Diseased_Grass, 3 Confused_Object, 4 Pole, 5 Sprinkler, 6 Drain, 7 Golf ball\n",
    "TARGET = {\n",
    "    0: 2400,  # Divot\n",
    "    1: 3000,  # Fixed_Divot\n",
    "    2: 100,   # Diseased_Grass\n",
    "    3: 0,     # Confused_Object\n",
    "    4: 24,   # Pole\n",
    "    5: 18,    # Sprinkler\n",
    "    6: 101,   # Drain\n",
    "    7: 33,   # Golf ball\n",
    "}\n",
    "ALL_CLASS_IDS = list(range(8))\n",
    "ALLOW_IDS = set(ALL_CLASS_IDS)\n",
    "\n",
    "# --- ì¤‘ë³µ ì–µì œ íŒŒë¼ë¯¸í„° ---\n",
    "MAX_USES_BASE = 1                     # ê¸°ë³¸ ì¬ì‚¬ìš© ìƒí•œ(ë‚®ê²Œ)\n",
    "MAX_USES_BOOST_PER_CLASS = {          # ë¶€ì¡± í´ë˜ìŠ¤ì¼ ë•Œ ê°€ì‚°ì¹˜\n",
    "    0: 2, 1: 2, 2: 3, 3: 0, 4: 3, 5: 3, 6: 3, 7: 3\n",
    "}\n",
    "MAX_PER_IMAGE_HARD = 1                # í•œ ì›ë³¸ì—ì„œ ì ˆëŒ€ ìµœëŒ€ ìƒì„± ìˆ˜\n",
    "RECENT_COOLDOWN = 5                   # ìµœê·¼ ì‚¬ìš© ì›ë³¸ ì¿¨ë‹¤ìš´ ê¸¸ì´\n",
    "\n",
    "# ë°°ê²½(ë¼ë²¨ ì—†ëŠ” train ì´ë¯¸ì§€) ì¦ê°• ë°°ìˆ˜\n",
    "BG_AUG_MULTIPLIER = 3                 # ì›ë³¸ 1 + ì¦ê°• 3 = ì´ 4ë°°\n",
    "\n",
    "# ========== ìƒ‰/ê°ë§ˆ ë³€í™˜ ì •ì˜(í”Œë¦½ ì œê±°, ìƒ‰/ê°ë§ˆë§Œ) ==========\n",
    "def _build_color_ops():\n",
    "    ops = {\n",
    "        \"gamma_bright\": A.RandomGamma(gamma_limit=(55, 85), p=1.0),    # ë” ë°ê²Œ\n",
    "        \"gamma_dark\":   A.RandomGamma(gamma_limit=(120, 180), p=1.0),  # ë” ì–´ë‘¡ê²Œ\n",
    "        \"warm\": A.Compose([\n",
    "            A.ColorJitter(brightness=0.10, contrast=0.15, saturation=0.08, hue=0.015, p=1.0),\n",
    "            A.RGBShift(r_shift_limit=(8, 20), g_shift_limit=(-4, 6), b_shift_limit=(-20, -8), p=0.9),\n",
    "            A.RandomGamma(gamma_limit=(95, 120), p=1.0),\n",
    "        ], p=1.0),\n",
    "        \"cool\": A.Compose([\n",
    "            A.ColorJitter(brightness=0.08, contrast=0.12, saturation=0.06, hue=0.015, p=1.0),\n",
    "            A.RGBShift(r_shift_limit=(-20, -8), g_shift_limit=(-6, 6), b_shift_limit=(8, 20), p=0.9),\n",
    "            A.RandomGamma(gamma_limit=(95, 120), p=1.0),\n",
    "        ], p=1.0),\n",
    "        \"desat\": A.Compose([\n",
    "            A.HueSaturationValue(hue_shift_limit=0, sat_shift_limit=(-45, -25), val_shift_limit=(-5, 10), p=1.0),\n",
    "            A.RandomGamma(gamma_limit=(90, 115), p=1.0),\n",
    "        ], p=1.0),\n",
    "        \"satboost\": A.Compose([\n",
    "            A.HueSaturationValue(hue_shift_limit=0, sat_shift_limit=(20, 45), val_shift_limit=(-5, 10), p=1.0),\n",
    "            A.RandomGamma(gamma_limit=(90, 115), p=1.0),\n",
    "        ], p=1.0),\n",
    "        # í•„ìš”ì‹œ ì•„ì£¼ ë‚®ì€ í™•ë¥ ë¡œ ì›ë³¸ ìœ ì§€:\n",
    "        # \"identity\": A.NoOp(p=1.0),\n",
    "    }\n",
    "    return ops\n",
    "\n",
    "COLOR_OPS = _build_color_ops()\n",
    "\n",
    "# ì„ íƒ ê°€ì¤‘ì¹˜(ì›ë³¸ ìœ ì§€ ë¹„í™œì„±í™”: identity ì œì™¸)\n",
    "WEIGHTS = {\n",
    "    \"gamma_bright\": 1.0,\n",
    "    \"gamma_dark\":   1.0,\n",
    "    \"warm\":         1.0,\n",
    "    \"cool\":         1.0,\n",
    "    \"desat\":        0.8,\n",
    "    \"satboost\":     0.8,\n",
    "    # \"identity\":   0.05,  # ì •ë§ ì›ë³¸ ìœ ì§€ê°€ ì¡°ê¸ˆ í•„ìš”í•˜ë©´ ë‚®ì€ ê°’ìœ¼ë¡œ ì¶”ê°€\n",
    "}\n",
    "\n",
    "def build_aug(weights: dict) -> A.Compose:\n",
    "    \"\"\"ì¢Œìš°ë°˜ì „(0.5) + ìƒ‰/ê°ë§ˆ ì¤‘ ë°˜ë“œì‹œ 1ê°œ ì„ íƒ\"\"\"\n",
    "    choices = []\n",
    "    for name, t in COLOR_OPS.items():\n",
    "        w = float(weights.get(name, 0.0))\n",
    "        if w > 0:\n",
    "            t.p = w  # OneOfì˜ ìƒëŒ€ ê°€ì¤‘ì¹˜\n",
    "            choices.append(t)\n",
    "    if not choices:\n",
    "        choices = [A.NoOp(p=1.0)]\n",
    "\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.2),     # â† ë¨¼ì € í”Œë¦½ ì‹œë„\n",
    "        A.OneOf(choices, p=1.0),     # â† ê·¸ë¦¬ê³  í•­ìƒ ìƒ‰/ê°ë§ˆ ì¤‘ 1ê°œ ì ìš©\n",
    "    ], bbox_params=A.BboxParams(\n",
    "        format=\"yolo\",\n",
    "        label_fields=[\"class_labels\"],\n",
    "        min_visibility=0.25\n",
    "    ))\n",
    "\n",
    "# ìµœì¢… aug (ì—¬ê¸°ì„œ WEIGHTSë§Œ ë°”ê¾¸ë©´ ë¹„ì¤‘ì´ ì¦‰ì‹œ ë°˜ì˜ë¨)\n",
    "aug = build_aug(WEIGHTS)\n",
    "\n",
    "# ========== ìœ í‹¸ ==========\n",
    "def read_yolo_label(lbl: Path):\n",
    "    boxes, cls = [], []\n",
    "    if not lbl.exists(): return boxes, cls\n",
    "    with open(lbl, \"r\") as f:\n",
    "        for line in f:\n",
    "            p = line.strip().split()\n",
    "            if len(p) < 5: continue\n",
    "            try:\n",
    "                c = int(float(p[0]))\n",
    "            except:\n",
    "                continue\n",
    "            try:\n",
    "                x, y, w, h = map(float, p[1:5])\n",
    "            except:\n",
    "                continue\n",
    "            x = min(max(x, 0.0), 1.0); y = min(max(y, 0.0), 1.0)\n",
    "            w = min(max(w, 0.0), 1.0); h = min(max(h, 0.0), 1.0)\n",
    "            if w <= 0.0 or h <= 0.0: continue\n",
    "            boxes.append([x, y, w, h]); cls.append(c)\n",
    "    return boxes, cls\n",
    "\n",
    "def write_yolo_label(lbl_path: Path, boxes, cls, *, allow_ids=ALLOW_IDS):\n",
    "    lbl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(lbl_path, \"w\") as f:\n",
    "        for c, (x, y, w, h) in zip(cls, boxes):\n",
    "            try:\n",
    "                c_int = int(float(c))\n",
    "            except:\n",
    "                continue\n",
    "            if allow_ids is not None and c_int not in allow_ids:\n",
    "                continue\n",
    "            x = float(max(0.0, min(1.0, x)))\n",
    "            y = float(max(0.0, min(1.0, y)))\n",
    "            w = float(max(0.0, min(1.0, w)))\n",
    "            h = float(max(0.0, min(1.0, h)))\n",
    "            if w <= 0.0 or h <= 0.0:\n",
    "                continue\n",
    "            f.write(f\"{c_int} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "\n",
    "def yolo_sanitize(boxes):\n",
    "    eps = 1e-6; out = []\n",
    "    for x, y, w, h in boxes:\n",
    "        l = max(0.0, min(1.0, x - w/2.0))\n",
    "        r = max(0.0, min(1.0, x + w/2.0))\n",
    "        t = max(0.0, min(1.0, y - h/2.0))\n",
    "        b = max(0.0, min(1.0, y + h/2.0))\n",
    "        if r - l <= eps or b - t <= eps: continue\n",
    "        nx = (l + r)/2.0; ny = (t + b)/2.0; nw = (r - l); nh = (b - t)\n",
    "        nx = min(max(nx, eps), 1.0 - eps); ny = min(max(ny, eps), 1.0 - eps)\n",
    "        nw = min(max(nw, eps), 1.0 - eps); nh = min(max(nh, eps), 1.0 - eps)\n",
    "        out.append([nx, ny, nw, nh])\n",
    "    return out\n",
    "\n",
    "def collect_background_images(img_dir: Path, lbl_dir: Path):\n",
    "    stems_lbl = {p.stem for p in lbl_dir.rglob(\"*.txt\")} if lbl_dir.exists() else set()\n",
    "    bg_images = []\n",
    "    for ext in IMG_EXTS:\n",
    "        for img in img_dir.rglob(f\"*{ext}\"):\n",
    "            if img.stem not in stems_lbl:\n",
    "                bg_images.append(img)\n",
    "    return sorted(bg_images)\n",
    "\n",
    "def load_image(path: Path):\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is None:\n",
    "        raise RuntimeError(f\"Failed to read image: {path}\")\n",
    "    return img\n",
    "\n",
    "def next_unique_name(stem: str, used: set, tag: str):\n",
    "    i = 1\n",
    "    while True:\n",
    "        s = f\"{stem}_{tag}{i:04d}\"\n",
    "        if s not in used:\n",
    "            used.add(s)\n",
    "            return s\n",
    "        i += 1\n",
    "\n",
    "def effective_cap_for_file(file_cls_counter: Counter, deficit: Counter):\n",
    "    cap = MAX_USES_BASE\n",
    "    best_boost = 0\n",
    "    for c, n in file_cls_counter.items():\n",
    "        if n <= 0: continue\n",
    "        if deficit.get(c, 0) > 0:\n",
    "            best_boost = max(best_boost, MAX_USES_BOOST_PER_CLASS.get(c, 0))\n",
    "    return cap + best_boost\n",
    "\n",
    "def current_counts(label_dir: Path):\n",
    "    cnt = Counter()\n",
    "    for lbl in label_dir.rglob(\"*.txt\"):\n",
    "        _, cls = read_yolo_label(lbl)\n",
    "        cnt.update(cls)\n",
    "    return cnt\n",
    "\n",
    "def list_train_stems(img_dir: Path):\n",
    "    return {p.stem for p in img_dir.rglob(\"*\") if p.suffix.lower() in IMG_EXTS}\n",
    "\n",
    "# ========== ë©”ì¸ ë£¨í‹´ ==========\n",
    "def run_one_base(BASE: Path):\n",
    "    IMG_DIR = BASE / \"images\" / \"train\"\n",
    "    LBL_DIR = BASE / \"labels\" / \"train\"\n",
    "    IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    LBL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ìœ ë‹ˆí¬ëª… ì¶©ëŒ ë°©ì§€: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” stem ëª¨ë‘ í™•ë³´\n",
    "    used_names = set(list_train_stems(IMG_DIR))\n",
    "\n",
    "    # 1) ë°°ê²½ ì¦ê°•\n",
    "    bg_list = collect_background_images(IMG_DIR, LBL_DIR)\n",
    "    print(f\"\\n[{BASE.name}] BG images in train: {len(bg_list)}\")\n",
    "\n",
    "    log_path = BASE / \"aug_train_log.csv\"\n",
    "    with open(log_path, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"src_img\",\"src_lbl\",\"out_img\",\"out_lbl\",\"add_per_class\",\"totals_after\"])\n",
    "\n",
    "    for bg_img in bg_list:\n",
    "        try:\n",
    "            img = load_image(bg_img)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for _ in range(BG_AUG_MULTIPLIER):\n",
    "            out_stem = next_unique_name(bg_img.stem, used_names, tag=\"bg\")\n",
    "            out_img = IMG_DIR / f\"{out_stem}{bg_img.suffix.lower()}\"\n",
    "            out_lbl = LBL_DIR / f\"{out_stem}.txt\"\n",
    "            transformed = aug(image=img, bboxes=[], class_labels=[])\n",
    "            cv2.imwrite(str(out_img), transformed[\"image\"])\n",
    "            with open(out_lbl, \"w\") as f:\n",
    "                pass\n",
    "    print(\"  BG augmentation done.\")\n",
    "\n",
    "    # 2) ë¼ë²¨ ìŒ í›„ë³´(ì˜¤ì§ train)\n",
    "    label_files = sorted(LBL_DIR.rglob(\"*.txt\"))\n",
    "    candidates = []  # (img_path, lbl_path, file_counts, stem)\n",
    "    skipped = 0\n",
    "    for lbl in label_files:\n",
    "        boxes, cls_list = read_yolo_label(lbl)\n",
    "        if not cls_list:\n",
    "            continue\n",
    "        img = None\n",
    "        for ext in IMG_EXTS:\n",
    "            cand = IMG_DIR / f\"{lbl.stem}{ext}\"\n",
    "            if cand.exists():\n",
    "                img = cand; break\n",
    "        if img is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        fcnt = Counter(cls_list)\n",
    "        candidates.append((img, lbl, fcnt, lbl.stem))\n",
    "    if not candidates:\n",
    "        print(\"  No labeled train pairs. Skip.\")\n",
    "        return\n",
    "    print(f\"  Labeled train pairs: {len(candidates)}, skipped(no image match): {skipped}\")\n",
    "\n",
    "    # 3) í˜„ì¬(train) ì¹´ìš´íŠ¸ & ê²°ì†\n",
    "    cur = current_counts(LBL_DIR)\n",
    "    deficit = Counter({c: max(0, TARGET.get(c, 0) - cur.get(c, 0)) for c in ALL_CLASS_IDS})\n",
    "\n",
    "    # 4) ë°˜ë³µ ì¦ê°•(ì¤‘ë³µ ì–µì œ í¬í•¨)\n",
    "    use_count = defaultdict(int)\n",
    "    recent_imgs = deque(maxlen=RECENT_COOLDOWN)\n",
    "    MAX_ITERS = 200000\n",
    "    iters = 0\n",
    "\n",
    "    def base_score(deficit: Counter, fcnt: Counter):\n",
    "        return sum(deficit[c] * fcnt.get(c, 0) for c in ALL_CLASS_IDS if deficit[c] > 0)\n",
    "\n",
    "    while any(deficit[c] > 0 for c in ALL_CLASS_IDS) and iters < MAX_ITERS:\n",
    "        iters += 1\n",
    "        best = None; best_score = 0.0\n",
    "\n",
    "        for img, lbl, fcnt, stem in candidates:\n",
    "            if img in recent_imgs:\n",
    "                continue\n",
    "            if use_count[img] >= MAX_PER_IMAGE_HARD:\n",
    "                continue\n",
    "            cap = effective_cap_for_file(fcnt, deficit)\n",
    "            if use_count[img] >= cap:\n",
    "                continue\n",
    "\n",
    "            sc0 = base_score(deficit, fcnt)\n",
    "            if sc0 <= 0:\n",
    "                continue\n",
    "\n",
    "            # ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜: ë§ì´ ì“´ ì´ë¯¸ì§€ëŠ” ë¶ˆë¦¬\n",
    "            diversity_w = 1.0 / (1.0 + use_count[img])\n",
    "            sc = sc0 * diversity_w\n",
    "\n",
    "            if sc > best_score:\n",
    "                best_score = sc\n",
    "                best = (img, lbl, fcnt, stem)\n",
    "\n",
    "        if not best:\n",
    "            print(\"  No more useful candidates (after caps/cooldown). Stop.\")\n",
    "            break\n",
    "\n",
    "        img_path, lbl_path, fcnt, stem = best\n",
    "        out_stem = next_unique_name(stem, used_names, tag=\"dup\")\n",
    "        out_img = IMG_DIR / f\"{out_stem}{img_path.suffix.lower()}\"\n",
    "        out_lbl = LBL_DIR / f\"{out_stem}.txt\"\n",
    "\n",
    "        # ì¦ê°•\n",
    "        img = load_image(img_path)\n",
    "        boxes0, cls0 = read_yolo_label(lbl_path)\n",
    "        boxes0 = yolo_sanitize(boxes0)\n",
    "\n",
    "        transformed = aug(image=img, bboxes=boxes0, class_labels=cls0)\n",
    "        aug_img = transformed[\"image\"]\n",
    "        aug_boxes = transformed[\"bboxes\"]\n",
    "        aug_cls = transformed[\"class_labels\"]\n",
    "\n",
    "        keep_boxes, keep_cls = [], []\n",
    "        for (x, y, w, h), c in zip(aug_boxes, aug_cls):\n",
    "            if w > 0 and h > 0:\n",
    "                keep_boxes.append([float(x), float(y), float(w), float(h)])\n",
    "                keep_cls.append(int(float(c)))\n",
    "\n",
    "        # ê²°ì† ë³´ê°• ì•ˆ ë˜ë©´ ì €ì¥ ìŠ¤í‚µ(ì‚¬ìš©ëŸ‰ì€ ì†Œëª¨)\n",
    "        if not keep_cls or sum(Counter(keep_cls)[c] for c in ALL_CLASS_IDS if deficit[c] > 0) <= 0:\n",
    "            use_count[img_path] += 1\n",
    "            continue\n",
    "\n",
    "        # ì €ì¥\n",
    "        cv2.imwrite(str(out_img), aug_img)\n",
    "        write_yolo_label(out_lbl, keep_boxes, keep_cls, allow_ids=ALLOW_IDS)\n",
    "\n",
    "        # ì¹´ìš´íŠ¸/ê²°ì† ê°±ì‹ \n",
    "        add_cnt = Counter(keep_cls)\n",
    "        cur.update(add_cnt)\n",
    "        for c in ALL_CLASS_IDS:\n",
    "            deficit[c] = max(0, TARGET.get(c, 0) - cur.get(c, 0))\n",
    "\n",
    "        use_count[img_path] += 1\n",
    "        recent_imgs.append(img_path)\n",
    "\n",
    "        # ë¡œê·¸\n",
    "        add_str = \"{\" + \", \".join(f\"{k}:{add_cnt.get(k,0)}\" for k in ALL_CLASS_IDS) + \"}\"\n",
    "        tot_str = \"{\" + \", \".join(f\"{k}:{cur.get(k,0)}\" for k in ALL_CLASS_IDS) + \"}\"\n",
    "        with open(log_path, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([str(img_path), str(lbl_path), str(out_img), str(out_lbl), add_str, tot_str])\n",
    "\n",
    "        if iters % 50 == 0 or all(deficit[c]==0 for c in ALL_CLASS_IDS):\n",
    "            print(f\"  [{iters}] cur={dict(cur)} deficit={dict(deficit)} last={out_stem}\")\n",
    "\n",
    "    print(\"  Final train counts:\", dict(cur))\n",
    "    print(\"  Remaining deficit:\", dict(deficit))\n",
    "    print(f\"  Log: {log_path}\")\n",
    "\n",
    "# ===== ì‹¤í–‰ =====\n",
    "if __name__ == \"__main__\":\n",
    "    for base in BASES:\n",
    "        run_one_base(base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d40bdb9",
   "metadata": {},
   "source": [
    "### ë°ì´í„° í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_move_to_top_for_yolo_and_logs.py\n",
    "from pathlib import Path\n",
    "import shutil, csv\n",
    "\n",
    "MERGE_ROOT = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data\")\n",
    "SOURCES = [\n",
    "    MERGE_ROOT / \"20250721_good_data\",\n",
    "    MERGE_ROOT / \"20250725_good_data\",\n",
    "    MERGE_ROOT / \"20250904_good_data\",\n",
    "    MERGE_ROOT / \"20250930_good_data\",\n",
    "]\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"}\n",
    "\n",
    "def ensure_dirs():\n",
    "    for base in [\"images\", \"labels\"]:\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            (MERGE_ROOT / base / split).mkdir(parents=True, exist_ok=True)\n",
    "    (MERGE_ROOT / \"logs\").mkdir(parents=True, exist_ok=True)\n",
    "    (MERGE_ROOT / \"logs\" / \"classes_sources\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def map_images(dir_path: Path):\n",
    "    return {p.stem: p for p in dir_path.glob(\"*\") if p.is_file() and p.suffix in IMG_EXTS}\n",
    "\n",
    "def move_pair(stem: str, img_path: Path | None, lbl_path: Path | None, split: str, prefix: str):\n",
    "    new_stem = f\"{prefix}__{stem}\"\n",
    "    if img_path and img_path.exists():\n",
    "        dst_img = MERGE_ROOT / \"images\" / split / f\"{new_stem}{img_path.suffix.lower()}\"\n",
    "        shutil.move(str(img_path), dst_img)\n",
    "    if lbl_path and lbl_path.exists():\n",
    "        dst_lbl = MERGE_ROOT / \"labels\" / split / f\"{new_stem}.txt\"\n",
    "        shutil.move(str(lbl_path), dst_lbl)\n",
    "\n",
    "def move_csvs_and_classes(src_root: Path):\n",
    "    \"\"\"ì†ŒìŠ¤ í´ë” ë£¨íŠ¸ì— ìˆëŠ” csv, classes.txt ì •ë¦¬ ì´ë™\"\"\"\n",
    "    prefix = src_root.name\n",
    "    logs_dir = MERGE_ROOT / \"logs\"\n",
    "\n",
    "    # CSVë“¤ ì´ë™(+ prefix ë¶™ì—¬ ì¶©ëŒ ë°©ì§€)\n",
    "    for csv_file in src_root.glob(\"*.csv\"):\n",
    "        dst = logs_dir / f\"{prefix}__{csv_file.name}\"\n",
    "        shutil.move(str(csv_file), dst)\n",
    "\n",
    "    # classes.txt ë°±ì—… ì´ë™\n",
    "    cls = src_root / \"classes.txt\"\n",
    "    if cls.exists():\n",
    "        dst = logs_dir / \"classes_sources\" / f\"{prefix}__classes.txt\"\n",
    "        shutil.move(str(cls), dst)\n",
    "\n",
    "def merge_and_move(src_root: Path):\n",
    "    prefix = src_root.name\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        img_dir = src_root / \"images\" / split\n",
    "        lbl_dir = src_root / \"labels\" / split\n",
    "        if not img_dir.exists() and not lbl_dir.exists():\n",
    "            continue\n",
    "        img_map = map_images(img_dir) if img_dir.exists() else {}\n",
    "        lbl_map = {p.stem: p for p in lbl_dir.glob(\"*.txt\")} if lbl_dir.exists() else {}\n",
    "        stems = set(img_map) | set(lbl_map)\n",
    "        for stem in stems:\n",
    "            move_pair(stem, img_map.get(stem), lbl_map.get(stem), split, prefix)\n",
    "\n",
    "    # CSV / classes ì •ë¦¬\n",
    "    move_csvs_and_classes(src_root)\n",
    "\n",
    "    # ë¹ˆ ì†ŒìŠ¤ í´ë” ì œê±°\n",
    "    shutil.rmtree(src_root, ignore_errors=True)\n",
    "\n",
    "def read_and_unify_class_names():\n",
    "    \"\"\"logs/classes_sources ì•„ë˜ì˜ classes.txtë“¤ì„ ë¹„êµí•´ í†µì¼. ì—†ìœ¼ë©´ ê¸°ë³¸ê°’.\"\"\"\n",
    "    defaults = [\"Divot\",\"Fixed_Divot\",\"Diseased_Grass\",\"Confused_Object\",\n",
    "                \"Pole\",\"Sprinkler\",\"Drain\",\"Golf ball\"]\n",
    "    src_dir = MERGE_ROOT / \"logs\" / \"classes_sources\"\n",
    "    files = sorted(src_dir.glob(\"*_classes.txt\"))\n",
    "    if not files:\n",
    "        return defaults, \"default\"\n",
    "    # ì²« íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ í†µì¼ì„± ê²€ì‚¬\n",
    "    def read_names(p: Path):\n",
    "        return [ln.strip() for ln in p.read_text(encoding=\"utf-8\").splitlines() if ln.strip()]\n",
    "    base_names = read_names(files[0])\n",
    "    consistent = True\n",
    "    for f in files[1:]:\n",
    "        if read_names(f) != base_names:\n",
    "            consistent = False\n",
    "            break\n",
    "    if not consistent or not base_names:\n",
    "        # ë¶ˆì¼ì¹˜ ì‹œ ì²« íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ í†µì¼í•˜ë˜, ê²½ê³  ë¡œê·¸ ë‚¨ê¹€\n",
    "        warn = MERGE_ROOT / \"logs\" / \"classes_mismatch.txt\"\n",
    "        with open(warn, \"w\", encoding=\"utf-8\") as w:\n",
    "            w.write(\"WARNING: classes.txt mismatch among sources. Using first one as canonical.\\n\")\n",
    "            for f in files:\n",
    "                w.write(f\"- {f.name}\\n\")\n",
    "        return (base_names if base_names else defaults), \"mismatch\"\n",
    "    return base_names, \"ok\"\n",
    "\n",
    "def write_classes_txt(names: list[str]):\n",
    "    out = MERGE_ROOT / \"classes.txt\"\n",
    "    out.write_text(\"\\n\".join(names) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\"âœ” classes.txt -> {out}\")\n",
    "\n",
    "def write_data_yaml(names: list[str]):\n",
    "    yaml_path = MERGE_ROOT / \"data.yaml\"\n",
    "    with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"train: images/train\\n\")\n",
    "        f.write(\"val: images/val\\n\")\n",
    "        f.write(\"test: images/test\\n\\n\")\n",
    "        f.write(f\"nc: {len(names)}\\n\")\n",
    "        f.write(\"names: [\")\n",
    "        f.write(\", \".join(f\"\\\"{n}\\\"\" for n in names))\n",
    "        f.write(\"]\\n\")\n",
    "    print(f\"âœ” data.yaml -> {yaml_path}\")\n",
    "\n",
    "def quick_summary():\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        n_img = len(list((MERGE_ROOT / \"images\" / split).glob(\"*\")))\n",
    "        n_lbl = len(list((MERGE_ROOT / \"labels\" / split).glob(\"*.txt\")))\n",
    "        print(f\"[{split}] images={n_img}, labels={n_lbl}\")\n",
    "\n",
    "def merge_all_aug_logs():\n",
    "    \"\"\"logs/ ì•„ë˜ prefix ë¶™ì—¬ ì˜®ê¸´ CSV ì¤‘ aug_* ë¡œê·¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê³  source ì»¬ëŸ¼ ì¶”ê°€\"\"\"\n",
    "    logs_dir = MERGE_ROOT / \"logs\"\n",
    "    out_path = logs_dir / \"merged_aug_log.csv\"\n",
    "    candidates = [p for p in logs_dir.glob(\"*.csv\") if \"aug\" in p.name.lower()]\n",
    "    if not candidates:\n",
    "        return\n",
    "    header = None\n",
    "    rows = []\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            with open(p, newline=\"\", encoding=\"utf-8\") as f:\n",
    "                r = csv.reader(f)\n",
    "                h = next(r, None)\n",
    "                if h is None:\n",
    "                    continue\n",
    "                if header is None:\n",
    "                    header = h + [\"source_csv\"]\n",
    "                elif h != header[:-1]:\n",
    "                    # í—¤ë” ë‹¤ë¥´ë©´ ìŠ¤í‚µ(í•„ìš”ì‹œ ë§¤í•‘ ì¶”ê°€ ê°€ëŠ¥)\n",
    "                    continue\n",
    "                for row in r:\n",
    "                    rows.append(row + [p.name])\n",
    "        except Exception:\n",
    "            continue\n",
    "    if header:\n",
    "        with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(header)\n",
    "            w.writerows(rows)\n",
    "        print(f\"âœ” merged_aug_log.csv -> {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ensure_dirs()\n",
    "    for src in SOURCES:\n",
    "        if src.exists():\n",
    "            print(f\"â†’ Moving from: {src}\")\n",
    "            merge_and_move(src)\n",
    "\n",
    "    # classes í†µí•©/ìƒì„± + data.yaml\n",
    "    names, status = read_and_unify_class_names()\n",
    "    write_classes_txt(names)\n",
    "    write_data_yaml(names)\n",
    "\n",
    "    # ë¡œê·¸ ìš”ì•½ ë³‘í•©\n",
    "    merge_all_aug_logs()\n",
    "\n",
    "    quick_summary()\n",
    "    print(\"âœ… Done. All files MOVED to top-level. CSV/logs & classes organized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acefdc46",
   "metadata": {},
   "source": [
    "### í•©ì³ì§„ ë°ì´í„° ë¡œê·¸ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_split_distribution.py\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "MERGE_ROOT = Path(\"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data\")\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".WEBP\"}\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "def read_class_names():\n",
    "    f = MERGE_ROOT / \"classes.txt\"\n",
    "    if f.exists():\n",
    "        names = [ln.strip() for ln in f.read_text(encoding=\"utf-8\").splitlines() if ln.strip()]\n",
    "        if names: return names\n",
    "    # fallback\n",
    "    return [\"Divot\",\"Fixed_Divot\",\"Diseased_Grass\",\"Confused_Object\",\"Pole\",\"Sprinkler\",\"Drain\",\"Golf ball\"]\n",
    "\n",
    "CLASS_NAMES = read_class_names()\n",
    "NC = len(CLASS_NAMES)\n",
    "\n",
    "def list_images(dirpath: Path):\n",
    "    return [p for p in dirpath.glob(\"*\") if p.suffix in IMG_EXTS and p.is_file()]\n",
    "\n",
    "def count_split(split: str):\n",
    "    img_dir = MERGE_ROOT / \"images\" / split\n",
    "    lbl_dir = MERGE_ROOT / \"labels\" / split\n",
    "    img_count = 0\n",
    "    labeled_image_stems = set()\n",
    "    obj_counter = Counter()\n",
    "    unknown_counter = 0\n",
    "\n",
    "    if img_dir.exists():\n",
    "        img_count = len(list_images(img_dir))\n",
    "    if lbl_dir.exists():\n",
    "        for txt in lbl_dir.glob(\"*.txt\"):\n",
    "            stem = txt.stem\n",
    "            labeled_image_stems.add(stem)\n",
    "            for line in txt.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5: \n",
    "                    continue\n",
    "                try:\n",
    "                    cid = int(float(parts[0]))\n",
    "                except:\n",
    "                    continue\n",
    "                if 0 <= cid < NC:\n",
    "                    obj_counter[cid] += 1\n",
    "                else:\n",
    "                    unknown_counter += 1\n",
    "\n",
    "    bg_images = max(0, img_count - len(labeled_image_stems))\n",
    "    return {\n",
    "        \"images_total\": img_count,\n",
    "        \"images_labeled\": len(labeled_image_stems),\n",
    "        \"images_bg\": bg_images,\n",
    "        \"objs_per_class\": obj_counter,\n",
    "        \"objs_unknown\": unknown_counter,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # ìˆ˜ì§‘\n",
    "    results = {s: count_split(s) for s in SPLITS}\n",
    "    # ì „ì²´ í•©ì‚°\n",
    "    total_imgs = sum(results[s][\"images_total\"] for s in SPLITS)\n",
    "    total_objs = Counter()\n",
    "    for s in SPLITS:\n",
    "        total_objs.update(results[s][\"objs_per_class\"])\n",
    "    grand_objs = sum(total_objs.values())\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥\n",
    "    out_txt = MERGE_ROOT / \"split_object_report.txt\"\n",
    "    lines = []\n",
    "    lines.append(f\"Root: {MERGE_ROOT}\")\n",
    "    lines.append(f\"Classes({NC}): {', '.join(CLASS_NAMES)}\\n\")\n",
    "\n",
    "    for s in SPLITS:\n",
    "        r = results[s]\n",
    "        lines.append(f\"[{s}] images_total={r['images_total']}, \"\n",
    "                     f\"labeled={r['images_labeled']}, bg={r['images_bg']}, \"\n",
    "                     f\"objs_total={sum(r['objs_per_class'].values())}, unknown={r['objs_unknown']}\")\n",
    "        # í´ë˜ìŠ¤ë³„\n",
    "        for i, name in enumerate(CLASS_NAMES):\n",
    "            lines.append(f\"  {i:02d} {name:16s}: {r['objs_per_class'].get(i,0)}\")\n",
    "        # ë¹„ìœ¨(ì´ split ë‚´ë¶€)\n",
    "        split_obj_sum = max(1, sum(r[\"objs_per_class\"].values()))\n",
    "        ratios = \", \".join([f\"{name}:{r['objs_per_class'].get(i,0)/split_obj_sum:.3f}\" \n",
    "                            for i, name in enumerate(CLASS_NAMES)])\n",
    "        lines.append(f\"  per-class ratios: {ratios}\\n\")\n",
    "\n",
    "    # ì „ì²´ ìš”ì•½\n",
    "    lines.append(\"=== OVERALL ===\")\n",
    "    lines.append(f\"Total images: {total_imgs}\")\n",
    "    lines.append(f\"Total objects: {grand_objs}\")\n",
    "    lines.append(\"Objects per class (overall):\")\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        lines.append(f\"  {i:02d} {name:16s}: {total_objs.get(i,0)}\")\n",
    "    if grand_objs > 0:\n",
    "        ratios_overall = \", \".join([f\"{name}:{total_objs.get(i,0)/grand_objs:.3f}\" \n",
    "                                    for i, name in enumerate(CLASS_NAMES)])\n",
    "        lines.append(f\"Overall ratios: {ratios_overall}\")\n",
    "\n",
    "    out_txt.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "    # CSVë¡œë„ ì €ì¥(ìŠ¤í”Œë¦¿ë³„ ìš”ì•½)\n",
    "    out_csv = MERGE_ROOT / \"split_object_report.csv\"\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"split\",\"images_total\",\"images_labeled\",\"images_bg\",\"objs_total\"] + [f\"obj_{i}_{CLASS_NAMES[i]}\" for i in range(NC)]\n",
    "        w.writerow(header)\n",
    "        for s in SPLITS:\n",
    "            r = results[s]\n",
    "            row = [s, r[\"images_total\"], r[\"images_labeled\"], r[\"images_bg\"], sum(r[\"objs_per_class\"].values())]\n",
    "            row += [r[\"objs_per_class\"].get(i,0) for i in range(NC)]\n",
    "            w.writerow(row)\n",
    "        # overall í–‰\n",
    "        row = [\"overall\", total_imgs, sum(results[s][\"images_labeled\"] for s in SPLITS),\n",
    "               sum(results[s][\"images_bg\"] for s in SPLITS), grand_objs]\n",
    "        row += [total_objs.get(i,0) for i in range(NC)]\n",
    "        w.writerow(row)\n",
    "\n",
    "    # ì½˜ì†” ìš”ì•½\n",
    "    print(f\"âœ… Saved: {out_txt}\")\n",
    "    print(f\"âœ… Saved: {out_csv}\")\n",
    "    for s in SPLITS:\n",
    "        r = results[s]\n",
    "        print(f\"[{s}] images={r['images_total']}  labeled={r['images_labeled']}  bg={r['images_bg']}  objs={sum(r['objs_per_class'].values())}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebb9fe",
   "metadata": {},
   "source": [
    "### ptë¥¼ì´ìš©í•œ ê²°ê³¼ í™•ì¸1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# YOLOv8 Test Metrics Only\n",
    "# ==============================\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ëª¨ë¸ ê°€ì¤‘ì¹˜ & ë°ì´í„° ì„¤ì •\n",
    "WEIGHT    = \"/home/dw/ws_job_msislab/Golf_Project/runs_yolo/20251010_data_yolov8s_img640_SGD_cls0.5_box7.5_dfl1.5_rectFalse_seed_13_20251010/weights/best.pt\"\n",
    "DATA_YAML = \"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/data.yaml\"\n",
    "model = YOLO(WEIGHT)\n",
    "\n",
    "\n",
    "# --- test split í‰ê°€ ---\n",
    "results = model.val( \n",
    "    data=DATA_YAML, # data.yaml (train/val/test ê²½ë¡œ í¬í•¨) \n",
    "    split=\"test\", # test ì„¸íŠ¸ ì‚¬ìš© \n",
    "    imgsz= 640, # ì…ë ¥ í•´ìƒë„ \n",
    "    batch= 1, # ë°°ì¹˜ í¬ê¸° ## 640 \n",
    "    rect = True, # default= True falseë¡œ í•˜ë©´ ì´ë¯¸ì§€ í¬ê¸° ìƒê°ì—†ì´ ë¬´ì¡°ê±´ ì •ì‚¬ê°í˜•ìœ¼ë¡œ ë§Œë“¬ \n",
    "    device=0 # GPU ID (ì—†ìœ¼ë©´ \"cpu\") \n",
    ")\n",
    "\n",
    "# # --- Test split í‰ê°€ ---\n",
    "# results = model.val(\n",
    "#     # ===== ë°ì´í„° & ë””ë°”ì´ìŠ¤ =====\n",
    "#     data=DATA_YAML,    # data.yaml (train/val/test ê²½ë¡œ í¬í•¨)\n",
    "#     split=\"test\",      # \"val\" | \"test\" | \"train\" ì„ íƒ\n",
    "#     imgsz=640,         # (int) ì…ë ¥ í•´ìƒë„ (default=640)\n",
    "#     batch=1,          # (int) ë°°ì¹˜ í¬ê¸° (default=16)\n",
    "#     rect = True,         # (bool) aspect ratio ìœ ì§€ ì—¬ë¶€ (default=True)\n",
    "#     device=0,          # (int|str|list) ë””ë°”ì´ìŠ¤ (default=None â†’ ìë™ ì„ íƒ)\n",
    "\n",
    "#     # ===== NMS / ì„ê³„ê°’ =====\n",
    "#     conf=0.45,         # (float) confidence threshold (default=0.25)\n",
    "#     iou=0.5,           # (float) NMS IoU threshold (default=0.6)\n",
    "#     # max_det=300,       # (int) ì´ë¯¸ì§€ë‹¹ ìµœëŒ€ íƒì§€ ê°œìˆ˜ (default=300)\n",
    "#     # agnostic_nms=False,# (bool) í´ë˜ìŠ¤ ë¬´ê´€ NMS (default=False)\n",
    "\n",
    "#     # # ===== í´ë˜ìŠ¤ ê´€ë ¨ =====\n",
    "#     # single_cls=False,  # (bool) ëª¨ë“  í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ë¡œ í†µí•© (default=False)\n",
    "#     # classes=None,      # (list[int]) íŠ¹ì • í´ë˜ìŠ¤ë§Œ í‰ê°€ (default=None â†’ ì „ì²´)\n",
    "\n",
    "#     # # ===== ê²°ê³¼ ì €ì¥ ì˜µì…˜ =====\n",
    "#     # save_json=False,   # (bool) COCO json ì €ì¥ (default=False)\n",
    "#     # save_txt=False,    # (bool) txt ì €ì¥ (default=False)\n",
    "#     # save_conf=False,   # (bool) confidence ê°’ë„ ì €ì¥ (default=False)\n",
    "#     # save_hybrid=False, # (bool) ë¼ë²¨+ì˜ˆì¸¡ ë³‘í•© ì €ì¥ (default=False)\n",
    "#     # plots=False,       # (bool) PR/F1 Curve ë“± ê·¸ë˜í”„ ì €ì¥ (default=False)\n",
    "#     # verbose=False      # (bool) ìƒì„¸ ë¡œê·¸ ì¶œë ¥ (default=False)\n",
    "# )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d06e1",
   "metadata": {},
   "source": [
    "### ptë¥¼ í™œìš©í•œ ê²°ê³¼í™•ì¸2 classë³„ conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# YOLOv8 Test Inference & Metrics (conf / iou ì„¤ì • ê°€ëŠ¥ ë²„ì „)\n",
    "# ==============================\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 1) ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "WEIGHT  = \"/home/dw/ws_job_msislab/Golf_Project/runs_yolo/20250918_merge_data_yolov8s_sgd/weights/best.pt\"\n",
    "DATASET = \"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20250918_merge_data\"\n",
    "model = YOLO(WEIGHT)\n",
    "\n",
    "out_dir = Path(\"runs_yolo/0918_0918_test_sgd\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# (ì„ íƒ) í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘: ì‹œê°í™” ë¼ë²¨ì— ì‚¬ìš©\n",
    "CLASS_NAMES = {\n",
    "    0: \"Divot\",\n",
    "    1: \"Fixed_Divot\",\n",
    "    2: \"Diseased_Grass\",\n",
    "    3: \"Confused_Object\",\n",
    "    4: \"Pole\",\n",
    "    5: \"Sprinkler\",\n",
    "    6: \"Drain\",\n",
    "    7: \"Golf ball\",\n",
    "}\n",
    "# â›” model.names = CLASS_NAMES  # â† ì´ ì¤„ì€ ì œê±° (ì½ê¸° ì „ìš©ì´ë¼ ì—ëŸ¬)\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ìµœì†Œ conf ì„ê³„ì¹˜\n",
    "MIN_CONF_PER_CLASS = {\n",
    "    0: 0.45,\n",
    "    1: 0.45,\n",
    "    2: 0.60,\n",
    "    3: 0.60,\n",
    "    4: 0.75,\n",
    "    5: 0.75,\n",
    "    6: 0.75,\n",
    "    7: 0.60,\n",
    "}\n",
    "\n",
    "GLOBAL_CONF = 0.001\n",
    "NMS_IOU = 0.40\n",
    "MAX_DET = 200\n",
    "\n",
    "pred_iter = model.predict(\n",
    "    source=str(Path(DATASET) / \"images\" / \"test\"),\n",
    "    imgsz=1024,\n",
    "    conf=GLOBAL_CONF,\n",
    "    iou=NMS_IOU,\n",
    "    max_det=MAX_DET,\n",
    "    device=0,\n",
    "    stream=True,\n",
    "    save=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "for res in pred_iter:\n",
    "    im0 = res.orig_img.copy()\n",
    "    p = Path(res.path)\n",
    "\n",
    "    if res.boxes is None or len(res.boxes) == 0:\n",
    "        cv2.imwrite(str(out_dir / p.name), im0)\n",
    "        continue\n",
    "\n",
    "    boxes = res.boxes\n",
    "    xyxy = boxes.xyxy.cpu().numpy()                 # (N,4)\n",
    "    conf = boxes.conf.cpu().numpy()                 # (N,)\n",
    "    cls  = boxes.cls.cpu().numpy().astype(np.int32) # (N,)\n",
    "\n",
    "    # í´ë˜ìŠ¤ë³„ conf ì„ê³„ì¹˜ ì ìš©\n",
    "    keep = [i for i, c in enumerate(cls) if conf[i] >= MIN_CONF_PER_CLASS.get(int(c), 0.5)]\n",
    "\n",
    "    annotator = Annotator(im0, line_width=2)\n",
    "    for i in keep:\n",
    "        c = int(cls[i])\n",
    "        lbl = f\"{CLASS_NAMES.get(c, str(c))} {conf[i]:.2f}\"\n",
    "        annotator.box_label(xyxy[i], lbl, color=colors(c, True))\n",
    "\n",
    "    cv2.imwrite(str(out_dir / p.name), annotator.result())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813f0ef",
   "metadata": {},
   "source": [
    "### í•™ìŠµ sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a288bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‹¤í—˜ í´ë” ì´ë¦„: 20251010_data_yolov8s_img1024_SGD_cls0.5_box7.5_dfl1.5_rectFalse_seed_13_20251013\n",
      "New https://pypi.org/project/ultralytics/8.3.213 available ğŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.202 ğŸš€ Python-3.10.12 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 4060 Ti, 7932MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=disk, cfg=None, classes=None, close_mosaic=0, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=500, erasing=0.0, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=0, half=False, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=20251010_data_yolov8s_img1024_SGD_cls0.5_box7.5_dfl1.5_rectFalse_seed_13_20251013, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=50, perspective=0, plots=True, pose=12.0, pretrained=True, profile=False, project=/home/dw/ws_job_msislab/Golf_Project/runs_yolo, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/dw/ws_job_msislab/Golf_Project/runs_yolo/20251010_data_yolov8s_img1024_SGD_cls0.5_box7.5_dfl1.5_rectFalse_seed_13_20251013, save_frames=False, save_json=False, save_period=1, save_txt=False, scale=0.0, seed=13, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.0, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0, warmup_epochs=0, warmup_momentum=0, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2119144  ultralytics.nn.modules.head.Detect           [8, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,138,696 parameters, 11,138,680 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1022.9Â±241.2 MB/s, size: 1269.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/labels/train.cache... 6769 images, 341 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 6854/6854 13.8Mit/s 0.0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=1024 at 60.0% CUDA memory utilization.\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 4060 Ti) 7.75G total, 0.70G reserved, 0.29G allocated, 6.76G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "    11138696       73.37         1.283         9.041         14.77      (1, 3, 1024, 1024)                    list\n",
      "    11138696       146.7         1.921         14.63         27.08      (2, 3, 1024, 1024)                    list\n",
      "    11138696       293.5         3.223         28.89         53.72      (4, 3, 1024, 1024)                    list\n",
      "    11138696         587         5.492         68.72         120.7      (8, 3, 1024, 1024)                    list\n",
      "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.75 GiB of which 29.62 MiB is free. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Process 3966224 has 156.32 MiB memory in use. Of the allocated memory 6.88 GiB is allocated by PyTorch, and 78.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 5 for CUDA:0 4.72G/7.75G (61%) âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 990.5Â±618.3 MB/s, size: 1556.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/labels/train.cache... 6769 images, 341 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 6854/6854 17.2Mit/s 0.0s\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (56.5GB Disk): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6854/6854 118.7Kit/s 0.1s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 641.0Â±93.2 MB/s, size: 1794.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/labels/val.cache... 916 images, 24 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 940/940 1.5Mit/s 0.0ss\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (7.7GB Disk): 100% â”â”â”â”â”â”â”â”â”â”â”â” 940/940 94.7Kit/s 0.0s\n",
      "Plotting labels to /home/dw/ws_job_msislab/Golf_Project/runs_yolo/20251010_data_yolov8s_img1024_SGD_cls0.5_box7.5_dfl1.5_rectFalse_seed_13_20251013/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005078125), 63 bias(decay=0.0)\n",
      "Image sizes 1024 train, 1024 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/dw/ws_job_msislab/Golf_Project/runs_yolo/20251010_data_yolov8s_img1024_SGD_cls0.5_box7.5_dfl1.5_rectFalse_seed_13_20251013\u001b[0m\n",
      "Starting training for 500 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/500      2.95G      1.969      3.405      1.488          7       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:40<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 13.8it/s 6.8s0.1s\n",
      "                   all        940       2941      0.658      0.443      0.481      0.214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/500      3.23G      1.761      1.298      1.352         26       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.568      0.565      0.598      0.266\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/500      3.23G      1.687      1.139      1.314         20       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.731      0.583      0.609      0.253\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/500      3.23G      1.623      1.036       1.28          9       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.751      0.563      0.617      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/500      3.25G      1.563     0.9541      1.246         15       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.617      0.626      0.657      0.296\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/500      3.25G      1.512     0.8862      1.222          9       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.688      0.635      0.674      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/500      3.25G       1.46     0.8251      1.192          7       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.664      0.649       0.69      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/500      3.25G      1.413     0.7818      1.166         10       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941       0.72      0.664      0.704      0.347\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/500      3.26G      1.365     0.7454      1.143          7       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 13.9it/s 6.8s0.1s\n",
      "                   all        940       2941      0.724      0.621      0.668      0.333\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/500      3.26G      1.322     0.7048      1.122         18       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.726      0.648      0.682      0.314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/500      3.26G      1.279     0.6752        1.1         11       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.727      0.661      0.696      0.358\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/500      3.28G      1.232     0.6421      1.082         18       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.676      0.669       0.69      0.347\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/500      3.28G      1.195     0.6197      1.061         13       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.735       0.63      0.704      0.349\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/500      3.28G      1.162     0.5924      1.045          8       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.772      0.683      0.719      0.364\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     15/500      3.28G      1.122     0.5713       1.03         10       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.6it/s 2:39<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 14.0it/s 6.7s0.1s\n",
      "                   all        940       2941      0.741      0.632      0.684      0.341\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     16/500      3.28G      1.088     0.5592      1.016          7       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.2it/s 2:48<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 12.9it/s 7.3s0.2s\n",
      "                   all        940       2941      0.795      0.603      0.694      0.338\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     17/500      3.28G      1.054     0.5363          1         24       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.0it/s 2:52<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 12.8it/s 7.3s0.2s\n",
      "                   all        940       2941      0.783      0.623      0.696      0.348\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     18/500      3.28G      1.017     0.5166     0.9846          6       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 7.9it/s 2:53<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 12.7it/s 7.4s0.2s\n",
      "                   all        940       2941      0.744      0.662      0.695      0.343\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     19/500      3.28G      1.012     0.5216     0.9816          9       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 7.9it/s 2:53<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 12.5it/s 7.5s0.2s\n",
      "                   all        940       2941      0.723      0.681      0.699      0.335\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     20/500      3.28G     0.9795     0.4978     0.9692         15       1024: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1371/1371 8.0it/s 2:51<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 13.7it/s 6.9s0.1s\n",
      "                   all        940       2941      0.766      0.631      0.698      0.342\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     21/500      3.28G      0.942      0.481     0.9545          6       1024: 83% â”â”â”â”â”â”â”â”â”â•¸â”€â”€ 1135/1371 12.3it/s 2:16<19.1s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# ëª¨ë¸ í•™ìŠµ ì‹œì‘\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[1;32m     55\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(MODEL_NAME)  \u001b[38;5;66;03m# â† ì—¬ê¸°ë§Œ ë°”ë€Œë©´ ë©ë‹ˆë‹¤\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ë°ì´í„° & í•˜ë“œì›¨ì–´ ì„¤ì •\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_YAML\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# (str) í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ê²½ë¡œì™€ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ë‹´ì€ data.yaml\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# (int) ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì •ì‚¬ê°í˜•ìœ¼ë¡œ resizeí•  í¬ê¸° (ex: 640)\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (int|float) ë°°ì¹˜ í¬ê¸°: -1 â†’ GPU ë©”ëª¨ë¦¬ ê¸°ì¤€ ìë™ íƒìƒ‰\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (int|str|list) GPU ì¸ë±ìŠ¤/ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” 'cpu'/'mps'\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# (int) DataLoader ë³‘ë ¬ ë¡œë“œ worker ìˆ˜ (CPU ì½”ì–´ ìƒí™©ì— ë§ì¶°)\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# í•™ìŠµ ìŠ¤ì¼€ì¤„/ìµœì í™”\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# (int) ì´ í•™ìŠµ epoch ìˆ˜\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# (int) val metric ê°œì„  ì—†ì„ ë•Œ ì¡°ê¸°ì¢…ë£Œ ëŒ€ê¸° epoch\u001b[39;49;00m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (float) ì´ˆê¸° í•™ìŠµë¥  (SGDëŠ” 0.01 ê¶Œì¥, Adam ê³„ì—´ì€ ë” ë‚®ê²Œ) 0.01 0.001\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlrf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (float) ìµœì¢… í•™ìŠµë¥  ë¹„ìœ¨ (lr0 * lrf = ë§ˆì§€ë§‰ epoch í•™ìŠµë¥ )\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.937\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# (float) SGD ëª¨ë©˜í…€ ë˜ëŠ” Adam Î²1 ê°’   \u001b[39;49;00m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# (float) L2 ì •ê·œí™” ê³„ìˆ˜ (ê³¼ì í•© ë°©ì§€)   0.01  0.0005\u001b[39;49;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPTIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# (str) ìµœì í™” ì•Œê³ ë¦¬ì¦˜: 'SGD','Adam','AdamW' ë“±\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcos_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# (bool|int) ì½”ì‚¬ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ì‚¬ìš© ì—¬ë¶€ (0 â†’ ë¹„í™œì„±)\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# (float) ì›Œë°ì—… epoch ìˆ˜ (0 â†’ ì›Œë°ì—… ì—†ìŒ)\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# (float) ì›Œë°ì—… ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ì´ˆê¸° ëª¨ë©˜í…€\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_bias_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# (float) ì›Œë°ì—… ë‹¨ê³„ì—ì„œ bias íŒŒë¼ë¯¸í„° í•™ìŠµë¥ \u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ì†ì‹¤ ê°€ì¤‘ì¹˜\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBOX_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# (float) ë°”ìš´ë”© ë°•ìŠ¤ íšŒê·€ ì†ì‹¤ ê°€ì¤‘ì¹˜\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLS_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# (float) í´ë˜ìŠ¤ ë¶„ë¥˜ ì†ì‹¤ ê°€ì¤‘ì¹˜\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdfl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDFL_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# (float) Distribution Focal Loss ê°€ì¤‘ì¹˜ (YOLOv8 box refinement)\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ë°ì´í„° ì¦ê°•(Online Augmentation)\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmosaic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# (float) mosaic ì¦ê°• ë¹„ìœ¨ (0 â†’ ì‚¬ìš© ì•ˆí•¨)\u001b[39;49;00m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclose_mosaic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# (int) ë§ˆì§€ë§‰ N epoch ë™ì•ˆ mosaic ë¹„í™œì„± (0 â†’ ë¹„í™œì„±í™”)\u001b[39;49;00m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# (float) mixup ë¹„ìœ¨\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_paste\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# (float) ê°ì²´ copy-paste ë¹„ìœ¨\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhsv_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# (float) Hue ìƒ‰ì¡° ë³€ë™ ë²”ìœ„\u001b[39;49;00m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhsv_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# (float) ì±„ë„(Saturation) ë³€ë™ ë²”ìœ„\u001b[39;49;00m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhsv_v\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# (float) ë°ê¸°(Value) ë³€ë™ ë²”ìœ„\u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# (float) íšŒì „ ê°ë„ ë²”ìœ„\u001b[39;49;00m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtranslate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# (float) ì´ë¯¸ì§€ í‰í–‰ì´ë™ ë¹„ìœ¨\u001b[39;49;00m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# (float) ìŠ¤ì¼€ì¼ ë³€í˜• ë¹„ìœ¨\u001b[39;49;00m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# (float) ê¸°ìš¸ì´ê¸° ë³€í˜• ë¹„ìœ¨\u001b[39;49;00m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperspective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# (float) ì›ê·¼ ë³€í˜• ë¹„ìœ¨\u001b[39;49;00m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43merasing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# (float) ëœë¤ ì˜ì—­ ì‚­ì œ ë¹„ìœ¨\u001b[39;49;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ê³ ê¸‰ ì˜µì…˜ ë° ì¬í˜„ì„±\u001b[39;49;00m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRECT_MODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# (bool) ë°°ì¹˜ ë‹¨ìœ„ë¡œ aspect ratio ìœ ì§€ ì—¬ë¶€ (train ê¸°ë³¸ False)\u001b[39;49;00m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# (bool|'ram'|'disk') ì´ë¯¸ì§€/ë¼ë²¨ ìºì‹œ: 'disk'ëŠ” I/O ì ˆê°\u001b[39;49;00m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# (bool) ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í•™ìŠµ í™œì„±í™” (0 â†’ ë¹„í™œì„±)\u001b[39;49;00m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# (bool) ëª¨ë“  í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ë¡œ í†µí•©\u001b[39;49;00m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# (list[int]|int) íŠ¹ì • í´ë˜ìŠ¤ë§Œ í•™ìŠµ. 0 â†’ ì „ì²´ í´ë˜ìŠ¤ ì‚¬ìš©\u001b[39;49;00m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# (float) ë°ì´í„°ì…‹ ì‚¬ìš© ë¹„ìœ¨ (1.0 â†’ ì „ì²´ ë°ì´í„°)\u001b[39;49;00m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# (int) ëœë¤ ì‹œë“œ (ì¬í˜„ì„±)\u001b[39;49;00m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# (bool) ì™„ì „ ê²°ì •ë¡ ì  ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©\u001b[39;49;00m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (bool) ìë™ í˜¼í•©ì •ë°€ë„(fp16) í•™ìŠµ\u001b[39;49;00m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (int|list) ì²˜ìŒ Nê°œ ë ˆì´ì–´ í˜¹ì€ íŠ¹ì • ë ˆì´ì–´ ì¸ë±ìŠ¤ ê³ ì •\u001b[39;49;00m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# (bool|str) torch.compile ì‚¬ìš© (0 â†’ ë¹„í™œì„±)\u001b[39;49;00m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# (bool) ONNX/TensorRT í”„ë¡œíŒŒì¼ë§\u001b[39;49;00m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (bool) í•™ìŠµ ì¤‘ ê²€ì¦ ì‹¤í–‰\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# (bool) ì²´í¬í¬ì¸íŠ¸ ë° ìµœì¢… ê°€ì¤‘ì¹˜ ì €ì¥\u001b[39;49;00m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_PERIOD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (int) ëª‡ epochë§ˆë‹¤ ì¤‘ê°„ ê°€ì¤‘ì¹˜ ì €ì¥ (0 â†’ ë¹„í™œì„±)\u001b[39;49;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ê²°ê³¼ ì €ì¥\u001b[39;49;00m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================\u001b[39;49;00m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/dw/ws_job_msislab/Golf_Project/runs_yolo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (str) ìƒìœ„ í”„ë¡œì íŠ¸ í´ë”\u001b[39;49;00m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# (str) í”„ë¡œì íŠ¸ í•˜ìœ„ í´ë”ëª…\u001b[39;49;00m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# (bool) ë™ì¼ ì´ë¦„ í´ë”ê°€ ìˆì–´ë„ ë®ì–´ì“°ê¸° í—ˆìš©\u001b[39;49;00m\n\u001b[1;32m    132\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# ğŸ”¹ sum4 ê°€ì¤‘ì¹˜ (ì‚¬ìš©ìê°€ ì§ì ‘ ìˆ˜ì • ê°€ëŠ¥)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m WEIGHTS_SUM4 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.20\u001b[39m,\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m:    \u001b[38;5;241m0.30\u001b[39m,\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap50\u001b[39m\u001b[38;5;124m\"\u001b[39m:     \u001b[38;5;241m0.25\u001b[39m,\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap5095\u001b[39m\u001b[38;5;124m\"\u001b[39m:   \u001b[38;5;241m0.25\u001b[39m,\n\u001b[1;32m    141\u001b[0m }\n",
      "File \u001b[0;32m~/venv/golf_venv/lib/python3.10/site-packages/ultralytics/engine/model.py:800\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m--> 800\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/venv/golf_venv/lib/python3.10/site-packages/ultralytics/engine/trainer.py:235\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/golf_venv/lib/python3.10/site-packages/ultralytics/engine/trainer.py:420\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# decouple inference and loss calculations for torch.compile convenience\u001b[39;00m\n\u001b[1;32m    419\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 420\u001b[0m loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[43munwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/venv/golf_venv/lib/python3.10/site-packages/ultralytics/nn/tasks.py:339\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/golf_venv/lib/python3.10/site-packages/ultralytics/utils/loss.py:258\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m dtype \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    257\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 258\u001b[0m imgsz \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# image size (h,w)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m anchor_points, stride_tensor \u001b[38;5;241m=\u001b[39m make_anchors(feats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from ultralytics.utils.torch_utils import strip_optimizer\n",
    "import csv, math\n",
    "import shutil   # best_map50 ì €ì¥ìš©\n",
    "\n",
    "# ==============================\n",
    "# ì‹¤í—˜ ê´€ë ¨ ê¸°ë³¸ ì„¤ì •\n",
    "# ==============================\n",
    "\n",
    "# ë°ì´í„° ì„¤ì •: train/val ê²½ë¡œ, í´ë˜ìŠ¤ ìˆ˜/ì´ë¦„ì´ ì •í™•í•´ì•¼ í•¨\n",
    "#DATA_YAML = \"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data_only_new_camera/data.yaml\"\n",
    "DATA_YAML = \"/home/dw/ws_job_msislab/Golf_Project/data/for_study/20251010_merge_data/data.yaml\"\n",
    "\n",
    "# ëª¨ë¸ ì„ íƒ: s(ê°€ë³ê³  ë¹ ë¦„) â†’ ê²°ê³¼ ì¢‹ì•„ì§€ë©´ mìœ¼ë¡œ í™•ì¥\n",
    "MODEL_NAME = \"yolov8s.pt\"\n",
    "\n",
    "# ====== ì‚¬ìš©ì ì§€ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ======\n",
    "IMG_SIZE  = 1024        # ì…ë ¥ í•´ìƒë„(ì •ì‚¬ê°). ì†Œë¬¼ì²´ â†‘. OOM ì‹œ 896/768ë¡œ   \n",
    "OPTIM     = \"SGD\"      # Optimizer ì„ íƒ (\"SGD\",\"AdamW\",\"Adam\" ë“±)\n",
    "CLS_W     = 0.5        # í´ë˜ìŠ¤ ë¶„ë¥˜ ì†ì‹¤ ê°€ì¤‘(ê¸°ë³¸â‰ˆ0.5)\n",
    "BOX_W     = 7.5       # bbox íšŒê·€ ì†ì‹¤ ê°€ì¤‘(ê¸°ë³¸â‰ˆ7.5)|\n",
    "DFL_W     = 1.5       # Distribution Focal Loss ê°€ì¤‘(ê¸°ë³¸â‰ˆ1.5)\n",
    "EPOCHS    = 500          # í•„ìš”ì‹œ ì¡°ì •\n",
    "DATA_TAG  = \"20251010\" # ë°ì´í„°ì…‹ ë‚ ì§œ(ë°ì´í„°ì…‹ì— ë§ê²Œ ìˆ˜ì •)\n",
    "RECT_MODE = False      # âœ… rect ì„¤ì •(True / False) â†’ í´ë”ëª…ì—ë„ ê¸°ë¡ë¨ ë””í´íŠ¸ëŠ” False\n",
    "SEED = 13\n",
    "\n",
    "\n",
    "# ì˜¤ëŠ˜ ë‚ ì§œ ìë™ ì‚½ì…\n",
    "TODAY     = datetime.now().strftime(\"%Y%m%d\")   # ì˜ˆ: 20250921\n",
    "\n",
    "# ==============================\n",
    "# ì‹¤í–‰ ì˜µì…˜\n",
    "# ==============================\n",
    "SAVE_PERIOD   = 1          # epoch*.pt ì €ì¥ ì£¼ê¸°(1ì´ë©´ ë§¤ ì—í¬í¬ ì €ì¥)\n",
    "STRIP_COPIES  = True       # ë³µì‚¬ë³¸ì„ ì¶”ë¡ ìš©ìœ¼ë¡œ ìŠ¬ë¦¼í™”(optimizer ë“± ì œê±°)\n",
    "CLEAN_EPOCHS  = True       # ë³µì‚¬/ìš”ì•½ í›„ ë‚¨ì€ epoch*.pt ì‚­ì œ(ìš©ëŸ‰ ì ˆê°)\n",
    "\n",
    "# ==============================\n",
    "# ì¶œë ¥ í´ë” ì´ë¦„ ìë™ ìƒì„±\n",
    "#    ì˜ˆ: 20250919_yolov8s_img896_SGD_cls0.5_box7.5_dfl1.5_rectFalse_20250921\n",
    "# ==============================\n",
    "exp_name = (\n",
    "    f\"{DATA_TAG}_data_yolov8s_img{IMG_SIZE}_{OPTIM}\"\n",
    "    f\"_cls{CLS_W}_box{BOX_W}_dfl{DFL_W}_rect{str(RECT_MODE)}_seed_{SEED}_{TODAY}\"\n",
    ")\n",
    "print(\"ì‹¤í—˜ í´ë” ì´ë¦„:\", exp_name)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
    "# ==============================\n",
    "model = YOLO(MODEL_NAME)  # â† ì—¬ê¸°ë§Œ ë°”ë€Œë©´ ë©ë‹ˆë‹¤\n",
    "\n",
    "results = model.train(\n",
    "    # =========================\n",
    "    # ë°ì´í„° & í•˜ë“œì›¨ì–´ ì„¤ì •\n",
    "    # =========================\n",
    "    data=DATA_YAML,          # (str) í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ê²½ë¡œì™€ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ë‹´ì€ data.yaml\n",
    "    imgsz=IMG_SIZE,          # (int) ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì •ì‚¬ê°í˜•ìœ¼ë¡œ resizeí•  í¬ê¸° (ex: 640)\n",
    "    batch=-1,                # (int|float) ë°°ì¹˜ í¬ê¸°: -1 â†’ GPU ë©”ëª¨ë¦¬ ê¸°ì¤€ ìë™ íƒìƒ‰\n",
    "    device=0,                # (int|str|list) GPU ì¸ë±ìŠ¤/ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” 'cpu'/'mps'\n",
    "    workers=8,               # (int) DataLoader ë³‘ë ¬ ë¡œë“œ worker ìˆ˜ (CPU ì½”ì–´ ìƒí™©ì— ë§ì¶°)\n",
    "\n",
    "    # =========================\n",
    "    # í•™ìŠµ ìŠ¤ì¼€ì¤„/ìµœì í™”\n",
    "    # =========================\n",
    "    epochs=EPOCHS,              # (int) ì´ í•™ìŠµ epoch ìˆ˜\n",
    "    patience=50,             # (int) val metric ê°œì„  ì—†ì„ ë•Œ ì¡°ê¸°ì¢…ë£Œ ëŒ€ê¸° epoch\n",
    "    lr0=0.01,                # (float) ì´ˆê¸° í•™ìŠµë¥  (SGDëŠ” 0.01 ê¶Œì¥, Adam ê³„ì—´ì€ ë” ë‚®ê²Œ) 0.01 0.001\n",
    "    lrf=0.01,                # (float) ìµœì¢… í•™ìŠµë¥  ë¹„ìœ¨ (lr0 * lrf = ë§ˆì§€ë§‰ epoch í•™ìŠµë¥ )\n",
    "    momentum=0.937,          # (float) SGD ëª¨ë©˜í…€ ë˜ëŠ” Adam Î²1 ê°’   \n",
    "    weight_decay=0.0005,       # (float) L2 ì •ê·œí™” ê³„ìˆ˜ (ê³¼ì í•© ë°©ì§€)   0.01  0.0005\n",
    "\n",
    "    optimizer=OPTIM,         # (str) ìµœì í™” ì•Œê³ ë¦¬ì¦˜: 'SGD','Adam','AdamW' ë“±\n",
    "    cos_lr=False,            # (bool|int) ì½”ì‚¬ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ì‚¬ìš© ì—¬ë¶€ (0 â†’ ë¹„í™œì„±)\n",
    "    warmup_epochs=0,         # (float) ì›Œë°ì—… epoch ìˆ˜ (0 â†’ ì›Œë°ì—… ì—†ìŒ)\n",
    "    warmup_momentum=0,       # (float) ì›Œë°ì—… ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ì´ˆê¸° ëª¨ë©˜í…€\n",
    "    warmup_bias_lr=0,        # (float) ì›Œë°ì—… ë‹¨ê³„ì—ì„œ bias íŒŒë¼ë¯¸í„° í•™ìŠµë¥ \n",
    "\n",
    "    # =========================\n",
    "    # ì†ì‹¤ ê°€ì¤‘ì¹˜\n",
    "    # =========================\n",
    "    box=BOX_W,                 # (float) ë°”ìš´ë”© ë°•ìŠ¤ íšŒê·€ ì†ì‹¤ ê°€ì¤‘ì¹˜\n",
    "    cls=CLS_W,                 # (float) í´ë˜ìŠ¤ ë¶„ë¥˜ ì†ì‹¤ ê°€ì¤‘ì¹˜\n",
    "    dfl=DFL_W,                 # (float) Distribution Focal Loss ê°€ì¤‘ì¹˜ (YOLOv8 box refinement)\n",
    "\n",
    "    # =========================\n",
    "    # ë°ì´í„° ì¦ê°•(Online Augmentation)\n",
    "    # =========================\n",
    "    mosaic=0.0,              # (float) mosaic ì¦ê°• ë¹„ìœ¨ (0 â†’ ì‚¬ìš© ì•ˆí•¨)\n",
    "    close_mosaic=0,          # (int) ë§ˆì§€ë§‰ N epoch ë™ì•ˆ mosaic ë¹„í™œì„± (0 â†’ ë¹„í™œì„±í™”)\n",
    "    mixup=0.0,               # (float) mixup ë¹„ìœ¨\n",
    "    copy_paste=0.0,          # (float) ê°ì²´ copy-paste ë¹„ìœ¨\n",
    "    hsv_h=0.0,             # (float) Hue ìƒ‰ì¡° ë³€ë™ ë²”ìœ„\n",
    "    hsv_s=0.0,               # (float) ì±„ë„(Saturation) ë³€ë™ ë²”ìœ„\n",
    "    hsv_v=0.0,               # (float) ë°ê¸°(Value) ë³€ë™ ë²”ìœ„\n",
    "    degrees=0.0,             # (float) íšŒì „ ê°ë„ ë²”ìœ„\n",
    "    translate=0.0,           # (float) ì´ë¯¸ì§€ í‰í–‰ì´ë™ ë¹„ìœ¨\n",
    "    scale=0.0,               # (float) ìŠ¤ì¼€ì¼ ë³€í˜• ë¹„ìœ¨\n",
    "    shear=0.0,               # (float) ê¸°ìš¸ì´ê¸° ë³€í˜• ë¹„ìœ¨\n",
    "    perspective=0,           # (float) ì›ê·¼ ë³€í˜• ë¹„ìœ¨\n",
    "    erasing=0.0,             # (float) ëœë¤ ì˜ì—­ ì‚­ì œ ë¹„ìœ¨\n",
    "\n",
    "    # =========================\n",
    "    # ê³ ê¸‰ ì˜µì…˜ ë° ì¬í˜„ì„±\n",
    "    # =========================\n",
    "    rect=RECT_MODE,          # (bool) ë°°ì¹˜ ë‹¨ìœ„ë¡œ aspect ratio ìœ ì§€ ì—¬ë¶€ (train ê¸°ë³¸ False)\n",
    "    cache=\"disk\",            # (bool|'ram'|'disk') ì´ë¯¸ì§€/ë¼ë²¨ ìºì‹œ: 'disk'ëŠ” I/O ì ˆê°\n",
    "    multi_scale=False,       # (bool) ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í•™ìŠµ í™œì„±í™” (0 â†’ ë¹„í™œì„±)\n",
    "    single_cls=False,        # (bool) ëª¨ë“  í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ë¡œ í†µí•©\n",
    "    classes=None,            # (list[int]|int) íŠ¹ì • í´ë˜ìŠ¤ë§Œ í•™ìŠµ. 0 â†’ ì „ì²´ í´ë˜ìŠ¤ ì‚¬ìš©\n",
    "    fraction=1.0,            # (float) ë°ì´í„°ì…‹ ì‚¬ìš© ë¹„ìœ¨ (1.0 â†’ ì „ì²´ ë°ì´í„°)\n",
    "    seed=SEED,               # (int) ëœë¤ ì‹œë“œ (ì¬í˜„ì„±)\n",
    "    deterministic=True,      # (bool) ì™„ì „ ê²°ì •ë¡ ì  ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©\n",
    "    amp=True,                # (bool) ìë™ í˜¼í•©ì •ë°€ë„(fp16) í•™ìŠµ\n",
    "    freeze=0,                # (int|list) ì²˜ìŒ Nê°œ ë ˆì´ì–´ í˜¹ì€ íŠ¹ì • ë ˆì´ì–´ ì¸ë±ìŠ¤ ê³ ì •\n",
    "    compile=False,           # (bool|str) torch.compile ì‚¬ìš© (0 â†’ ë¹„í™œì„±)\n",
    "    profile=False,           # (bool) ONNX/TensorRT í”„ë¡œíŒŒì¼ë§\n",
    "    val=True,                # (bool) í•™ìŠµ ì¤‘ ê²€ì¦ ì‹¤í–‰\n",
    "    save=True,               # (bool) ì²´í¬í¬ì¸íŠ¸ ë° ìµœì¢… ê°€ì¤‘ì¹˜ ì €ì¥\n",
    "    save_period=SAVE_PERIOD, # (int) ëª‡ epochë§ˆë‹¤ ì¤‘ê°„ ê°€ì¤‘ì¹˜ ì €ì¥ (0 â†’ ë¹„í™œì„±)\n",
    "\n",
    "    # =========================\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    # =========================\n",
    "    project=\"/home/dw/ws_job_msislab/Golf_Project/runs_yolo\",  # (str) ìƒìœ„ í”„ë¡œì íŠ¸ í´ë”\n",
    "    name=exp_name,           # (str) í”„ë¡œì íŠ¸ í•˜ìœ„ í´ë”ëª…\n",
    "    exist_ok=True            # (bool) ë™ì¼ ì´ë¦„ í´ë”ê°€ ìˆì–´ë„ ë®ì–´ì“°ê¸° í—ˆìš©\n",
    ")\n",
    "\n",
    "\n",
    "# ğŸ”¹ sum4 ê°€ì¤‘ì¹˜ (ì‚¬ìš©ìê°€ ì§ì ‘ ìˆ˜ì • ê°€ëŠ¥)\n",
    "WEIGHTS_SUM4 = {\n",
    "    \"precision\": 0.20,\n",
    "    \"recall\":    0.30,\n",
    "    \"map50\":     0.25,\n",
    "    \"map5095\":   0.25,\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# í›„ì²˜ë¦¬: results.csv ê¸°ë°˜ ë² ìŠ¤íŠ¸ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬\n",
    "# ==============================\n",
    "proj_dir    = Path(\"/home/dw/ws_job_msislab/Golf_Project/runs_yolo\")\n",
    "exp_dir     = proj_dir / exp_name\n",
    "weights_dir = exp_dir / \"weights\"\n",
    "csv_path    = exp_dir / \"results.csv\"\n",
    "\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(f\"results.csvì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}\")\n",
    "\n",
    "CANDIDATES = {\n",
    "    \"precision\": [\"metrics/precision(B)\", \"metrics/precision\", \"precision\"],\n",
    "    \"recall\":    [\"metrics/recall(B)\",    \"metrics/recall\",    \"recall\"],\n",
    "    \"map50\":     [\"metrics/mAP50(B)\",     \"metrics/mAP50\",     \"map50\"],\n",
    "    \"map5095\":   [\"metrics/mAP50-95(B)\",  \"metrics/mAP50-95\",  \"mAP50-95\"],\n",
    "}\n",
    "\n",
    "def read_results_csv(path: Path):\n",
    "    with path.open(\"r\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "        fields = reader.fieldnames or []\n",
    "    return rows, fields\n",
    "\n",
    "def pick_col(cands, fieldnames):\n",
    "    for c in cands:\n",
    "        if c in fieldnames: return c\n",
    "    raise KeyError(f\"CSVì— í•´ë‹¹ ë©”íŠ¸ë¦­ ì»¬ëŸ¼ì´ ì—†ìŒ: {cands}\")\n",
    "\n",
    "def to_float_safe(v):\n",
    "    try:\n",
    "        x = float(v); return x if not math.isnan(x) else -1.0\n",
    "    except: return -1.0\n",
    "\n",
    "rows, fields = read_results_csv(csv_path)\n",
    "col_prec    = pick_col(CANDIDATES[\"precision\"], fields)\n",
    "col_rec     = pick_col(CANDIDATES[\"recall\"],    fields)\n",
    "col_map50   = pick_col(CANDIDATES[\"map50\"],     fields)\n",
    "col_map5095 = pick_col(CANDIDATES[\"map5095\"],   fields)\n",
    "\n",
    "# ==============================\n",
    "# Top3 ê´€ë¦¬ í•¨ìˆ˜\n",
    "# ==============================\n",
    "def update_top3(top3, value, epoch):\n",
    "    b_val, b_ep = top3[0]; s_val, s_ep = top3[1]; t_val, t_ep = top3[2]\n",
    "    if value > b_val or (value == b_val and (b_ep == -1 or epoch < b_ep)):\n",
    "        if b_ep != -1 and b_ep != epoch:\n",
    "            t_val, t_ep = s_val, s_ep\n",
    "            s_val, s_ep = b_val, b_ep\n",
    "        b_val, b_ep = value, epoch\n",
    "    elif epoch != b_ep and (value > s_val or (value == s_val and (s_ep == -1 or epoch < s_ep))):\n",
    "        if s_ep != -1 and s_ep != epoch:\n",
    "            t_val, t_ep = s_val, s_ep\n",
    "        s_val, s_ep = value, epoch\n",
    "    elif epoch not in (b_ep, s_ep) and (value > t_val or (value == t_val and (t_ep == -1 or epoch < t_ep))):\n",
    "        t_val, t_ep = value, epoch\n",
    "    top3[0], top3[1], top3[2] = (b_val, b_ep), (s_val, s_ep), (t_val, t_ep)\n",
    "\n",
    "top3 = {m:[(-1.0,-1)]*3 for m in [\"precision\",\"recall\",\"map50\",\"map5095\",\"sum4\"]}\n",
    "\n",
    "for r in rows:\n",
    "    if not r.get(\"epoch\"): continue\n",
    "    e_csv = int(r[\"epoch\"])\n",
    "    p,rc,m50,m95 = map(to_float_safe, [r.get(col_prec), r.get(col_rec), r.get(col_map50), r.get(col_map5095)])\n",
    "    update_top3(top3[\"precision\"], p, e_csv)\n",
    "    update_top3(top3[\"recall\"],    rc, e_csv)\n",
    "    update_top3(top3[\"map50\"],     m50, e_csv)\n",
    "    update_top3(top3[\"map5095\"],   m95, e_csv)\n",
    "    sum4 = (p*WEIGHTS_SUM4[\"precision\"] +\n",
    "            rc*WEIGHTS_SUM4[\"recall\"] +\n",
    "            m50*WEIGHTS_SUM4[\"map50\"] +\n",
    "            m95*WEIGHTS_SUM4[\"map5095\"])\n",
    "    update_top3(top3[\"sum4\"], sum4, e_csv)\n",
    "\n",
    "# ==============================\n",
    "# ì²´í¬í¬ì¸íŠ¸ ë³µì‚¬\n",
    "# ==============================\n",
    "best_file, last_file = weights_dir/\"best.pt\", weights_dir/\"last.pt\"\n",
    "kept_files = set([f.name for f in (best_file,last_file) if f.exists()])\n",
    "summary_lines=[]\n",
    "\n",
    "def ckpt_path_from_csv_epoch(e_csv: int) -> Path:\n",
    "    return weights_dir / f\"epoch{e_csv-1}.pt\"\n",
    "\n",
    "def same_file(a: Path, b: Path) -> bool:\n",
    "    return a.exists() and b.exists() and a.read_bytes() == b.read_bytes()\n",
    "\n",
    "def maybe_copy(tag_prefix, rank, epoch_csv, value):\n",
    "    if epoch_csv < 0 or value < 0: return\n",
    "    src = ckpt_path_from_csv_epoch(epoch_csv)\n",
    "    if not src.exists(): return\n",
    "    if same_file(src,best_file) or same_file(src,last_file): return\n",
    "    dst = weights_dir / f\"{tag_prefix}_{rank}_epoch{epoch_csv}_{value:.5f}.pt\"\n",
    "    shutil.copy2(src,dst)\n",
    "    if STRIP_COPIES: strip_optimizer(str(dst))\n",
    "    kept_files.add(dst.name)\n",
    "    summary_lines.append(f\"{tag_prefix} {rank}: epoch={epoch_csv}, value={value:.6f} -> {dst.name}\")\n",
    "\n",
    "for metric in [\"precision\",\"recall\",\"map50\",\"map5095\",\"sum4\"]:\n",
    "    for idx,rank in enumerate([\"BEST\",\"SECOND\",\"THIRD\"]):\n",
    "        v,e = top3[metric][idx]\n",
    "        maybe_copy(metric,rank,e,v)\n",
    "\n",
    "# ==============================\n",
    "# ë‚¨ì€ epoch ì‚­ì œ\n",
    "# ==============================\n",
    "if CLEAN_EPOCHS:\n",
    "    for p in weights_dir.glob(\"epoch*.pt\"):\n",
    "        if p.name not in kept_files:\n",
    "            p.unlink(missing_ok=True)\n",
    "\n",
    "summary_path = exp_dir / \"best_by_metric_summary.txt\"\n",
    "with summary_path.open(\"w\") as f:\n",
    "    f.write(\"=== Top3 by Metric (with weighted sum4) ===\\n\")\n",
    "    f.write(f\"WEIGHTS_SUM4 = {WEIGHTS_SUM4}\\n\\n\")\n",
    "    for line in summary_lines: f.write(line+\"\\n\")\n",
    "\n",
    "print(\"\\n[ì™„ë£Œ] ë³´ì¡´ëœ ì²´í¬í¬ì¸íŠ¸:\")\n",
    "for n in sorted(kept_files): print(\" -\",n)\n",
    "print(f\"ìš”ì•½ ì €ì¥ë¨: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65c065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "golf_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
